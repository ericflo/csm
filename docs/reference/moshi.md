# Moshi: Full-Duplex Speech-Text Model Architecture and Implementation

**Introduction:** Moshi is a recently introduced speech-to-speech foundation model by Kyutai Labs that enables real-time, full-duplex spoken dialogue ([](https://kyutai.org/Moshi.pdf#:~:text=We%20introduce%20Moshi%2C%20a%20speech,Finally%2C%20they%20rely%20on)) ([](https://kyutai.org/Moshi.pdf#:~:text=also%20illustrate%20how%20it%20can,labs%2Fmoshi)). Unlike traditional voice assistants that pipeline ASR (speech recognition), NLP, and TTS (speech synthesis), Moshi treats conversation as a single end-to-end process: it directly **generates speech from speech**, using text only as an internal intermediate representation ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,the%20modeling%20of%20arbitrary%20conversational)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20speech,time)). This unified approach preserves non-verbal cues (like emotion and intonation) and allows truly interactive conversations with overlapping speech (no fixed turn-taking) ([](https://kyutai.org/Moshi.pdf#:~:text=their%20complex%02ity%20induces%20a%20latency,text%20language%20model%20backbone%2C%20Moshi)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=This%20allows%20for%20the%20removal,of%20160ms%2C%20200ms%20in%20practice)). Moshi’s design is powered by three primary components: a **text language model backbone (Helium)** for language understanding/generation, a **neural audio codec (Mimi)** for compressing/decompressing speech audio into discrete tokens, and a **multi-stream Transformer architecture** (Temporal and Depth Transformers) to model the simultaneous audio streams of user and system in a hierarchical fashion ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)). We provide an in-depth technical breakdown of Moshi’s architecture, training regime, real-time inference process, model topology, implementation details, and safety considerations – all the information an expert ML engineer would need to reimplement Moshi from scratch.

## 1. Understanding Moshi’s Architecture and Components

([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/)) _High-level architecture of Moshi’s full-duplex dialogue model. The Helium Temporal Transformer (7B) autoregressively models time steps (at 12.5 Hz), while a smaller Depth Transformer generates the multiple audio codec tokens per time step (semantic & acoustic tokens). The user’s incoming audio is encoded by Mimi into discrete tokens (purple) fed into the model as one stream, and Moshi’s own outgoing audio tokens (orange) are generated simultaneously on another stream. An “Inner Monologue” text token (if any) may be produced as a prefix to Moshi’s audio tokens, improving linguistic quality (see §3.4.4 in paper). Moshi’s audio output tokens are decoded by Mimi back to speech audio. This architecture enables processing of two audio streams concurrently with minimal latency._ ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=It%20operates%20as%20a%20full,200%20milliseconds%20on%20L4%20GPUs))

### 1.1 Helium: Text Language Model Backbone (7B LLM)

**Architecture:** Helium is a 7-billion-parameter autoregressive language model that forms the core “brain” of Moshi ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=Moshi%20comprises%20three%20key%20elements%3A,and%20Moshi%20on%20separate%20channels)). It uses a Transformer decoder architecture similar to recent large language models (comparable to LLaMA-7B) with several modern tweaks for efficiency and performance ([](https://kyutai.org/Moshi.pdf#:~:text=Helium%20is%20an%20autoregressive%20language,a%20context%20length)) ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)). Helium adopts **RMSNorm** normalization layers (instead of LayerNorm) at the input of attention/FFN blocks and output, **Rotary Positional Embeddings (RoPE)** for 4,096 token context, and **Gated Linear Unit (GLU)** feed-forward layers with SiLU activation ([](https://kyutai.org/Moshi.pdf#:~:text=Helium%20is%20an%20autoregressive%20language,a%20context%20length)) ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)). These choices follow best practices from contemporary LLM research – e.g. RMSNorm and RoPE help stable training at long context lengths, and GLU (a gated feed-forward such as SwiGLU) improves parameter efficiency and learning capacity. The model dimension is 4096 with 32 Transformer layers and 32 attention heads, and feed-forward hidden size ~11k (11264) ([](https://kyutai.org/Moshi.pdf#:~:text=Hyper,Learning%20rate%203%20%C2%B7%2010%E2%88%924)), consistent with a 7B parameter scale. A SentencePiece unigram tokenizer with a vocabulary of 32,000 subword tokens (primarily English) is used for text; it includes byte fallback to avoid unknown tokens and splits numbers into individual digits to preserve information ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)).

**Training Data & Objectives:** Helium was pretrained on an extremely large text corpus of **2.1 trillion tokens** of English text ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)). Kyutai curated high-quality sources like Wikipedia, StackExchange, and a large collection of scientific articles, then augmented with filtered CommonCrawl web data to reach the required scale ([](https://kyutai.org/Moshi.pdf#:~:text=Training%20data%20is%20one%20of,See%20more%20details%20on%20data)). The data pipeline involved aggressive deduplication (using hash-based filtering and bloom filters), language ID filtering for English, and a **quality filtering** classifier that scored pages (favoring those related to high-quality domains like STEM or humanities) ([](https://kyutai.org/Moshi.pdf#:~:text=obtain%20a%20large%20and%20high,quality)). This ensured Helium’s text training data is rich and diverse while minimizing low-quality or toxic content. Training Helium to convergence required on the order of 500k optimization steps with very large batches (4.2M tokens per batch) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)) – achieved via distributed training on GPU clusters. Optimization used the **AdamW** optimizer with a fixed learning rate and cosine decay schedule ([](https://kyutai.org/Moshi.pdf#:~:text=digits%2C%20and%20use%20byte,7)). Helium’s final perplexity and language understanding are strong, providing Moshi with a solid foundation in linguistic knowledge and reasoning ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%20is%20built%20on%20top,We%20also)).

**Role in Moshi:** In the Moshi architecture, Helium serves as the **Temporal Transformer** – it processes the dialogue history (in text and audio-token form) over time and produces latent context embeddings that will be used to generate new outputs ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)). Essentially, Helium is responsible for understanding the conversational context and deciding _what_ to say next (at the semantic/text level), leveraging its LLM capabilities. However, rather than directly outputting text, Moshi uses Helium’s next-step embedding to condition a secondary model that will produce audio tokens (speech) for that step. This allows the model to **speak in audio** while still thinking in terms of language. Helium’s large capacity enables Moshi to carry on coherent, contentful dialogues – it provides the language backbone that ensures responses are contextually relevant and logically sound ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%20is%20built%20on%20top,We%20also)). Helium was even fine-tuned on dialogue-style data (e.g. OpenHermes synthetic dialogue and real transcripts) to better handle interactive conversations ([](https://kyutai.org/Moshi.pdf#:~:text=bullet%20points%2C%20long%20enumerations%29,then%20synthesize%20them%20with%20our)), which helps Moshi produce realistic conversational behavior.

### 1.2 Mimi: Streaming Neural Audio Codec

**Purpose:** Mimi is Moshi’s **neural audio codec**, responsible for converting raw speech waveforms into discrete token sequences (and back). It compresses audio at 24 kHz into a low-bitrate sequence of codec tokens that the language model can handle ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Mimi%20is%20a%20neural%20audio,tokens%20for%20a%20streaming%20Transformer)). Without Mimi, Moshi would have to deal with raw audio signals directly, which is intractable for an LLM – Mimi provides a learned discrete representation of audio that is far more compact while preserving speech content and quality.

**Codec Structure:** Mimi follows the general design of neural codecs like **SoundStream/EnCodec** ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,audio%20compression%20at%20low%20bitrates)): it has an **encoder** (which compresses audio to a latent space), a quantization bottleneck, and a **decoder** (which reconstructs audio from quantized latents) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=Being%20an%20audio%20codec%2C%20Mimi,composed%20by%20two%20main%20components)) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=The%20Decoding%20Process)). The encoder is a convolutional neural network that progressively downsamples the input waveform; specifically, Mimi’s encoder uses a stack of residual conv blocks with strides 4×5×6×8×2 (total downsampling factor 1920) to turn 24 kHz audio into a 512-dimensional latent vector at **12.5 Hz frame rate** (i.e. one 512-d vector every 80 ms) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L694%20striding%20fac%02tors,and%20dimension%20D%20%3D%20512)) ([](https://kyutai.org/Moshi.pdf#:~:text=striding%20fac%02tors%20,and%20dimension%20D%20%3D%20512)). All convolutions are causal (no future context) with dilations, ensuring the encoder can run in a streaming fashion (producing output as audio comes in) ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)) ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). The decoder mirrors this (with transpose convolutions) to upsample 12.5 Hz latent back to 24 kHz audio ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)).

At the bottleneck, Mimi uses **Residual Vector Quantization (RVQ)** to discretize the 512-d latent. Importantly, Mimi’s quantizer is _split_ into two parts: one **semantic codebook** and several **acoustic codebooks** ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=3,produces%201%20token%20per%20frame)). In total Mimi uses **Q = 8 quantizers** (codebooks) per frame in the paper’s design ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=3,produces%201%20token%20per%20frame)) ([](https://kyutai.org/Moshi.pdf#:~:text=Quantization%20rate,We%20moreover%20follow)). The first quantizer produces a “semantic token” capturing high-level linguistic content of that 80ms audio frame, while the remaining 7 quantizers produce “acoustic tokens” that capture the detailed voice timbre, prosody, and other low-level audio features ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=3,produces%201%20token%20per%20frame)). Each codebook has 2048 possible entries (11 bits) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,are%202048%20possible%20audio%20tokens)). Thus for each 80ms frame, Mimi yields 8 discrete tokens (1 semantic + 7 acoustic), and the overall bitrate is **12.5 Hz _ 8 tokens _ 11 bits ≈ 1.1 kbps** per audio stream ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,are%202048%20possible%20audio%20tokens)). This is a **very low bitrate** compression – much smaller than traditional codecs – making it feasible for Moshi to model speech with manageable sequence lengths. (For reference, without this hierarchical approach, modeling even 1 second of speech at 50 Hz with purely acoustic tokens would require ~50 tokens/second per stream, which is far more onerous ([](https://kyutai.org/Moshi.pdf#:~:text=with%20Q%20%3D%208%20codebooks,of%20audio%2C%20this%20would%20amount)).)

**Transformer-Enhanced Codec:** A key innovation in Mimi is the inclusion of Transformer layers in the codec bottleneck to improve compression quality. Mimi adds two small Transformer models (8 layers, model dim 512) in the latent pipeline – one inserted _before_ quantization and one _after_ dequantization – to better encode long-range context in the audio representation ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021)) ([](https://kyutai.org/Moshi.pdf#:~:text=which%20preserves%20the%20compatibility%20of,Both%2010)). These are causal Transformers (using RoPE positional encoding, context window ~250 frames = 20s) that process the sequence of latent frames, akin to the “Transformer Encoder/Decoder” in EnCodec’s recent improvements ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021)). By doing this, Mimi can capture semantic information spread over time (e.g. phonetic content spanning multiple 80ms frames) and allocate it efficiently into the first quantizer. The Transformers significantly improved perceived audio quality and the efficacy of semantic token distillation ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%3A%20a%20speech,parameters%2C%20Mimi%20is%20causal%20and)) ([](https://kyutai.org/Moshi.pdf#:~:text=,67)). To keep training stable, Mimi used **LayerScale** initialization (transformer weights scaled to 0.01 initially) and applied strong regularization (notably weight decay 5e-2 on Transformer parameters only) ([](https://kyutai.org/Moshi.pdf#:~:text=additional%20regular%02ization%20with%20weight%20decay,250%20frames%20before)) ([](https://kyutai.org/Moshi.pdf#:~:text=optimizer,250%20frames%20before%20the%20last)). Mimi’s encoder/decoder still remain mostly convolutional (for speed), but these Transformer modules help bridge the gap between purely semantic encoders (like HuBERT/WavLM) and acoustic codecs.

**Semantic Token Distillation:** The **semantic vs acoustic split** in Mimi’s quantizers is achieved by a **knowledge distillation** technique inspired by Facebook’s SpeechTokenizer ([](https://kyutai.org/Moshi.pdf#:~:text=these%20acous%02tic%20tokens%20provide%20appropriate,conditioning%2C%20by%20using%20semantic%20audio)) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20sep%02arate%20encoders%20represents%20a,com%2Fchongo%2Ftech%2Fcomp%2Ffnv%209)). A pretrained self-supervised speech model (WavLM-Large) was used to provide high-level semantic embeddings of the audio; during Mimi’s training, the encoder’s first quantizer is optimized to produce tokens that approximate WavLM’s embeddings for the same audio segment ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L661%20inspiration%20from,encoding%20and%20decoding%20of%20semantic)) ([](https://kyutai.org/Moshi.pdf#:~:text=inspiration%20from%20previous%20work%20on,encoding%20and%20decoding%20of%20semantic)). Concretely, WavLM generates a 50 Hz sequence of features from the audio (16 kHz), which are downsampled to 12.5 Hz and projected to match Mimi’s latent dimension; Mimi’s first codebook output is trained (via a cosine similarity loss) to mimic these “non-causal” semantic embeddings ([](https://kyutai.org/Moshi.pdf#:~:text=inspiration%20from%20previous%20work%20on,encoding%20and%20decoding%20of%20semantic)) ([](https://kyutai.org/Moshi.pdf#:~:text=Quantization%20rate,We%20moreover%20follow)). This distillation forces Mimi’s first token per frame to encode phonetic/content information (what is being said) in a way that’s useful for a language model, while the remaining tokens focus on reconstructing exact audio quality ([](https://kyutai.org/Moshi.pdf#:~:text=these%20acous%02tic%20tokens%20provide%20appropriate,conditioning%2C%20by%20using%20semantic%20audio)) ([](https://kyutai.org/Moshi.pdf#:~:text=counterpart%2C%20semantic%20tokens%20do%20not,with%20language%20allows%20generating%20intelligible)). Notably, Kyutai found that doing this in a **split RVQ** manner (one dedicated semantic quantizer + 7 residual quantizers) outperformed other strategies – it maintains the **phonetic discriminability** of the first token without overly degrading reconstruction quality in the others ([](https://kyutai.org/Moshi.pdf#:~:text=quality%20metrics%2C%20while%20human%20evaluations,scale%20STFT%20discriminator.%20The%20exact)) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)). In summary, Mimi yields a single stream of tokens that **jointly contain semantic and acoustic information** ([](https://kyutai.org/Moshi.pdf#:~:text=Mimi%20%28Section%203,To)), making it ideal input/output for Moshi’s Transformer (which can treat these tokens similarly to text tokens).

**Training Procedure:** Mimi was trained on a large collection of speech audio (the paper mentions on the order of “millions of hours” of audio, primarily English) ([](https://kyutai.org/Moshi.pdf#:~:text=4,majority%20of%20which%20contains%20English)). Its training objective combined traditional codec losses with adversarial training and the new distillation loss. As a baseline, Mimi started with the same losses as EnCodec: a multi-scale spectrogram reconstruction loss plus a multi-scale STFT discriminator loss (GAN-style) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,scale%20STFT)) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)). But Kyutai experimented with **“adversarial-only” training**, meaning they _drop the direct reconstruction loss_ and rely purely on the discriminator (and feature matching loss) to drive the generator ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)) ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). Surprisingly, at such a low bitrate, removing the L1/MSE reconstruction loss improved subjective audio quality (even though objective distortion metrics worsened) ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). They report a “remarkable boost” in audio naturalness using adversarial-only training for Mimi ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)) – an unusual but insightful finding that GAN-only training can prioritize perceptual quality over numerical error. In addition, Mimi’s training used a form of **quantizer dropout** (a.k.a. partial vector quantization): during training, for 50% of audio samples they bypassed quantization for some latents (feeding continuous latent to decoder) ([](https://kyutai.org/Moshi.pdf#:~:text=et%20al,that%20this%20significantly%20improves%20objective)). This technique (also used by EnCodec and others) helps the model learn scalable bitrate – i.e. the codec doesn’t overly rely on any one quantizer and can function if some are dropped ([](https://kyutai.org/Moshi.pdf#:~:text=before%20the%20decoder,%282023%29%2C%20this%20means)) ([](https://kyutai.org/Moshi.pdf#:~:text=et%20al,that%20this%20significantly%20improves%20objective)). Mimi’s final configuration operates fully **causally** with an 80ms latency (it outputs the first 80ms of decoded audio after seeing 80ms of input) ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). This means Mimi can encode or decode streaming audio on-the-fly with only a small lookahead, which is critical for real-time dialogue.

**Efficiency:** Mimi’s design is optimized for integration with an LLM. Operating at **12.5 Hz** and ~1.1 kbps means the sequence of audio tokens is quite slow and sparse compared to real time – only 12–13 tokens per second, which a Transformer can handle in streaming mode. The codec’s fully causal, streaming-friendly design ensures Moshi doesn’t introduce undue delays. In fact, Mimi’s encode/decode steps are light enough to contribute negligible latency (~80ms) relative to the Transformer inference ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). By compressing audio aggressively (while preserving intelligibility through semantic tokens), Mimi effectively **bridges the audio and text worlds**: the audio tokens are analogous to text tokens and can be modeled with similar Transformer architectures ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Similarly%20to%20,batch_size%2C%20num_codebooks%2C%20sequence_length)). This allows Moshi to treat user speech input and system speech output **as token sequences** and apply language modeling techniques to both.

### 1.3 Multi-Stream Modeling: Temporal & Depth Transformers for Dual Audio Streams

A core challenge in Moshi is modeling **two simultaneous streams** of audio tokens (user and system) along with any text tokens, without flattening everything into an exorbitantly long sequence. To handle this, Moshi introduces a **hierarchical Transformer architecture** called the **RQ-Transformer** ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). It consists of: (a) the large **Temporal Transformer** (Helium) which operates along the time axis, and (b) a smaller **Depth Transformer** which operates along the “codebook depth” axis at each time step ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=as%20illustrated%20in%20Figure%203,by%20the%20Temporal%20Transformer%2C%20and)). This effectively factorizes the generation: Helium predicts the progression of conversation over time (in steps of 80ms increments), and at each time step, the Depth Transformer produces all the required codec tokens for that time frame. By doing so, Moshi’s architecture avoids the need to handle every audio token in a single sequence. Instead of one giant autoregressive sequence of length _T × K_ (time steps × codebook tokens) for a conversation, it can handle _T_ steps with the big model and up to _K_ steps with the smaller model, dramatically reducing computational cost ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)).

**Temporal Transformer (Helium):** The Temporal Transformer operates at the granularity of **discrete time steps (frames)** – roughly 12.5 steps per second. At each step _s_, it ingests the _events/tokens from the previous step_ and outputs a **temporal context vector** z*s ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)). This context vector is an embedding that condenses what should happen at time *s* given all that has come before. Internally, Helium is essentially the same architecture as the base LLM (7B Transformer decoder) but now its “vocabulary” includes not just text tokens but also **audio token inputs** from both speakers ([](https://kyutai.org/Moshi.pdf#:~:text=as%20illustrated%20in%20Figure%203,by%20the%20Temporal%20Transformer%2C%20and)) ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). To represent multiple streams at each step, Moshi uses a clever input representation: it defines a set of sub-sequence values V*{s,k} for each stream _k_ at time _s_ (e.g. specific codebook tokens for user or system) and then projects them into the Temporal Transformer input. In practice, the model has separate learned embedding tables for each type of token (e.g. for each codebook index of each speaker, and for text) – these embeddings are summed or concatenated to form the input for that time step ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L976%20the%20Temporal,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)) ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). For example, suppose at the previous time step _s-1_ the system had produced some audio tokens and the user also was speaking; Helium will take the combination of “user’s token at s-1” and “system’s token at s-1” (and possibly a text token, see Inner Monologue below) by summing their respective embeddings to feed as the context representation of step _s-1_ ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). This way, multiple parallel streams are encoded as a **single composite token per time step** in the Temporal Transformer ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). Helium then attends over the sequence of these composite tokens from 0 up to s-1 to produce z*s = Tr<sub>Temp</sub>(V_0,...,V*{s-1}) ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)), which captures the state of the conversation at time _s_. Notably, Helium runs for **S steps rather than S×K** – e.g. modeling 5 minutes of audio might be S=~3750 steps (at 12.5 Hz) instead of ~30k tokens/steps if flattened ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). This hierarchy is what makes real-time inference feasible.

**Depth Transformer:** Once Helium provides the temporal context vector z*s for the current time step, Moshi uses the **Depth Transformer** to generate the actual audio token outputs (for that time step) in each stream ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=as%20illustrated%20in%20Figure%203,by%20the%20Temporal%20Transformer%2C%20and)). The Depth Transformer is a smaller autoregressive model (6 layers, 1024-d model, 16 heads ([](https://kyutai.org/Moshi.pdf#:~:text=Depth%20Transformer%20Model%20dimension%20,space%20Text%20cardinality%2032000%2032000))) that operates *conditioned on* z_s. Essentially, it takes z_s as a conditioning input (via cross-attention or concatenation with an input embedding) and autoregressively predicts the sequence of **K codebook tokens** for this time frame ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20an%20acoustic%20delay%20%CF%84,At%20inference%20time)). In other words, for time s, the Depth Transformer will generate tokens: e.g. first the semantic token, then acoustic token 2, then token 3, … up to token K, one by one (within that 80ms step) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20an%20acoustic%20delay%20%CF%84,At%20inference%20time)). By the design of Mimi’s codec, K=8 tokens represent the full audio for that frame (for one speaker’s audio). If both the user and system have audio at time s, those can be treated as separate sub-sequences to generate. The paper formalizes this as the Depth Transformer mapping both z_s and the previously generated sub-sequence tokens (V*{s,1}, ..., V\_{s,k-1}) to the next token logits ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). The **important outcome** is that the Depth Transformer **generates all codebook tokens in parallel across streams** for each time step, but its sequence length is limited to K (≤8) instead of the full time length ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L972%20Importantly%2C%20the,In%20practice)) ([](https://kyutai.org/Moshi.pdf#:~:text=Importantly%2C%20the%20number%20of%20steps,In%20practice)). Helium’s context vector acts like a **conditioning variable** that influences the Depth Transformer’s output for that frame. In implementation, z*s might be injected via cross-attention layers or used to initialize the Depth Transformer’s hidden state ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). The separation of duties is clear: Helium (Temporal) decides \_when and generally what* content should be spoken, and the Depth Transformer fills in _how to realize that in codec tokens_ at that time, also ensuring consistency across the multiple codebooks.

**Multi-Stream Integration:** Moshi’s model is **multi-stream** in that it explicitly tracks tokens for two speakers: the user and the system (Moshi itself). At each time step, there can be tokens from the user’s audio stream (if the user is speaking at that moment) and tokens from the system’s audio (if Moshi is speaking), potentially both simultaneously. Moshi handles this by concatenating or stacking the two streams’ tokens as part of the Depth Transformer’s output for the time step ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)). In practice, they arrange the token order such that the model can distinguish the source: e.g., **Moshi’s tokens vs user’s tokens have distinct positions or embedding offsets**. According to the paper, they stack the user and system token sequences for each time: _“multi-stream modeling which stacks the tokens of Moshi and the user for each timestep”_ ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)). For example, one scheme is: at time s, let V<sub>s,1...Q</sub> be Moshi’s Q audio tokens and V<sub>s,Q+1...2Q</sub> be the user’s Q audio tokens (with some fixed ordering). The Temporal Transformer then sees the combined effect of both streams from the previous step. This enables **full-duplex** capabilities – the model can generate Moshi’s response while simultaneously receiving user input tokens, since both are just streams of tokens it conditions on. There is no hard turn-taking; overlapping speech is naturally represented by both streams having tokens at the same time indices ([](https://kyutai.org/Moshi.pdf#:~:text=information%20that%20modifies%20meaning%E2%80%94%20such,of%20the%20user%20into%20parallel)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). Moshi’s training data included many instances of such overlaps, interruptions, and backchannels, teaching the model how to handle them. The **speaker identity** is implicitly encoded by whether a token came from the user’s stream or Moshi’s stream (and possibly the model uses separate embedding tables for each to differentiate). In sum, Moshi’s architecture merges two input/output channels through the Temporal Transformer – a design analogous to multi-channel sequence modeling – letting the model learn the dynamics of conversation (e.g. one person pauses, the other interjects, etc.) without any turn segmentation ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,the%20modeling%20of%20arbitrary%20conversational)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)).

**Inner Monologue (Text Prefix):** One additional component in Moshi’s architecture is the **“Inner Monologue”** mechanism ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=contribution%20is%20the%20Inner%20Monologue%2C,linguistic%20information)). This refers to Moshi predicting **time-aligned text tokens as a prefix to its own audio tokens** when it speaks ([](https://kyutai.org/Moshi.pdf#:~:text=We%20moreover%20extend%20the%20hierarchical,200ms%20in%20practice%2C%20and%20is)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=contribution%20is%20the%20Inner%20Monologue%2C,linguistic%20information)). In other words, before (or while) the Depth Transformer generates Moshi’s acoustic tokens for a time frame, the model first outputs the corresponding text token of what it’s about to say. These text tokens are not spoken aloud; they are an internal representation (hence “inner monologue”) that the model uses. For example, if Moshi is saying “hello” spanning frames s=1 to s=3, it might output the text token “Hello,” at the start, aligned to the audio start, then proceed with the audio tokens that produce the spoken word ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=between%20each%20token%20enunciation)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,are%20synchronized%20with%20the%20audios)). This extension effectively gives the language model a chance to **“think in words”** just ahead of producing speech sounds. It was found to massively improve the linguistic coherence and fluency of the generated speech ([](https://kyutai.org/Moshi.pdf#:~:text=We%20moreover%20extend%20the%20hierarchical,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=This%20allows%20for%20the%20removal,of%20160ms%2C%20200ms%20in%20practice)), since Helium is fundamentally better at next-word prediction than next-audio-frame prediction. The inner text serves as a guiding transcript for the acoustic generation. Notably, Moshi’s inner monologue is **time-aligned**: the text tokens are interwoven into the audio token timeline (with appropriate padding to align them) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=The%20original%20model%20is%20synchronized,in%20between%20each%20token%20enunciation)). Moshi predicts these text tokens only for _its own_ speech, not for the user (the user’s speech is not transcribed by Moshi during normal generation) ([](https://kyutai.org/Moshi.pdf#:~:text=Note%20that%20we%20do%20not,of%20the%20user%2C%20as%20transcribing)). By keeping these text tokens as part of the output sequence (as a special stream or prefix in the multi-stream), Moshi essentially operates in a **speech-to-text-to-speech mode** internally – but it’s all within one unified Transformer model. This is a novel middle ground between pure speech generation and cascaded ASR+TTS: Moshi remains a direct speech generator, but with an internal textual “whisper” that aids it.

**Benefits:** The multi-stream, two-level Transformer architecture is the key enabler of Moshi’s **real-time full-duplex** performance. It reduces the effective sequence length dramatically (Temporal Transformer steps = ~12.5 per second, vs 100 tokens/sec if all codebooks were flattened) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). This means the large 7B model only needs to do ~12.5 forward steps to generate one second of audio ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=previously%20proposed%20for%20discrete%20image,art%20audio%20language%20modeling%20performance)). Indeed, Kyutai highlights that thanks to the Depth Transformer, they “only need 12.5 passes through the 7B backbone for 1s of audio” ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=previously%20proposed%20for%20discrete%20image,art%20audio%20language%20modeling%20performance)). That corresponds to about an 80ms stride, which is how they achieve low latency. The **Depth Transformer** itself is much smaller and thus fast; it can generate the codebook tokens per step with negligible overhead. Also, by stacking user and system streams, the model handles overlap intrinsically – no separate voice activity detection or turn management is needed ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,the%20modeling%20of%20arbitrary%20conversational)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). This architecture, termed RQ-Transformer in the paper, was inspired by RQ-VAE models in image generation and proved effective for audio ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)). It is a major architectural innovation of Moshi, allowing it to be _the first large-scale spoken dialogue model that is truly real-time and simultaneous_ ([](https://kyutai.org/Moshi.pdf#:~:text=Monologue%E2%80%9D%20method%20significantly%20improves%20the,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)).

## 2. Training Procedures and Data

Moshi’s training pipeline was conducted in **multiple phases**, each with different data and objectives ([](https://kyutai.org/Moshi.pdf#:~:text=of%20our%207B,custom%20dataset%20built%20from%20synthetic)). The model starts from Helium’s pretrained weights for the Temporal Transformer, then is gradually taught to handle audio tokens and multi-speaker dialogue, and finally fine-tuned for conversational instruction-following. In parallel, the Mimi codec is trained separately on audio data before being fixed for Moshi. We detail the datasets used, how they were preprocessed, the distinct training stages, and the loss/optimization strategies at each stage.

### 2.1 Datasets and Preprocessing

- **Text Pretraining Data:** As mentioned, Helium was trained on 2.1 trillion tokens of English text. High-quality sources included Wikipedia, StackExchange Q&A, and scientific publications, supplemented heavily by web crawl data (CommonCrawl) that was filtered for quality ([](https://kyutai.org/Moshi.pdf#:~:text=obtain%20a%20large%20and%20high,quality)). The filtering pipeline first removed duplicative content (using hash fingerprints of lines and dedup at shard and line level) and non-English text ([](https://kyutai.org/Moshi.pdf#:~:text=obtain%20a%20large%20and%20high,quality%20training%20set)). Then a classifier (fastText-based) scored each line for content quality and topic, to keep only those above a certain threshold ([](https://kyutai.org/Moshi.pdf#:~:text=as%20Wikipedia%2C%20Stack%20Exchange%20and,In%20the)). They likely aimed for a broad but balanced corpus, avoiding too much of any single domain or low-quality web text. The resulting text dataset was extremely large (trillions of tokens) and required distributed training over many GPUs for several weeks. Helium’s final training was unsupervised (next-token prediction) with a cross-entropy loss on the text tokens.

- **Unsupervised Audio Pretraining Data:** For teaching Moshi to model speech audio, Kyutai assembled an **enormous audio dataset (~7 million hours)** of “readily available audio content” ([](https://kyutai.org/Moshi.pdf#:~:text=4,majority%20of%20which%20contains%20English)). This is orders of magnitude larger than typical speech corpora – it likely includes a variety of sources such as audio books, podcasts, web videos, conversational speech datasets, etc. The majority is English (which matches Helium’s language), but possibly some multilingual content was included given the scale. They call it “unsupervised” because these audio clips are not annotated with transcriptions or speaker labels for the most part. To use this data, they rely on the **Mimi codec** to turn the raw audio into sequences of tokens. In early training phases, Moshi is exposed to this audio data in a _single-stream_ fashion (only one speaker’s audio at a time) to learn audio generation and understanding. It’s worth noting that 7M hours is an extremely large dataset – likely collected from many open-source audio repositories. Preprocessing steps for audio would include resampling everything to 24 kHz (the codec’s rate), normalizing volumes, and possibly segmenting long recordings into shorter chunks (they mention using random 12-second windows during training ([](https://kyutai.org/Moshi.pdf#:~:text=,codebook%20size%20of%20NA)) ([](https://kyutai.org/Moshi.pdf#:~:text=weights%20with%20a%20decay%20of,While%20the%20latent%20dimension%20is))). Since no transcripts are used initially, the model’s task is essentially to model the distribution of Mimi audio token sequences (similar to how AudioLM and SoundStream Language Models are trained to predict compressed audio tokens).

- **Simulated Multi-Stream Data (Diarization-based):** After initial audio training, Moshi undergoes a **post-training phase with “simulated multi-stream” audio based on diarization** ([](https://kyutai.org/Moshi.pdf#:~:text=The%20training%20of%20Moshi%20goes,keep%20training%20half%20of%20the)). This means they created pseudo-conversations from raw audio by taking single-speaker recordings and mixing or pairing them to form two-channel inputs. Likely, they applied a speaker diarization algorithm on some multi-speaker recordings or meetings to get separate speaker segments, and then treated those as parallel streams for training Moshi. Alternatively, they could randomly pair segments from the unsupervised audio data, designating one as “user” and another as “system,” possibly overlapping them to simulate interruptions. The goal in this phase is to introduce Moshi to **two-stream audio** scenarios _without_ relying on transcripts. The model learns to handle two concurrent token streams and the temporal relationships between them (e.g. one stream pausing while the other speaks). Since no textual supervision is yet present, Moshi at this stage is essentially learning to predict future audio tokens on both streams given past audio tokens on both streams – a complex acoustic modeling task. Diarization ensures that in training examples the two streams indeed correspond to different speakers. This phase used a subset of data or fewer steps (the paper notes ~100k steps with 8h audio batches) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)).

- **Supervised Conversation Data (Fisher Dataset):** To teach Moshi actual conversational dynamics with real transcript grounding, they fine-tuned on the **Fisher English corpus** ([](https://kyutai.org/Moshi.pdf#:~:text=Temporal%20Transformer%20initialized%20from%20Helium,2004%29%20to%20gain%20its)) ([](https://kyutai.org/Moshi.pdf#:~:text=the%20same%20time,paired%20participants%2C%20with%20a%20given)). Fisher is ~2,000 hours of telephone conversations (2 speakers on separate channels) with transcriptions – a classic dataset for speech diarization and ASR ([](https://kyutai.org/Moshi.pdf#:~:text=the%20same%20time,paired%20participants%2C%20with%20a%20given)). Because Fisher has each side recorded separately, it provides **ground-truth two-channel audio** (one channel per speaker) aligned with text. Kyutai leveraged this in two ways. First, they used the audio to further fine-tune Moshi’s ability to handle _real_ overlapping dialogue (which might have different characteristics than the simulated mixes). They mention using 10k training steps on Fisher to “gain fully duplex capabilities” ([](https://kyutai.org/Moshi.pdf#:~:text=Temporal%20Transformer%20initialized%20from%20Helium,for%20Helium%2C%20using%20a%20separate)) ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Fisher%20dataset%20,We%20train%20for%2010k)) – essentially teaching Moshi to cope with realistic timing of turn-taking, interruptions, and silence in human dialogue. They likely input the Fisher conversations into Moshi by mapping one speaker’s audio tokens to Moshi’s stream and the other speaker to user stream ([](https://kyutai.org/Moshi.pdf#:~:text=For%20both%20Fisher%20and%20this,For)). The transcripts could be used to insert **Inner Monologue text tokens** for Moshi’s side (the “main speaker”) during training ([](https://kyutai.org/Moshi.pdf#:~:text=For%20both%20Fisher%20and%20this,For)) ([](https://kyutai.org/Moshi.pdf#:~:text=Fisher%2C%20the%20text%20stream%20only,along%20with%20the%20medium%20Whisper)). They did align the transcripts with audio using Whisper (medium model) to get accurate word timing ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L1322%20Fisher%2C%20the,along%20with%20the%20medium%20Whisper)), ensuring the text tokens could be placed at correct timestamps. Second, they actually did **not train Moshi directly on Fisher text** for the user side (the user’s words were not fed as text, only as audio) ([](https://kyutai.org/Moshi.pdf#:~:text=Note%20that%20we%20do%20not,of%20the%20user%2C%20as%20transcribing)). The text stream was only used for Moshi’s own utterances (main speaker) in that fine-tuning. This mimics Moshi’s deployment setting: it doesn’t get transcripts of user speech, only of its own. The Fisher fine-tune primarily improved Moshi’s **listening and barge-in behavior** – after this, the model learned to not freak out with overlapping inputs and to appropriately yield or continue speaking just as humans do.

- **Synthetic Conversation Data (Instruction Fine-tuning):** The final training stage was an **instructional fine-tune** on a large **synthetic dialogue dataset** created by Kyutai ([](https://kyutai.org/Moshi.pdf#:~:text=based%20on%20diarization%3B%20Fine,using%20a%20separate%20optimizer%20state)). Because high-quality, large-scale conversational data with audio is hard to obtain, they generated their own. They did so by leveraging Helium and Moshi’s TTS capabilities in a clever loop: (1) They fine-tuned Helium (as a text model) on existing dialogue datasets like OpenAI’s OpenChat or the OpenHermes dataset of multi-turn assistant conversations ([](https://kyutai.org/Moshi.pdf#:~:text=bullet%20points%2C%20long%20enumerations%29,then%20synthesize%20them%20with%20our)). (2) Using this, they _generated thousands of new dialogue scripts_ – i.e., wrote conversations between a user and an AI (Moshi persona) in text form ([](https://kyutai.org/Moshi.pdf#:~:text=bullet%20points%2C%20long%20enumerations%29,then%20synthesize%20them%20with%20our)). They likely prompted Helium to produce interactive Q&A, role-play dialogues, factual conversations, etc., to cover a wide range of scenarios. (3) They then used a **multi-stream TTS** system to convert these text dialogues into audio with two distinct voices (one for user, one for Moshi) on separate channels. The multi-stream TTS could have been an application of the Moshi model itself: since Moshi can generate both sides, one could condition it to output both user and system speech for a given script with appropriate timing. However, since Moshi wasn’t fully trained yet, they might also have used a conventional TTS for each role, ensuring the audio overlapped according to the script’s timing (like inserting interruptions and “backchannel” acknowledgments). They varied the “user” voice’s accent and noise conditions to enrich acoustic diversity, while keeping Moshi’s voice consistent (so that the model doesn’t learn to change its own voice). By doing this, they created on the order of **20,000 hours of synthetic conversational audio** paired with transcripts – a massive amount, far more than any real dataset. This dataset included many **instruction-following scenarios** (since Helium’s script generation would incorporate Q&A, user commands, etc.), which allowed Moshi to learn **assistant behaviors and obey prompts**. In training, both streams’ audio tokens are fed to Moshi, and Moshi’s stream also gets the inner monologue text tokens (the transcripts for Moshi’s own speech) as part of the target sequence. They refer to this as a custom dataset built from “synthetic interaction scripts” ([](https://kyutai.org/Moshi.pdf#:~:text=based%20on%20diarization%3B%20Fine,using%20a%20separate%20optimizer%20state)), used for instruction fine-tuning. The fine-tuning on this data (roughly 30k steps with 2.7h of audio per batch) taught Moshi to act like a helpful conversational agent, to follow user instructions/questions, and to maintain character and consistency over a dialogue. It essentially imbued the model with the **dialogue management and factual knowledge** that pure audio pretraining might not cover. Additionally, they also generated some **safety conversation scripts** (user asks something inappropriate and Moshi responds with a refusal) to fine-tune alignment/safety behavior ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)).

**Summary of Training Phases:** According to the paper, Moshi’s training had four phases ([](https://kyutai.org/Moshi.pdf#:~:text=of%20our%207B,custom%20dataset%20built%20from%20synthetic)):

1. **Pre-training (Audio-LM phase):** Using _unlabeled audio_ (7M hours) – Temporal Transformer initialized from Helium’s weights, Depth Transformer from scratch ([](https://kyutai.org/Moshi.pdf#:~:text=of%20our%207B,custom%20dataset%20built%20from%20synthetic)). Loss: predict next audio tokens (for one stream) as a language model. _Half of the training time, they still did text-only batches_ using the original text data, to ensure Helium’s language skills were not lost ([](https://kyutai.org/Moshi.pdf#:~:text=fully%20duplex%20capabilities%3B%20Instruction%20fine,training%20fisher%20fine)). (They maintained a separate optimizer state for these text-only updates ([](https://kyutai.org/Moshi.pdf#:~:text=fully%20duplex%20capabilities%3B%20Instruction%20fine,training%20fisher%20fine)).) This lasted ~1M steps, large batch (16 hours audio per batch) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,train%20LLMs%3A%20we%20now%20describe)). Additionally, an **acoustic delay** of 2 frames was used here ([](https://kyutai.org/Moshi.pdf#:~:text=Training%20steps%20500k%201M%20100k,6%200%200%200)), meaning the model sees semantic token and only 2 frames later the acoustic tokens – this stagger may help it predict semantic content slightly ahead of acoustics (a precursor to inner monologue usage).

2. **Post-training (Dual audio streams, unsupervised):** Using _simulated two-speaker audio_ (from diarization/mixing). The model (Temporal+Depth) learns to handle two parallel audio token streams. This was ~100k steps (8h audio batch) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)). By now Depth Transformer is definitely involved (predicting the multiple codebooks). They reduced acoustic delay to 1 (meaning semantic vs acoustic token offset of 1 frame) and set text delay to 0 (no text, since still unsupervised audio) ([](https://kyutai.org/Moshi.pdf#:~:text=Training%20steps%20500k%201M%20100k,6%200%200%200)). They also mention using _text-only batches 10% of the time_ during this phase ([](https://kyutai.org/Moshi.pdf#:~:text=a%20cosine%20learning%20rate%20starting,from%20the%20audio%20dataset%2C%20we)) to keep language modeling sharp.

3. **Supervised Fine-tuning (Conversational audio):** Using _real dialogue audio with transcripts_ (Fisher). This was a much shorter training (10k steps, 40 min audio per batch) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)). The model was now fully capable of multi-stream generation. Here they introduced **Inner Monologue text tokens aligned with Moshi’s speech**, using the Fisher transcripts for Moshi’s side (with Whisper alignment) ([](https://kyutai.org/Moshi.pdf#:~:text=Fisher%2C%20the%20text%20stream%20only,along%20with%20the%20medium%20Whisper)). Text delay was 0 (transcripts aligned properly) and acoustic delay 1 in this phase ([](https://kyutai.org/Moshi.pdf#:~:text=Training%20steps%20500k%201M%20100k,6%200%200%200)). The goal was to teach the model to utilize the text prefix when available and learn from a smaller but higher-quality dialogue dataset.

4. **Instruction Fine-tuning:** Using _synthetic multi-turn conversations_ (20k hours, generated as above). This final phase (~30k steps, 2.7h batch) integrates **instruction following, knowledge Q&A, and safety behavior** into Moshi ([](https://kyutai.org/Moshi.pdf#:~:text=based%20on%20diarization%3B%20Fine,using%20a%20separate%20optimizer%20state)). Essentially this is analogous to the “RLHF/safety fine-tune” stage for ChatGPT, except done with a huge synthetic supervised dataset. Moshi learns to follow user prompts, respond helpfully, and avoid unsafe content. At this stage, the model’s text-handling on Moshi’s side is fully in place (inner monologue of what it says), and possibly they might delay _user_ text if they wanted to output transcripts as ASR – but since Moshi primarily remains speech-to-speech, they likely did not include user transcripts in training (the user side is only audio tokens). The result of this stage is a model that not only _can_ carry a full-duplex conversation, but knows _how_ to act as a conversational agent with good manners and compliance to instructions.

### 2.2 Loss Functions and Optimization

**Moshi Model Loss:** During all these phases, Moshi (Temporal+Depth) is trained as a **causal language model over a composite vocabulary** (text tokens + audio codec tokens). The primary loss is the cross-entropy of predicting the next token in each stream. At each time step, the model predicts the next token(s) in the sequence of combined streams. For example, if the sequence (flattened by time) goes: [User_audio_token(s)_t=1, Moshi_text_token_t=1, Moshi_audio_token(s)_t=1, User_audio_token(s)_t=2, ...], the model is trained to predict each token given all previous. They likely apply a masking such that Moshi cannot “see” future user tokens (to prevent peeking ahead in a conversation) – training data is sequentialized in time order. A subtlety is that Moshi’s output includes both text and audio tokens, which have very different densities and importance. The **Inner Monologue text tokens** are relatively sparse but extremely informative. The paper notes they gave higher loss weight to semantic tokens: e.g. a weight α=100 for semantic tokens vs 1 for acoustic tokens in the loss ([](https://kyutai.org/Moshi.pdf#:~:text=%CE%B1k%20is%20set%20to%20100,21)). This huge up-weighting forces the model to get the linguistic content right (the semantic token and text) even at the expense of some acoustic fidelity. This makes sense: a small error in an acoustic token might cause slight voice distortion, whereas an error in a semantic token could change a word’s identity and ruin intelligibility ([](https://kyutai.org/Moshi.pdf#:~:text=counterpart%2C%20semantic%20tokens%20do%20not,with%20language%20allows%20generating%20intelligible)). So Moshi’s loss function is a weighted sum of token prediction losses, with semantic/text tokens prioritized. Additionally, an **acoustic token delay τ** was used (τ=1 in final model) ([](https://kyutai.org/Moshi.pdf#:~:text=codebooks%20correspond%20to%20acoustic%20features,and%20acoustic%20tokens%20led%20to)): the semantic token for a time frame is predicted slightly ahead of the acoustic tokens for that same frame, which helped the model plan phonemes first then acoustics ([](https://kyutai.org/Moshi.pdf#:~:text=codebooks%20correspond%20to%20acoustic%20features,and%20acoustic%20tokens%20led%20to)). This effectively implements the inner monologue approach (predict semantic content just before the detailed acoustic tokens).

**Mimi Loss:** Mimi’s training combined three types of loss: (1) **Reconstruction loss** – comparing the reconstructed waveform to the original, via multi-scale mel-spectrogram L1 loss, etc. (2) **Adversarial loss** – discriminator(s) output for real vs reconstructed audio (with a feature matching loss) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,scale%20STFT)). (3) **Distillation loss** – cosine similarity between Mimi’s first-layer quantized embedding and WavLM’s embedding for the same audio segment ([](https://kyutai.org/Moshi.pdf#:~:text=inspiration%20from%20previous%20work%20on,encoding%20and%20decoding%20of%20semantic)). In practice, they experimented with dropping (1) and relying only on (2)+(3) after a certain point, as mentioned above, to maximize perceptual quality ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)) ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). Mimi used AdamW optimizer with a learning rate ~8e-4 and fairly long training (up to 4M steps on 128 clips batch) ([](https://kyutai.org/Moshi.pdf#:~:text=optimizer,250%20frames%20before%20the%20last)) ([](https://kyutai.org/Moshi.pdf#:~:text=,codebook%20size%20of%20NA)). They also used an exponential moving average on model weights (decay 0.99) to stabilize training ([](https://kyutai.org/Moshi.pdf#:~:text=,and%20symmetrically%20for%20the%20decoder)). By end of training, Mimi achieved very high-quality compression – evaluators noted it outperforms previous codecs like SpeechTokenizer, RVQ-VAE (SoundStream) and SemantiCodec in quality ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Mimi%20is%20a%20neural%20audio,tokens%20for%20a%20streaming%20Transformer)). An ABX phoneme discrimination test confirmed Mimi’s semantic tokens carry as much linguistic content as dedicated semantic encoders ([](https://kyutai.org/Moshi.pdf#:~:text=Mimi,based%20ABX%20%28Schatz%20et%20al)).

**Optimization Strategies:** Throughout training, various strategies ensured efficiency and retention of skills:

- **Balanced Multi-Task Training:** During Moshi’s unsupervised pre-training, they maintained text-only language modeling alongside speech modeling (using separate optimizer states) ([](https://kyutai.org/Moshi.pdf#:~:text=fully%20duplex%20capabilities%3B%20Instruction%20fine,training%20fisher%20fine)). This prevented “catastrophic forgetting” of Helium’s knowledge when the model started modeling audio. Even in later audio-heavy phases, they still periodically fed pure text batches (10% in one phase) to preserve and intermix textual knowledge ([](https://kyutai.org/Moshi.pdf#:~:text=a%20cosine%20learning%20rate%20starting,from%20the%20audio%20dataset%2C%20we)). This multi-task approach was critical given the model has a dual role (text and audio generation).

- **Two-Optimizer Setup:** They essentially treated the text-LM task and audio-LM task with different learning rates and states at first. The table of hyperparams shows Helium’s Transformer had a smaller learning rate (e.g. 3e-5) when training on audio, whereas the Depth Transformer and new tasks could use higher LR (2e-4) ([](https://kyutai.org/Moshi.pdf#:~:text=a%20cosine%20learning%20rate%20starting,from%20the%20audio%20dataset%2C%20we)) ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%20pre,2%2C%20using%20a)). This is to gently fine-tune the large LLM weights (so it doesn’t drift too far) while quickly learning new audio modeling with the Depth module.

- **Curriculum Learning:** The training progressed from easier single-speaker audio modeling to full two-speaker interactions and then to complex instruction following. This curriculum – unsupervised to supervised, single to multi, etc. – allowed the model to incrementally build capability. By the time it was doing instruction tuning, it already had the mechanics of speech generation down cold.

- **FlashAttention and Efficient Implementation:** Given the enormous sequence lengths for text pretraining (4096 tokens) and model size, they used memory-optimized kernels like **FlashAttention** during training ([](https://kyutai.org/Moshi.pdf#:~:text=model,Kudo%20and%20Richardson)) to speed up and reduce memory overhead in attention computations. They also likely used mixed precision (bfloat16) and sharded data-parallel training (FSDP or ZeRO) to handle the 7B model across GPUs.

- **Regularization:** Techniques like dropout, and specifically for Mimi, quantizer dropout and weight decay on Transformer sub-modules, were used to avoid overfitting and ensure the codec’s robustness at different bitrates ([](https://kyutai.org/Moshi.pdf#:~:text=before%20the%20decoder,%282023%29%2C%20this%20means)) ([](https://kyutai.org/Moshi.pdf#:~:text=optimizer,250%20frames%20before%20the%20last)). The text data was so large that overfitting wasn’t a concern for Helium, but for the smaller fine-tuning sets (Fisher, synthetic dialogues), they likely used early stopping or low learning rates to avoid degrading the pretrained model’s generality.

In terms of fine-tuning methodology, the instruction/safety tuning is analogous to InstructGPT: they fed in conversation prompts and expected Moshi to continue the dialogue in the desired helpful manner. They might have also done some form of RLHF or at least manually curated prompt-response pairs, but given the synthetic data approach, it sounds like they relied on generated supervised data rather than human feedback loops. All fine-tuning was done on top of the same architecture – no new parameters or adapters were added, they trained end-to-end. The final model that emerged (after these phases) was evaluated extensively, as described later.

## 3. Inference and Real-Time Processing

One of Moshi’s headline achievements is being the **first real-time, full-duplex large language model for speech** ([](https://kyutai.org/Moshi.pdf#:~:text=Monologue%E2%80%9D%20method%20significantly%20improves%20the,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)). In deployment, Moshi can listen and speak concurrently, with low latency, enabling natural conversation flow much closer to human-human interaction than prior systems. Here we explain how the inference process works, how streaming is implemented, and what performance/latency characteristics the system has. We also note considerations for deployment and scalability.

### 3.1 Full-Duplex Streaming Mechanism

During inference, Moshi operates in a **continuous loop** processing audio in frames (e.g. 40–80ms chunks). It treats the incoming user audio and its own generated audio as parallel streams of tokens that grow over time, and it generates new tokens autoregressively as time advances. A typical real-time session proceeds as follows:

- **Audio Input Encoding:** As the user speaks into the microphone, audio is streamed through the Mimi encoder to produce tokens in real time. Every 80ms of user speech yields one new semantic token and 7 acoustic tokens (once fully quantized) – though Mimi can output the first token slightly earlier due to causal streaming. These user tokens are fed incrementally into Moshi’s Temporal Transformer as they arrive (on the user stream channel). In practice, the system likely buffers a short initial window (e.g. 160ms) to get a couple of frames encoded before Moshi starts responding, but after that it can work frame-by-frame.

- **Autoregressive Generation:** The Moshi model (Temporal+Depth) generates its output tokens step by step. At each time step, it takes into account all tokens so far (user and Moshi’s) and decides whether Moshi should speak (and what). If the model “chooses” to speak, it will produce a semantic token (and an inner text token) followed by acoustic tokens for that time. If the model decides to remain silent (perhaps because the user is talking and it should listen), it might produce a special silence token or simply no Moshi tokens at that step (just padding). This behavior is learned from training where overlapping speech was present – Moshi can generate tokens on its stream while user stream tokens are also present, effectively learning appropriate timing. The inference uses the Depth Transformer to generate all required audio tokens for each time step on the fly. Importantly, this is all done autoregressively **with streaming context**: Moshi doesn’t wait for the user to finish an utterance; it is constantly conditioning on partial user audio and potentially producing partial responses.

- **Output Audio Decoding:** As soon as Moshi generates some audio tokens for its own stream, those tokens are sent to the Mimi decoder to synthesize actual waveform audio that is played back through the speaker. Because generation happens frame by frame, Moshi can begin speaking _while_ the user is still talking, or immediately after the user starts a question (for backchannels like “uh-huh” etc.). The Mimi decoder also works in streaming mode, decoding 80ms at a time with only 80ms latency ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). Therefore, the system can output speech with only a brief delay after the tokens are generated.

This pipeline effectively creates a **tight loop**: user audio -> Mimi encode -> Moshi model -> Mimi decode -> system audio. The architecture’s design ensures all pieces are low-latency. The **Temporal Transformer** runs one forward pass per 80ms frame. On modern hardware, a 7B transformer can easily compute one step in far less than 80ms, so it keeps up in real-time. The **Depth Transformer** is much smaller and its computation per step is negligible in comparison. Mimi’s encode/decode each add ~80ms initial latency but then operate continuously streaming. Thus, once the pipeline is filled, Moshi can respond with as little as a single-frame delay relative to the user.

**Latency:** The theoretical minimal latency of the full system is about **160 ms** (as reported by the authors) ([](https://kyutai.org/Moshi.pdf#:~:text=also%20illustrate%20how%20it%20can,labs%2Fmoshi)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)). In practice, they achieved about **200 ms** end-to-end latency on an NVIDIA L4 GPU ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)). This latency budget includes input audio buffering (~80ms frame), model computation (~80ms or less), and perhaps another frame of lookahead or safety margin. 200ms is about the time of a typical human conversational turn-taking pause, so interactions feel quite seamless. For comparison, conventional voice assistants often have 2–3 seconds latency (due to waiting for end-of-speech, performing ASR+NLP, then TTS) ([](https://kyutai.org/Moshi.pdf#:~:text=We%20introduce%20Moshi%2C%20a%20speech,Finally%2C%20they%20rely%20on)) ([](https://kyutai.org/Moshi.pdf#:~:text=their%20complex%02ity%20induces%20a%20latency,text%20language%20model%20backbone%2C%20Moshi)). Moshi’s approach is ~10× faster in response.

Several techniques ensure low latency:

- Moshi does not wait for a full utterance or a “stop speaking” signal; it uses the multi-stream model to handle partial overlap. This eliminates the need for a separate VAD (voice activity detector) to find end-of-user-speech, which typically adds hundreds of ms.
- The **token delay** mechanism can be tuned to trade off latency vs. understanding. For instance, they mention if you delay Moshi’s audio tokens by a couple seconds, Moshi essentially becomes a streaming TTS (getting more text context before synthesizing) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). Conversely, delaying the text tokens yields a streaming ASR ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). In the live system, they likely use a small fixed text delay (the acoustic delay τ=1 corresponds to ~80ms) so that a rough word transcript is formed just slightly after the audio begins, not too far in advance.
- The computational graph is lightweight per step. Running a 7B model for 80ms of conversation is feasible on a single GPU, especially using mixed precision and possibly compiling the model. The authors reported that Moshi can **run in real-time on a single NVIDIA L4 GPU or an Apple M3 MacBook Pro** ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=for%20Helium%20,the%20token%20delay%20of%20MusicGen)), which are relatively accessible devices. L4 is a low-power data center GPU (about 22 TFLOPS), and the M3 with Apple’s Neural Engine can handle the 7B model through their accelerated framework (MLX).

In streaming inference implementation, typically a **scheduler thread** reads microphone audio continuously, chunking it into 80ms frames. Each frame, it encodes new tokens via Mimi and appends them to the user token sequence. Then it invokes Moshi’s generate function for one step (or a few steps) to produce new Moshi tokens. Any new Moshi audio tokens get decoded to sound and immediately played. This loop runs fast enough that it keeps up with real time. Because the model is large, a common strategy is to use a **sliding context window**: after a certain number of steps (say, 3000 steps = ~4 minutes of conversation, which is Moshi’s 4096 token context limit), they might start trimming the context from the beginning to keep the prompt length manageable. Alternatively, because Helium is an LLM with 4096 context, it can handle several minutes of continuous talk without truncation – likely sufficient for most interactions, beyond which some memory of earlier content might be lost. For extended sessions, one could summarize or compress old dialogue into a brief prompt if needed.

### 3.2 Real-Time Dialogue Behavior

Moshi’s full-duplex capability means it can engage in **natural conversational behaviors** that earlier systems could not. For example, Moshi can produce **backchannel acknowledgments** (“mm-hmm”, “I see”) while the user is speaking, to show it’s listening – because its user stream is active and it can decide to inject a short response on its stream without waiting ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,modeling%20of%20arbitrary%20conversational%20dynamics)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). It can handle **interruptions**: if the user interjects while Moshi is talking, Moshi (being an AI) might stop its speech generation mid-way or adjust on the fly. The model was trained on such scenarios (overlapping speech in Fisher and synthetic data), so it can learn to yield the floor when the user speaks over it. This leads to a more **dynamic, human-like interaction** where both parties don’t strictly alternate with long silences; instead, there can be overlaps and quick interjections, which Moshi can navigate ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)).

In terms of output quality, thanks to Inner Monologue and the strong language backbone, Moshi’s spoken responses are linguistically coherent and contextually appropriate. Evaluations showed that using the inner text prefix dramatically improved word error rates and fluency of the generated speech ([](https://kyutai.org/Moshi.pdf#:~:text=We%20moreover%20extend%20the%20hierarchical,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=This%20allows%20for%20the%20removal,of%20160ms%2C%20200ms%20in%20practice)). Essentially, if you transcribe Moshi’s speech, it tends to be as sensible as what a text-only LLM would produce for that query. Also, because Mimi is a high-fidelity codec, the voice output is natural-sounding (not robotic). Moshi’s default voice was trained to be consistent (they kept Moshi’s voice the same in synthetic training) – it’s described as a specific voice/persona. In the open-source release, they even provided two fine-tuned variants with different voices (male “Moshiko” and female “Moshika”) for variety ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=How%20Moshi%20Works%3F)). The user’s voice can be any input; Moshi doesn’t change the user audio except to encode it. The model can understand different accents and noise conditions because the synthetic data was varied on the user side. This makes it robust in practical use (to some extent – extremely heavy noise might still challenge Mimi’s encoding).

**Streaming ASR and TTS modes:** An interesting byproduct of Moshi’s design is that it can be used in one-way modes as well. If you _disable_ Moshi’s audio output stream and only use inner monologue text, Moshi essentially functions as a streaming ASR (transcribing the user in real-time with alignment) by delaying text tokens to line up with audio ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). Conversely, if you input text and have it generate audio tokens (with a slight audio delay), it acts as a streaming TTS, producing speech with low latency ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). While these are side-effects (the main goal is dialogue), it shows the flexibility of having a unified model. Indeed, one could get a text transcription of what Moshi says (from the inner monologue text tokens) or even of what the user said if they force the model to output a guess of user’s words – but that wasn’t a focus (explicit ASR for user is left out in design).

### 3.3 Deployment, Scalability, and Hardware

Kyutai Labs made Moshi available with **streaming inference code in PyTorch, Rust, and Apple’s MLX** frameworks ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Today%2C%20we%20release%20several%20Moshi,in%20PyTorch%2C%20Rust%20and%20MLX)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)). This means developers can run Moshi locally or on servers with relative ease. The PyTorch implementation is straightforward for GPU servers, while the Rust implementation caters to performance-critical use (and can target different hardware). The MLX (Metal Lightning) version is optimized for Mac GPUs/ANEs, demonstrating Moshi running on an Apple Silicon laptop in real-time ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=for%20Helium%20,the%20token%20delay%20of%20MusicGen)).

For deployment, one needs the Moshi model weights (~≪ a 7B model in 16-bit is around 14 GB, plus ~1 GB for Mimi). Currently, **quantization** is not yet supported in the PyTorch release (as of late 2024), so a full 16-bit model requires a GPU with at least **24 GB VRAM** ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20company%20said%20that%20they,installed%2C%20including%20the%20nvcc%20compiler)). An NVIDIA 3090 or 4090 or A6000 would suffice, as would an L4 (22GB) in slightly compressed form. The Rust version may allow some quantization or use of lower precision, but it requires latest CUDA toolkit for GPU acceleration ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20company%20said%20that%20they,installed%2C%20including%20the%20nvcc%20compiler)). The Mac MLX version likely uses 16-bit on the 64GB M3 Max/Ultra machines; they did mention testing on an M3 MacBook Pro, which presumably worked (possibly using 32-bit ANE accumulation with 16-bit weights). As the ecosystem progresses, we can expect 4-bit or 8-bit quantized Moshi to become available, which would drastically reduce memory and allow even smaller GPUs to run it. In fact, the paper notes that the Depth Transformer was quite robust to quantization in tests ([](https://kyutai.org/Moshi.pdf#:~:text=sizes,is%20reasonably%20robust%20to%20quantization)), and likely Helium could be quantized similarly to LLaMA-7B with minimal loss.

**Scalability:** Moshi is primarily a single-user, single-session model – it wasn’t designed for high-throughput server workloads with multiple parallel inferences on one GPU. To serve multiple users, one would probably run multiple instances or use model parallelism across GPUs if needed. The 7B size is actually relatively lightweight in the LLM world, so serving a few concurrent sessions per GPU is not infeasible (especially if each can run at ~12.5 inferences/sec). The bottleneck is that each session’s state (the KV cache of the transformer) occupies memory proportional to the context length. For a 4096 context of 7B model, that’s a few hundred MB. But since conversation context is often pruned, the memory per session might be manageable. Still, Moshi’s novelty is more about _capability_ than mass deployment scaling – it’s for building an interactive agent.

**Integration:** Developers can integrate Moshi into applications using the open-source code. The GitHub repo provides examples and possibly a demo (they had an online demo at moshi.chat) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=,bf16)). The HuggingFace Transformers library has already integrated Moshi as a model class, so one can load `MoshiForConditionalGeneration` and use `.generate()` to run it, albeit with some caveats due to its multi-stream nature ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20streaming%20auto,what%20the%20user%20said%2Fwill%20say)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=MoshiForConditionalGeneration)). To use it via HuggingFace’s API, you must supply three inputs: `input_ids` (text prompt tokens so far), `user_audio_codes` (tensor of user audio token history), and `moshi_audio_codes` (tensor of Moshi’s own audio token history) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=the%20other%20audio%20stream%20corresponds,what%20the%20user%20said%2Fwill%20say)). These three must be synchronized in length (each time index corresponds across them) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=history)). In a live system, managing these inputs simply means appending new tokens to the appropriate sequence each step. The HF interface is a bit manual, but Kyutai’s provided code likely wraps this into a simpler streaming loop.

**Resource Requirements:** Running Moshi in real-time requires decent compute but not unattainable: a single modern GPU or high-end CPU can do it. On CPU, it would be challenging to hit 12.5 forward passes/sec for 7B parameters (though possible with optimized BLAS and int8 quantization, it might be borderline). The focus has been on GPU and Apple Neural Engine. The Rust version might allow deploying on an edge device with CUDA support (like Jetson) but probably still needs a strong device given the model size.

For training from scratch, of course, the requirements were massive (TPUs/GPUs for weeks, 7M hour audio dataset, etc.). But an engineer reimplementing might leverage the open training recipe and possibly smaller scale to experiment.

In summary, the inference pipeline is well-engineered for low latency. The fact that Andrej Karpathy tested Moshi on a MacBook and found it “cool” and working ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=Karpathy%20shared%20his%20excitement%20about,away%20important%20information%2C%E2%80%9D%20he%20said)) speaks to its practicality. Some early users did note the model sometimes behaves a bit oddly (interrupting too often, etc.), which are things that can be refined with further fine-tuning ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=interrupts%2C%20it%20is%20a%20bit,worthy%2C%20quipped%20Karpathy)). But the ability to locally run a ChatGPT-like voice agent that **hears and speaks simultaneously** is a significant milestone, and Moshi’s design and training solve the key technical hurdles to achieve it.

## 4. ML Model Architecture and Layout

In this section, we delve deeper into Moshi’s model topology, providing a detailed breakdown of each component and how they interact. We will also discuss various architectural innovations, parameter counts and efficiency, input/output representations, and how Moshi handles context and prompting. This is effectively a **blueprint** of Moshi’s neural network, suitable for an engineer wanting to implement it.

### 4.1 Topology and Component Interaction (Block Diagram Explanation)

Moshi’s overall architecture can be visualized as a **stack of modules** forming a pipeline: **Mimi Encoder -> Temporal Transformer (Helium) -> Depth Transformer -> Mimi Decoder**. The _inputs_ to the system are raw audio waveforms (user speech) and optionally text prompts; the _outputs_ are raw audio (Moshi’s speech) and optionally recognized text. Internally, everything is represented as token sequences:

- **Audio tokens:** Represented as $(batch, \text{num\_codebooks}, \text{seq\_len})$ as noted earlier ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Similarly%20to%20,batch_size%2C%20num_codebooks%2C%20sequence_length)). For ease, one can flatten the codebook dimension when feeding into the model (with distinct embeddings for each codebook slot).

- **Text tokens:** Standard 1D token sequence (batch, seq_len).

**Helium (Temporal Transformer):** This is a Decoder-Only Transformer with 32 layers, 4096-d model, 32 heads (each head ~128-d) – closely resembling a GPT-style or LLaMA model ([](https://kyutai.org/Moshi.pdf#:~:text=Hyper,Learning%20rate%203%20%C2%B7%2010%E2%88%924)). It has a maximum context of 4096 tokens. At any given inference step, Helium takes in a sequence of embeddings representing the history of all streams up to the previous timestep. How are these embeddings formed? For text tokens (like system’s inner monologue text), Helium uses a learned word embedding table of size 32k. For audio tokens, Helium uses a separate embedding mechanism: since each time step can have up to Q=8 tokens per stream, and potentially 2 streams, one approach is to assign each possible “position” a separate embedding matrix. In the HuggingFace implementation, they mention “each timestamp – each codebook – gets its own set of Linear layers and Embeddings” ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=2)). This suggests that for each codebook index (1 through Q) and each speaker (user or system), there is a unique embedding matrix mapping the token ID (0–2047) to a vector. Those vectors (for all tokens present at that time) are summed to form the input for that time step ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). Additionally, a positional encoding in time is applied – Helium likely uses Rotary Positional Embeddings which inherently handle temporal positions, so it rotates these summed embeddings by the position index. There may also be a trainable vector for the start token V0 (which they denote as 0) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)).

Inside Helium, multi-head self-attention operates over the sequence of these time-step embeddings, attending up to 4096 back in time. It uses FlashAttention for efficiency (computing softmax attention in optimized kernels) ([](https://kyutai.org/Moshi.pdf#:~:text=model,Kudo%20and%20Richardson)). Feed-forward layers use gated SiLU linear units (effectively doubling the channel then gating), which increases non-linearity. By the end of Helium’s stack, for the current time step _s_, we extract the output embedding for that position (context vector z_s). In a standard Transformer LM, we would multiply that by a language head to get logits over vocabulary; here, instead, we pass z_s to the Depth Transformer as conditioning.

**Depth Transformer:** The Depth Transformer is another decoder-only Transformer but much smaller (6 layers, 1024 model dim). Its “sequence length” is the number of token slots per time step. In the paper, they often call this K (with K=Q plus maybe text) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). Essentially, if we consider Moshi’s output at time s to be a sequence like [Text_token, Acoustic1, Acoustic2, ..., AcousticQ] (for system) and similarly for user, the Depth Transformer might handle them either in parallel or separately. According to the HuggingFace docs, they implement Depth as generating over the codebook dimension with context length = num_codebooks ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=2)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=On%20its%20own%2C%20it%E2%80%99s%20also,generates%20over%20the%20codebook%20dimension)). This implies the Depth Transformer sees a “position” for each codebook index 1..Q. If inner monologue text is present, that might be treated as position 0 or a separate condition. The Depth Transformer likely employs **cross-attention** to incorporate z_s (the context from Helium). In practice, one can prepend the context vector as if it were a “memory” and allow each layer of Depth to attend to it (similar to how encoder-decoder Transformers work). Alternatively, they could simply add z_s to the input embeddings (through a linear projection and addition) – but cross-attention is more flexible.

The **inputs to Depth** for generation are: initially, perhaps a special start token for the sequence (or simply an all-zero embedding). Then it generates the first token (which should correspond to the semantic code or text). Once the first token is generated, it’s fed back in (with its embedding) to generate token 2, and so on, until Q tokens are generated. In training, all Q tokens at time s are known, so they can be input together with causal masking to ensure token k only attends to < k within that time step. The Depth Transformer attends to z_s (which encodes all prior time steps context) and to already generated tokens of the current step. At token generation time, the Depth Transformer’s final layer outputs logits over the audio codebook vocabulary (size 2048) for that position. If the position is the semantic token (codebook 1), it chooses among 2048 semantic codes; if it’s an acoustic token position, likewise 2048 possibilities. Possibly, they could use different output projections for semantic vs acoustic codebooks, but it’s simpler if they share the same size (which they do – NA=2048 for all). If an inner text token is included as position 0, that would have logits over the text vocab instead, so likely they handle text prefix in the Temporal model rather than via Depth to avoid mixing modalities in one softmax. Indeed, the “text token as prefix to acoustic” is handled by Helium predicting a text token that then just gets aligned to time s (with a pad until audio starts) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=The%20original%20model%20is%20synchronized,in%20between%20each%20token%20enunciation)). So Depth mainly deals with audio codebooks.

Once Depth has generated all needed tokens for the system’s audio at step s, those tokens are handed to Mimi’s decoder to produce sound. Meanwhile, the user’s tokens at step s were given (from Mimi’s encoder), not generated by Depth. However, how does the model incorporate user tokens at the same step s? This is where the **multi-stream stacking** happens. In the simplest approach, one could treat the user tokens as _additional tokens to predict_ as well, but actually, the user tokens are observations, not predictions. In training, they incorporate user tokens into the input context for future steps, but you wouldn’t have the model predict them as a loss (since that would be like trying to do ASR). Instead, they likely feed user tokens as part of the input for the next time steps (s+1 and onward) via the Temporal Transformer embedding, but do not include them in the generated sequence at s. Alternatively, one can imagine an architecture where the model is trying to predict the future of both user and system streams – essentially modeling a joint distribution P(user, system). The paper’s formulation hints they model both streams jointly as well ([](https://kyutai.org/Moshi.pdf#:~:text=only%20models%20semantic%20tokens,is%20full%20duplex%20and%20can)) ([](https://kyutai.org/Moshi.pdf#:~:text=first%20generates%20all%20the%20semantic,semantic%20and%20acoustic%20tokens%20jointly)), which might mean the model _does_ have a loss on user stream tokens too (predicting what the user will say next). However, that would be odd for deployment, since we don’t want the model hallucinating user speech. It might have been done just for pretraining (like modeling general conversation flow). In inference, obviously, we override the user token stream with actual input (so we’re not sampling user tokens, we are feeding them from mic). This approach is akin to treating the user as part of a stochastic process that the model can also simulate if needed (like in synthetic data they _did_ simulate user speech by sampling from the model). For safety, let’s assume at inference we always replace the user stream with real input (if available), ignoring what the model would predict for user tokens. The HuggingFace interface reflects this: `user_audio_codes` are provided as input, not generated by the model, whereas `moshi_audio_codes` are generated ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20streaming%20auto,what%20the%20user%20said%2Fwill%20say)).

To summarize the architecture interaction: At each time step, Helium (Temporal) consumes the previous time’s combined token info and outputs context. Depth (with context) generates Moshi’s current audio tokens. User’s current audio tokens (if any) are just fed in externally. Then increment time and repeat. Both Helium and Depth are Transformers with causal masks (Helium masks future in time, Depth masks future in codebook order). They share no parameters except whatever ties might exist (none indicated, Helium and Depth are separate). The whole thing forms a single unified autoregressive model when viewed from a higher level – but implemented as two nested transformers.

### 4.2 Helium and Depth Transformer Details

**Helium (Temporal) Transformer Architecture:** As a typical decoder-only Transformer, Helium uses **multi-head self-attention**: $Attention(Q,K,V) = \text{softmax}(QK^T/\sqrt{d})V$. It has 32 attention layers, each with 32 heads of dimension 128 (so d_model=4096). It also has 32 feed-forward layers of dimension 11264 with GLU activation (two linear projections: one 11264→4096 gated by another 11264→4096 via elementwise multiplication after SiLU, per Shazeer 2020) ([](https://kyutai.org/Moshi.pdf#:~:text=of%204%2C096%20tokens%20and%20FlashAttention,We%20split%20all%20numbers%20into)) ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)). The Transformer uses **pre-normalization** (RMSNorm) – meaning each sub-block (attention or FFN) is preceded by RMSNorm on the input, and there’s an initial RMSNorm on embeddings and a final RMSNorm before output. Positional encoding is done through RoPE which multiplies the $q, k$ vectors by a rotation matrix based on their position index (it encodes position angles implicitly without adding to embeddings) ([](https://kyutai.org/Moshi.pdf#:~:text=original%20architecture%3A%20First%2C%20we%20use,2022%29%20for%20efficient%20training)). Helium’s output at a position is a 4096-d vector. For integration with Depth, they likely project this down if needed. In HuggingFace’s conversion, they indicate that **Moshi’s main decoder (Helium) is “strictly a classic text LLM”** with a language modeling head for text logits ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)). Indeed, during Helium pretraining, they attached an output softmax over the 32k text vocab. In Moshi, however, Helium is not directly producing final logits for audio – instead, its output is consumed by Depth. But Helium still can produce text if needed (e.g. for inner monologue text, Helium could directly output that as a normal LM output).

**Depth Transformer Architecture:** Depth’s 6 layers each have 16 attention heads (d_model=1024, so 16\*64 per head or similar). It also likely uses RMSNorm and RoPE (but context length = 8, which is tiny). However, Depth’s context length is actually the number of codebooks, which is fixed and small, so positional encoding might be trivial (it could even use a static positional embedding for positions 1..8 since that’s so small). The Depth Transformer must incorporate the Helium context vector. If using cross-attention, each Depth layer would have an additional attention that attends to z_s. But z_s is just one vector (or could be extended to one per layer of Helium or something). It might be simpler: some implementations of RQ-Transformer concatenate the context vector to the keys and values of Depth’s self-attention (so Depth heads attend not only to earlier depth tokens but also to z_s which is available as a key/value vector). Since Helium context is one per time step, they could repeat z_s across heads or project it to match head dim. Another approach: at Depth layer 1, add z_s to the self-attention output or feed-forward input in a gated manner. The exact implementation might vary, but conceptually Depth is **conditioned** on Helium’s output.

**Parameter Count & Efficiency:** Helium has ~7B parameters. Depth has much fewer: 6 layers of 1024-d – roughly (attention + FFN) per layer ~ (1024*1024*3*16 + 2*1024\*4096) = on the order of 50M or less. So Depth is ~50 million params, negligible compared to Helium. Mimi’s encoder/decoder conv layers and transformer bottleneck might add another ~50M. So Moshi in total might be ~7.1B parameters. That’s why it fits in ~14GB fp16.

One might wonder if Helium uses any parameter sharing or special structure. The paper doesn’t mention any parameter sharing between layers. They do mention using **LayerScale** in Mimi’s Transformers (with small initial weights) ([](https://kyutai.org/Moshi.pdf#:~:text=after,Both%20Transformers%20use%20causal%20masking)), but not in Helium. Helium’s training was standard next-token, which implicitly gave it world knowledge and reasoning. Depth’s training is tied to Helium’s (they train together in Moshi phases).

**Attention Mechanisms:** Helium’s attention is standard causal self-attention with flash-attn for speed ([](https://kyutai.org/Moshi.pdf#:~:text=model,Kudo%20and%20Richardson)). Depth’s attention might use **causal masking** along depth dimension (so token 2 can attend token 1 but not vice versa) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20an%20acoustic%20delay%20%CF%84,At%20inference%20time)). It likely also uses some mechanism to incorporate temporal context (z_s). Possibly they mask such that all depth tokens can attend an input representing z_s (like a “memory” at position 0 which is not masked). That would ensure every predicted codebook token sees the context.

### 4.3 Mimi Audio Codec Architecture

We have described Mimi conceptually; here’s a structured summary of its architecture:

- **Encoder:** 1D ConvNet (fully causal). The exact configuration from EnCodec is used with modifications. They mention cascaded residual blocks with **strides 4,5,6,8,2** at some layers ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L694%20striding%20fac%02tors,and%20dimension%20D%20%3D%20512)). Typically, EnCodec uses something like 5 conv blocks each with downsampling factors that multiply to the total downsample. The **SeaNet** architecture from Tagliasacchi et al. 2020 is referenced ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)) – that means they use residual units with dilated conv (for context) and weight normalization. Each residual block likely uses kernel size like 3 and has multiple dilation rates, similar to WaveNet’s structure, ensuring a receptive field covering ~250ms or more by the last layer. The final output of the encoder is a (time_frames x 512) matrix.

- **Pre-Quantization Transformer:** Before quantization, the 512-d sequence goes through an 8-layer Transformer encoder (causal) which has context of 250 frames (10s) ([](https://kyutai.org/Moshi.pdf#:~:text=after,Both%20Transformers%20use%20causal%20masking)). It uses 8 heads, model dim 512, FFN 2048, GELU activation ([](https://kyutai.org/Moshi.pdf#:~:text=after,Both%20Transformers%20use%20causal%20masking)). This encoder refines the latent representation by capturing long-term dependencies (like phoneme identity or repeated sounds) that convs might not unify.

- **Quantizer:** Instead of doing an 8-level RVQ sequentially, Mimi does a **split: one level + parallel 7-level RVQ**. The first quantizer takes the output of the transformer encoder and produces a 512-d vector which is then vector-quantized to nearest code (out of 2048) – this is the semantic token. Then the _residual_ (difference between the pre-quantization vector and the quantized vector reconstructed) is passed into the next stage, which is a 7-level residual VQ (like SoundStream) to produce 7 more codes ([](https://kyutai.org/Moshi.pdf#:~:text=quality%20metrics%2C%20while%20human%20evaluations,scale%20STFT)) ([](https://kyutai.org/Moshi.pdf#:~:text=parameters%20can%20be%20found%20in,removing%20reconstruction%20losses%20majorly%20degrades)). However, they mention an alternate approach: “we propose a split RVQ: rather than a single RVQ with 8 levels, we distill semantic info into a plain VQ and an RVQ with 7 levels in parallel. We sum their outputs, such that while both can be used for reconstruction, we remove the constraint that acoustic info should be in residual of the semantic quantizer” (paraphrasing from text around page 12 in the paper) ([](https://kyutai.org/Moshi.pdf#:~:text=quality%20metrics%2C%20while%20human%20evaluations,scale%20STFT)) ([](https://kyutai.org/Moshi.pdf#:~:text=parameters%20can%20be%20found%20in,removing%20reconstruction%20losses%20majorly%20degrades)). This implies that after obtaining the semantic quantized vector, they _also_ feed the original latent into a 7-level RVQ that works in parallel, not strictly on the residual. Then they sum the decoded contributions from semantic and acoustic part. This could preserve the semantic info in first codebook without forcing it to also reconstruct some of the signal. The exact implementation might be: let E = encoder output, T = transformer output. Compute Q0 = VQ(T) (semantic codebook). Also compute R = E (or T) fed into a normal 7-level RVQ (with smaller dimension like 256? uncertain). Then the decoder will take both Q0 embedding and the 7 RVQ embeddings summed to reconstruct audio. This would align with their statement that the distillation loss on Q0 conflicted with recon loss if done in the same pipeline, so splitting resolves that ([](https://kyutai.org/Moshi.pdf#:~:text=not%20compati%02ble%20with%20real,negligible%20computational%20burden)) ([](https://kyutai.org/Moshi.pdf#:~:text=first%20generates%20all%20the%20semantic,semantic%20and%20acoustic%20tokens%20jointly)). In any case, the final outcome is 8 tokens per frame.

- **Decoder:** The decoder essentially inverts the encoder. It likely takes the quantized embeddings (8 code vectors of 512/Q each or combined) and first passes them through a **post-quantization Transformer** (the second transformer module they added) with the same config (8 layers, 512-d) ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021)). Then it applies a series of transpose conv layers (stride 2,8,6,5,4 in reverse) with residual connections to upsample back to 24 kHz. The decoder’s conv are also causal (ensuring streaming decode). They mention using WeightNorm and nonlinearities (probably LeakyReLU or GELU) in these conv blocks too ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)).

Because Mimi is separate from Moshi during inference (it just feeds or takes tokens), one could replace Mimi with another codec if desired, but Mimi was optimized for this use. It runs at 12.5 Hz which conveniently matches typical phoneme rates.

**Computational Optimizations in Mimi:** They used vectorized operations for convs, maybe TorchScript for real-time. Mimi’s transformers use small context (250) so the cost is limited. They trained with a batch size 128 of 12s clips for 4M steps – which is extremely heavy (128*4M*12s = ~170k hours of audio seen) ([](https://kyutai.org/Moshi.pdf#:~:text=,codebook%20size%20of%20NA)) ([](https://kyutai.org/Moshi.pdf#:~:text=weights%20with%20a%20decay%20of,While%20the%20latent%20dimension%20is)) but not the full 7M hours presumably (so they likely did multiple epochs or a subset). The quantizer dropout (50%) means half the time they bypass quantization and feed continuous latents to decoder – implemented likely by randomly not quantizing some samples.

### 4.4 Input/Output Representations and Tokenization

**Text Tokenization:** Moshi uses a SentencePiece unigram model with 32k vocabulary trained on English text ([](https://kyutai.org/Moshi.pdf#:~:text=2020,We)). All text (prompts, transcripts, etc.) is encoded with this tokenizer. They handle numbers by splitting digits (so “2025” -> “2 0 2 5”) to ensure each digit is preserved (because a large number might not appear enough to get its own token) ([](https://kyutai.org/Moshi.pdf#:~:text=2020,We)). For Unicode characters or out-of-vocab items, it falls back to byte encoding (byte-level BPE) so nothing is truly out-of-vocab ([](https://kyutai.org/Moshi.pdf#:~:text=tokenizer%20is%20based%20on%20the,We)). This is standard to robustly handle any input text.

**Audio Token representation:** Each audio 80ms frame for each speaker is represented by up to 8 tokens: essentially an 8-dimensional token vector. In Moshi’s data pipeline, these are often handled as separate token IDs with a known grouping. One can imagine flattening it as 8 consecutive tokens in the sequence (which is what you’d do if not using the RQ-Transformer approach). But since they do RQ-Transformer, they treat them differently. The training code likely creates something like: for each time step s, create a structured token object that includes maybe a placeholder for a text token and Q tokens for each codebook for user and Q for system. They then linearize those for feeding into the model with special masking. In the HuggingFace conversion, they actually flatten everything and rely on the model architecture to know how to group them.

**Speaker differentiation:** The user and system audio tokens need to be distinguished. The architecture does this inherently by using separate input embeddings for each stream’s tokens. So, for example, the embedding table for “codebook 3 of Moshi” is distinct from “codebook 3 of User.” Thus, even if the token ID (0-2047) is the same number, the vector added to Helium is different. This effectively acts like a learned speaker embedding. Additionally, they could incorporate a special tag at the start of the sequence indicating who is speaking first, etc., but they didn’t explicitly mention using a role token. Instead, the network learns from context which stream corresponds to the AI vs user. In the synthetic data, they always assigned one stream as Moshi and one as user with random pick who’s first ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L1318%20For%20both,For)), so the model doesn’t overfit to always one pattern (they sometimes swapped which side starts) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L1318%20For%20both,For)).

**Combining Text and Audio in one sequence:** During inner monologue segments, text tokens (from the same 32k vocab) appear interspersed with audio tokens. To avoid confusion, they likely treated text tokens effectively as another “codebook” or stream in the model. However, since Helium is naturally outputting text, they might simpler: when Moshi is about to speak a word, they output the text token in the sequence. For alignment, they might insert padding tokens in between audio tokens to hold the place of text in the timeline. The HuggingFace guide gave an example: to align “Hello, I’m Moshi” with audio, they might represent it as `"Hello,<pad><unk>I'm Moshi"` ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=The%20original%20model%20is%20synchronized,in%20between%20each%20token%20enunciation)). This indicates they padded between words so that each text token aligns to roughly the moment that word is spoken. Possibly <unk> was used as a timing token. The details are a bit hacky, but the principle is: the model can output a text token at time s and no audio tokens at that exact moment, then audio tokens in subsequent steps. In practice, one can implement inner monologue by having a special “text stream” in parallel that only the system uses. The architecture diagram suggests “Text” comes out of Depth Transformer as well (they showed a separate orange square labeled Text coming from Depth conditioned on context). It’s possible they did extend Depth Transformer to also generate a text token at codebook slot 0 (like before the semantic codebook 1). If so, Depth’s first token could be from a different vocabulary (text vocab). This is tricky to do in one softmax. Alternatively, Helium might generate the text token itself and then they inject it. Given the complexity, perhaps they kept it simple: Helium deals with text, Depth deals with audio only. In fine-tuning with transcripts, they just trained Helium to output those transcript tokens at appropriate times (with some small architectural addition to align them).

Regardless, from an implementation perspective, one could implement inner monologue by having a **third stream** for Moshi’s text, which Depth doesn’t need to generate (since Helium’s output is basically those text tokens). At inference, you could run Helium one step further ahead to get a text token, then feed that as context while generating audio. This area is a bit advanced, but it’s not strictly necessary to replicate unless one is aiming for the exact fidelity – one could also choose to omit inner monologue (the model will still produce audio, just slightly less coherently).

**Prompt Engineering and Context Handling:** When starting a conversation, one can provide an initial text prompt or system message (e.g. “You are Moshi, an AI assistant. Have a conversation…”). This would be tokenized and fed as `input_ids` to Moshi with no audio tokens yet. Moshi was instruction-tuned, so it knows how to behave given an instruction like “Moshi: … User: … etc.” It’s unclear if Kyutai uses explicit role tokens or just relies on separate streams for roles. In text fine-tuning (OpenHermes etc.), they might have included prompts like “[System: ...][User: ...][Moshi: ...]”. If those were included, the model would expect some role indicator tokens. The documentation doesn’t mention special tokens, so they might not use any explicit token like “<|user|>” or such, relying purely on audio vs text channel to delineate roles. For safety, an implementer could include a system prompt text at start such as: “(The user speaks in audio. Moshi hears and responds in a friendly manner.)” – but since the model was fine-tuned on interactive data, it likely doesn’t need much prompting to behave. The open-source model presumably comes with some default persona (Moshi’s voice and style).

**Context management:** Moshi has a 4096 token context which includes both text and audio tokens. Since audio tokens come at 12.5 per second _per stream_, in one second you get up to 25 audio tokens (user+system) plus maybe a couple text tokens. So in one minute (~60 sec), that’s at most 1500 tokens – well within 4096. If a conversation goes on for more than ~4-5 minutes continuously, then the context may hit the limit. At that point, one strategy is **sliding window**: drop the oldest tokens as new ones come in. However, dropping blindly could remove important long-term context (like the conversation topic). Ideally, one would keep some summary of the history. Possibly, since Helium is a strong LLM, one could periodically insert a summary text token that compresses old dialogue (just speculation). The authors haven’t detailed this, but for real implementation, one may implement a rolling buffer. Most likely, 4096 is enough for normal interactions (few mins before a reset or concluding the session).

**Architectural Innovations Recap:**

- The **hierarchical RQ-Transformer** (Temporal + Depth) is a novel application in audio sequence modeling, drastically cutting sequence length and enabling full-duplex token handling ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)).
- The integration of **semantic token distillation** into a streaming codec (Mimi) is innovative – prior works like AudioLM used a separate semantic tokenizer that was not causal ([](https://kyutai.org/Moshi.pdf#:~:text=these%20acous%02tic%20tokens%20provide%20appropriate,conditioning%2C%20by%20using%20semantic%20audio)) ([](https://kyutai.org/Moshi.pdf#:~:text=prefix%20to%20predicting%20acoustic%20tokens,into%20the%20tokens%20produced%20by)). Mimi provides a unified token space that carries semantic info in a causal way.
- **Inner Monologue** as implemented is an extension of ideas from AudioLM (which used semantic tokens as intermediate) and SpeechGPT (which did text then speech in a chain) ([](https://kyutai.org/Moshi.pdf#:~:text=removal%20of%20explicit%20speaker%20turns%2C,to)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=contribution%20is%20the%20Inner%20Monologue%2C,linguistic%20information)). Moshi instead folds a text generation step into an otherwise speech model _without_ breaking streaming – this is quite new.
- The **multi-stream modeling** allowing overlapping speech is a significant departure from the turn-based dialogue assumption in virtually all prior dialog systems ([](https://kyutai.org/Moshi.pdf#:~:text=information%20that%20modifies%20meaning%E2%80%94%20such,of%20the%20user%20into%20parallel)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). This required creative training data generation and careful architectural design to avoid interference between streams.
- In terms of trade-offs: Moshi chooses a moderately sized backbone (7B) to balance reasoning and speed. Larger LLMs (like 70B) might produce more accurate answers but would be far too slow to run 12.5 steps/sec on current hardware. So 7B was a sweet spot for real-time; indeed they mention Moshi’s knowledge is somewhat limited on complex tasks or tool use ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=On%20Apple%20Mac%2C%20the%20model,integration%2C%20according%20to%20the%20company)), but it handles casual dialogue well. Another trade-off is the **audio codec bitrate**. They targeted 1.1 kbps, which is ultra-low. This keeps token rates low but means some audio quality loss (e.g., maybe the voice can sound a bit artificial or muffled at times). They did find training tricks to maximize quality at that bitrate, but still, to an audiophile, 1.1 kbps is low. In fact, the open model apparently runs at 4.4 kbps (32 codebooks) for better audio, at the cost of 4x more tokens ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=Although%2C%20here%20we%20find%20another,1kbps%20unreleased%20model)). So one could trade latency for quality by adjusting codebooks. The architecture allows that scaling: increasing Q (codebooks) improves fidelity linearly but also increases Depth gen time and token rate. They stuck with Q=8 for research, but open users might use Q=32 for nicer sound if their GPU can handle ~50 tokens/sec instead of 12.5.

- Another innovation: **adversarial-only training for codec** – not directly an architecture thing, but a training innovation that yielded very natural sound at low bitrates ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). And the use of **quantizer dropout** to effectively train one model that can operate at multiple bitrates (just drop some quantizers and it still works) means Mimi could be used in variable bandwidth scenarios.

In conclusion, Moshi’s architecture is a complex orchestration of components, each specialized yet working together. Implementing it from scratch would involve constructing the Helium and Depth Transformers as described, integrating Mimi or an equivalent codec for I/O, and carefully handling the multi-modal sequence formatting. The clear modularity (separate classes for Helium LM, Depth LM, Mimi codec) makes it feasible. HuggingFace’s integration demonstrates that it can be broken into `MoshiForCausalLM` (the Helium part), a `DepthDecoder`, and `MimiModel` for the codec ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=3)). They even provide a conversion script, meaning the architecture is now understood to the point it can be reimplemented in standard transformer libraries ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Tips%3A)).

## 5. Implementation Details

Reimplementing Moshi from scratch would require careful attention to engineering and correctness. In this section, we outline practical considerations: code structure, libraries, training regimen, and debugging techniques. We assume one has access to high-level descriptions (as above) and possibly the open-source repository as a reference. We’ll focus on how an expert might go about building Moshi’s components, validating them, and optimizing performance.

### 5.1 Code Structure and Frameworks

Kyutai’s implementation of Moshi is in PyTorch (with some Rust and MLX variants for deployment) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Today%2C%20we%20release%20several%20Moshi,in%20PyTorch%2C%20Rust%20and%20MLX)). A logical code structure might break the model into the following classes/modules:

- `HeliumModel` (or reuse an existing transformer decoder class): This would be configured with `n_layers=32, d_model=4096, n_heads=32, d_ff=11264`, RMSNorm, RoPE, etc., matching Helium. One could inherit from a GPT-Neo or LLaMA model class in HuggingFace and just adjust layer sizes and activation functions (HuggingFace’s integration mentions similarity to their `Gemma` model, which is likely an internal name) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)). The tokenizer (SentencePiece) can be handled via the `sentencepiece` Python library or HuggingFace’s `AutoTokenizer` if the model is added to the hub.

- `MimiCodec` class: This would include `MimiEncoder` and `MimiDecoder` submodules and implement `encode(audio_waveform) -> audio_tokens` and `decode(audio_tokens) -> waveform`. There is a HuggingFace `MimiModel` that likely does exactly this encoding/decoding pipeline ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20is%20a%20high,end%20fashion)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Use%20the%20following%20code%20to,install%20the%20required%20Python%20packages)). An implementer can study Meta’s `EncodecModel` in the audiocraft library as a starting point, then incorporate the modifications (extra Transformer, semantic distillation). For training from scratch, one would need a dataset of raw audio and the WavLM model to distill from (WavLM-Large is available from Microsoft). Losses for mel spectrogram and adversarial require integrating a multi-scale discriminator – the Facebook Audiocraft repository provides a good template for that, as it implements SoundStream/EnCodec training.

- `DepthTransformer` class: A custom Transformer decoder that has cross-attention to a provided context vector. Alternatively, one can implement it as a decoder where the “encoder keys/values” are just one vector (z_s) repeated or projected. In code, you could hack this by setting up an encoder with one token, but easier is to implement an attention mechanism that concatenates z_s to the self-attention context. Given its small size, this is not too complex. Each Depth generation step is short (≤8 tokens), so it might be acceptable to loop through generating each token. HuggingFace’s approach possibly unrolls it as a single forward with causal mask (since generate() can handle up to fixed length K easily).

- `MoshiModel` wrapper: This would tie Helium and Depth together. For training, one would probably write a custom forward that takes in a batch of data structured as (text_tokens, user_audio_tokens, moshi_audio_tokens) and computes the joint loss. The forward pass might do:

  1. Embed the inputs for the previous time steps (taking user tokens and moshi tokens from time 0 to s-1 as context) for Helium.
  2. Run Helium to get z_s for current step (or rather run it for all time steps in sequence if training with teacher-forcing – possibly they unroll the entire sequence using teacher forcing across time).
  3. Feed each z_s and the ground-truth audio tokens at time s into Depth to get logits for those tokens.
  4. Compute loss.
     This can be done sequentially for each time or using clever masking to do it in parallel. In fact, one could fold time into batch dimension for training to parallelize, but managing the state might be hard. The paper suggests they might treat each time step as sequential in training as well, which is slower. However, since they did 1M steps of pretraining, probably they found a way to parallelize across time to some extent. The RQ-Transformer paper (Lee et al. 2022) might have techniques for that. Alternatively, they may simply treat the entire conversation as one long sequence of tokens (with an ordering that places codebook tokens after each time’s text token) and train it like a standard LM – this would be simpler code-wise (no special training loop), but you’d have to pad tokens to align and do masking to prevent mis-ordering. Actually, one can do this: flatten the sequence in order: for s from 0 to S:
     - Insert Moshi text token (if any) with certain mask,
     - Insert Moshi semantic token,
     - Insert Moshi acoustic tokens,
     - Insert user semantic token,
     - Insert user acoustic tokens.
       Then apply a custom attention mask that ensures tokens at time s only attend to tokens ≤ s and, within time s, each acoustic token attends earlier ones (so a block of masks). This is a single sequence training. The complexity is constructing that mask and indexing embeddings properly. The authors likely did something along those lines. They denote formulas in the paper for stacking sub-sequences and applying masks ([](https://kyutai.org/Moshi.pdf#:~:text=Vs%2C2%20%3D%20As%2C1%20semantic%20tokens,Vs%2C1%2BQ%2B1%20%3D%20A%E2%80%B2%20s%2C1)).

  In implementation terms, doing a single sequence would allow using standard Transformer implementations if you can pass a custom attention mask matrix of shape (T_total, T_total). But T_total (like 30k for 5 min audio) might be large to handle as one sequence with full attention, which defeats the purpose of the hierarchy. So perhaps they did not flatten fully, they truly used the two-transformer approach in code. They might run the Temporal transformer across S steps (which is like running an RNN or using caching in autoregressive loop because each step depends on the last). In training, they might not backprop through the entire unrolled 30k length due to memory – they might use truncated BPTT or treat each step independently except through hidden state (which in a Transformer is complicated – you can’t easily get hidden state without running from start anyway). It’s intricate. For a reimplementation, one might simplify by training it as a standard LM on a flattened representation with masking. Given that HuggingFace has a conversion, they likely flattened the positional encoding scheme or used some trick.

- **Libraries and Dependencies:**
  - PyTorch for model building and training.
  - HuggingFace Transformers could be used to quickly get a GPT model for Helium and then modify it.
  - SentencePiece for tokenizer.
  - For audio: `torchaudio` can load audio files, and could compute mel-spectrograms for loss.
  - The adversarial training would need a discriminator: possibly reuse the MultiScaleDiscriminator from torchaudio or audiocraft.
  - WavLM model for distillation (the HF hub has `microsoft/wavlm-large`).
  - FlashAttention: can be integrated via the `flash_attn` library or using PyTorch 2.0’s scaled_dot_product_attention (though that might not be as fast for long sequences).
  - Opting for BitsandBytes (8-bit weights) for inference can reduce mem usage on GPU.

Given complexity, one might not train from scratch but use Kyutai’s weights. But an expert implementing from scratch might test pieces individually (e.g. train Mimi alone first, then freeze it; train Helium or use an existing LM like LLaMA as Helium if they want to shortcut, etc.).

### 5.2 Validation and Testing

Implementing such a model requires extensive testing at each stage:

- **Unit tests for Mimi:** After training Mimi codec, one should verify that encoding+decoding yields audio close to original (subjectively and via metrics like PESQ or STOI). One can also encode some speech and listen to confirm quality, and that the compression matches expected bitrate (e.g. ensure output tokens length = 12.5 Hz, code usage across quantizers looks reasonable). If implementing WavLM distillation, one can verify that the first quantizer’s output is predictive of phonetic content by doing an ABX test as they did ([](https://kyutai.org/Moshi.pdf#:~:text=Mimi,based%20ABX%20%28Schatz%20et%20al)) or simply checking that feeding the tokens into a KNN classifier on phoneme labels yields high accuracy.

- **Transformer integration tests:** Helium alone can be validated by ensuring it can overfit a small corpus (say a few sample texts) and produce sensible text. Also, one might load existing LLaMA-7B weights into Helium’s architecture (since it’s nearly the same) to verify the code works with known outputs. Depth Transformer can be unit-tested on synthetic data: for example, fix a random context vector and train Depth to output a known pattern of tokens – see if it can learn to do that.

- **End-to-end no-audio testing:** Before integrating audio, one might test Moshi in a text-only mode. If we remove Mimi and treat it purely as a text generator (like just chat with Helium), does that part function? Helium after pretraining should produce fluent text. Then gradually add audio: e.g. fix a simple codec (maybe a trivial one-hot per letter spoken) to test multi-stream mechanism.

- **Overfit a toy conversation:** A good debugging step is to create a very short synthetic “conversation” (like one or two time steps) and train Moshi to reproduce it. For instance: user says “Hello” (maybe 1 second of audio), Moshi replies “Hi”. Construct a single training example where user audio tokens correspond to some fixed pattern, Moshi audio tokens correspond to another fixed pattern. Train for a while to see if model learns to output those tokens when prompted with user’s. This ensures the multi-stream forwarding and loss are correct.

- **Performance testing:** Ensure that the iterative generation can actually run in real-time. For instance, simulate streaming by feeding audio from a file to the model and measure latency. Optimize any bottlenecks (maybe use caching for Helium’s KV states across time steps so you don’t recompute all previous attention at each step – Transformers support caching key/values for past tokens in autoregressive generation; Helium in inference mode will use that to only compute new attention for new step, which is critical).

- **Debugging techniques:** Visualization can help – for example, one can track attention weights of Helium to see if it focuses appropriately on recent tokens vs older; or check the distribution of predicted audio tokens – if Moshi is spitting out the same audio token repeatedly, something’s wrong. Also, monitor the loss for each component: perhaps log the text token loss separately from audio token loss to ensure the model isn’t ignoring one. Using small portions of real speech data (like just 1 minute from Fisher) to train a tiny model can reveal if the data formatting is correct (does loss decrease, do generated tokens match patterns, etc.).

Because Moshi is generative and multi-modal, qualitative testing (listening to outputs) is crucial. For example, after training on synthetic scripts, play back a generated conversation: does it align with expectations? If Moshi starts speaking before user even speaks or talks over itself, maybe the training or streaming scheduling has an issue.

### 5.3 Dependencies and Engineering Challenges

**Memory management:** Training with 4.2M token text batches and 16h audio batches is huge. They likely used gradient accumulation and multi-node training. An implementer might not replicate those sizes; one could scale down (e.g. 1M tokens, 1h audio per batch on 8 GPUs). Using mixed precision (fp16 or bfloat16) is a must. For inference, as noted, quantization could be applied (though the official PyTorch version hasn’t, one could apply 4-bit quant to Helium using something like GPTQ if needed, at some quality cost).

**Synchronization of streams:** One tricky part of implementation is ensuring that the user and system token streams remain time-synchronized during generation. The model expects inputs aligned. In a streaming loop, this means you can’t let the model get too far ahead or behind. For example, if user hasn’t said anything for some steps, you may be feeding “blank” user tokens (maybe zeros or a special silence token) to keep step count equal ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,are%20synchronized%20with%20the%20audios)). The HF guidance suggests padding with zeros for absence of user input ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=You%20can%20dynamically%20use%20the,what%20you%20want%20to%20test)). Indeed, if user is silent, you feed a zero tensor as user audio for that step, so the model knows “no user speech”. Similarly, if Moshi is not supposed to speak at some moment, probably a special no-output token or just not generating new tokens works. The training likely included such silent periods (explicitly or via zeros). So an implementer must decide on how to represent “no token this frame” – possibly they have a special codebook entry that corresponds to silence for acoustic tokens.

**External integration:** The final system needs audio I/O (microphone capture and speaker playback). In Python, one can use `sounddevice` or `pyaudio` for streaming audio. The Rust implementation likely handles audio I/O more robustly. But those are outside the model scope; still, for a complete system test, hooking those up is needed.

**Reproducibility:** If implementing from scratch for research, one might want to reproduce the results in the paper. That means re-training Helium on a huge dataset etc., which is extremely compute-intensive. Instead, one could take an existing pretrained LLM (like LLaMA2-7B) and fine-tune it as Helium (saving time). Similarly, one could fine-tune EnCodec as Mimi with distillation. This would drastically reduce cost to perhaps a few hundred GPU-hours instead of tens of thousands. The open source release provides weights that could be loaded to verify one’s code.

**Debugging tricky issues:** Because the model is novel, one might encounter odd issues – e.g., training might diverge (especially the GAN part for Mimi). To debug that, one can start with reconstruction loss only then introduce adversarial gradually (as they likely did baseline vs advanced in paper). Another potential issue: mismatch between training and inference processes. If the model was always trained to have some text token before audio, but at inference you do differently, you might get distribution shift. Ensuring that the generation procedure matches how training data was structured is key (including any delays or special tokens). The authors mention “we illustrate how delaying audio vs text tokens yields TTS or ASR” ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)) – meaning in training they probably did experiment with various fixed delays between text and audio. Reimplementers should be mindful of those alignments.

### 5.4 Performance and Debugging Tools

- Using a profiler (like PyTorch’s autograd profiler or Nsight Systems) can identify if any part of the forward is slow. The most expensive part is Helium’s attention. If one doesn’t use FlashAttention, that could be slow for 4096 context. So integrating an efficient attention (maybe PyTorch 2’s scaled_dot_product_attention with flash enabled) is important.
- Depth Transformer is trivial by comparison; Mimi encoder/decoder are conv-heavy but 512 dims at 12.5 Hz is not too bad (some 5k length conv ops).
- If running on CPU for testing, one might disable the Depth loop and just test Helium outputs. But for final system, GPU is needed.

- Logging and visualization: Logging the waveforms or spectrograms of generated output vs reference can show if model is generating reasonable audio. For instance, one could feed a test user utterance and have Moshi respond, then check if the content of response matches expectations (maybe transcribe Moshi’s output with an external ASR to verify its word accuracy – ironically using Whisper on Moshi’s output is a good automated way to measure intelligibility and whether it’s staying on script).

In summary, implementing Moshi is a complex but tractable project: it involves combining techniques from NLP (Transformer LLMs) and Speech (neural codecs, ASR/TTS). The code will span data loading (both text and audio), multi-task training loops, and integration of different loss functions. An expert engineer would leverage existing tools as much as possible (for tokenization, base models, etc.) and only custom-build the novel bits (Depth transformer and the multi-stream data collator). Careful stepwise testing, as described, would be essential to arrive at a functioning model.

## 6. Ethical and Safety Considerations

Building and deploying a system like Moshi raises important ethical questions and challenges. As an AI that can **listen and speak in real time**, Moshi combines the usual NLP concerns (like harmful content generation, bias, misinformation) with new issues related to voice and audio (such as impersonation, audio deepfakes, privacy of voice data). Kyutai Labs acknowledged these concerns and took several steps to mitigate risks ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)) ([](https://kyutai.org/Moshi.pdf#:~:text=6%20Safety%20In%20parallel%20with,this%20section%2C%20we%20specifically%20consider)).

**Toxic Content and Harassment:** Like any language model, Moshi could produce inappropriate or hateful speech if not properly aligned. To address this, the final fine-tuning phase included **safety-oriented dialogues** – for example, conversations where the user requests disallowed content and Moshi is supposed to refuse ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)). By training on these, Moshi learns to respond with refusals or safe completions when prompted with e.g. hate speech or NSFW queries. The paper mentions generating conversations where _"the user asks unethical or NSFW questions, and Moshi refuses"_ ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)). This aligns Moshi with a kind of content policy similar to ChatGPT’s. Additionally, they evaluated Moshi on a toxicity benchmark (the **ALERT** dataset) across categories like hate, self-harm, violence, etc. ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2761%20evaluates%20safety,The)) ([](https://kyutai.org/Moshi.pdf#:~:text=evaluates%20safety%20under%20multiple%20categories,The)). Moshi’s overall safety score was in the mid range compared to other LLMs – not perfect, but not the worst ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2768%20Overall%20Safety,table%20in%20terms%20of%20rank)) ([](https://kyutai.org/Moshi.pdf#:~:text=Overall%20Safety%20Score%2083,table%20in%20terms%20of%20rank)). The analysis indicates Moshi generally does **not generate toxic content** and stays consistent in persona ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L3146%20while%20displaying,suite%20of%20models%20and%20recipes)). However, users did note in informal testing that Moshi’s personality can be a bit **“rude” or abrupt** at times ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=%E2%80%9CI%20find%20the%20Moshi%20model,worthy%2C%20quipped%20Karpathy)) – this may be an artifact of the training dialogues or the nature of overlapping conversation (it might interrupt more than an ideal polite assistant). This is a subtle safety point: _conversational etiquette_. If Moshi frequently interrupts or ignores user queries, it could lead to user frustration (as one tester commented they “almost lost patience” due to frequent interruptions ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=confusing%20but%20also%20very%20funny,worthy%2C%20quipped%20Karpathy))). While not a catastrophic risk, it shows the challenge of balancing full-duplex freedom with politeness. Further fine-tuning or rules might be needed to refine this behavior (e.g. encourage Moshi to wait slightly longer before cutting in, or to use phrases like “sorry to interrupt”).

**Bias and Fairness:** The underlying training data (text from the web, audio from various sources) likely carries biases (cultural, gender, etc.). Helium’s text corpus was filtered for quality, but not explicitly for bias or ideology, so it might have the same biases as a general web-scraped model. The synthetic fine-tuning might inadvertently amplify certain biases depending on the scripts. For example, if OpenHermes dialogues had certain stereotypes or less diversity, Moshi could reflect that. They did not detail bias testing, but an ethical implementer should be mindful of outputs related to sensitive attributes. It might be wise to do adversarial prompts to see if Moshi exhibits bias (e.g., how does it respond to different accents or dialects? does it assume genders or ethnicities in scenarios?). The **audio domain** adds bias aspects too: if Mimi was trained mostly on certain accents (say American English), Moshi might struggle with others or transcribe them incorrectly internally. The synthetic data tried to include accent variety for the user voice ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=our%20own%20models%3A%20Helium%20writes,sure%20Moshi%20stays%20in%20character)), which helps, but bias could still occur (like preferring certain speech patterns as “normal”). Evaluating Moshi with users of different backgrounds and making adjustments (via further fine-tuning or prompting to be inclusive) is important.

**Privacy:** Since Moshi processes live audio, there are privacy considerations: user speech content is very personal (could include names, health info, etc.). Running Moshi locally (on-device) mitigates many privacy issues because data need not leave the device ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=Overall%2C%20it%20provides%20a%20compelling,still%20has%20room%20for%20improvement)). This is a big advantage over cloud-based voice assistants. However, if Moshi is deployed in a cloud or as a public demo, developers should ensure the audio and transcriptions are handled securely (e.g., encrypted transmission, not stored without consent). Another aspect is **bystander privacy**: Moshi could potentially pick up background voices not intended for it. If running on a phone or smart speaker, it might inadvertently record others. Care must be taken to have a clear “activation” or user consent for recording – albeit Moshi tries to eliminate the need for a wake word by continuously listening, that is double-edged ethically. Continuous listening devices raise privacy flags, so an application might still choose to have a push-to-talk or wake word to delineate when it’s actively listening.

**Impersonation and Deepfake Voice:** Moshi’s voice output is currently a single fixed AI voice. Kyutai intentionally kept Moshi’s voice constant and even in open-source replaced it with two synthetic voices to avoid copying a real person’s voice ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=our%20own%20models%3A%20Helium%20writes,sure%20Moshi%20stays%20in%20character)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)). This is crucial because if Moshi’s voice sounded like a specific individual, one could misuse it to impersonate them. By using an artificial voice (and releasing two obviously synthetic voices), they reduce the immediate risk of voice mimicry. However, the technology could be adapted to clone voices – one could fine-tune Mimi’s decoder to a target voice, or prompt the model to speak in a different style if it had multi-voice capability. That could be used maliciously (fake audio recordings). Thus, limiting voice cloning features and watermarking outputs can be considered. Since Moshi is CC-BY licensed and open, others could attempt to modify it for multi-voice. That’s an ethical risk inherent in open-source speech models. The community should discourage misuse, and perhaps incorporate detectors for synthesized audio (audio deepfake detection is a developing field).

**Non-linguistic sounds and Emotional Content:** Moshi can output not just words but any sound in theory (because acoustic tokens can represent laughter, sighs, etc.). This is powerful but can be misused (e.g., generating realistic sounds of gunfire or screams in a prank scenario). On the other hand, it allows positive uses like more natural, empathetic responses (laughing at a joke, etc.). Ensuring Moshi doesn’t output very startling or dangerous sounds unexpectedly is part of safety. Possibly some filter on output type (like disallow extremely loud or certain wave patterns) could be implemented. Also, emotional manipulation is a risk: with a human-like voice, users might develop trust or attachment (the ELIZA effect). Designers should make clear Moshi is an AI, to avoid deception or over-reliance by vulnerable users. If Moshi gives advice (like medical or legal), that’s another area – it should have been fine-tuned to provide disclaimers or correct itself, but it’s only as good as its data. The paper noted Moshi is not strong at complex tasks or tool use ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=On%20Apple%20Mac%2C%20the%20model,integration%2C%20according%20to%20the%20company)), so deploying it in high-stakes scenarios could be problematic.

**Audio Safety:** The paper points out that **audio safety is less developed** compared to text safety ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2749%20toxicity%20problems,apple%20comparison)). For example, detecting hate speech in audio is harder than in text. Moshi sort of bypasses that by converting audio to tokens (so it can potentially detect if the user’s audio tokens map to a slur or something?), but it wasn’t trained to moderate user input. If a user says hateful things, Moshi likely will respond with something (maybe it refuses if it recognizes the request as unethical). But understanding hate in audio without transcription is tough. Possibly, the inner monologue text acts as a de-facto transcript for Moshi’s own speech, which they used to evaluate toxicity of Moshi’s outputs (they likely transcribed Moshi’s speech via the model itself or external means to check content) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2761%20evaluates%20safety,The)) ([](https://kyutai.org/Moshi.pdf#:~:text=Appendix%20E,Values%20in%20the%20last%20row)). They show an overall safety score table, implying they compared Moshi’s output text to others. It scored moderately (like ~85 out of 100) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2768%20Overall%20Safety,table%20in%20terms%20of%20rank)) ([](https://kyutai.org/Moshi.pdf#:~:text=Overall%20Safety%20Score%2083,table%20in%20terms%20of%20rank)), meaning it does pretty well but not perfect. As implementers, it’s wise to also include a final safety layer: e.g. run Moshi’s intended text output through a filter for banned phrases before synthesizing, to catch anything severe that slipped through. Similarly, for user input, one could have a hotword detector (like if user is screaming an obscenity, maybe Moshi should not cheerfully mirror that).

**Consistency and Voice Stability:** They highlight Moshi remains _consistent in its voice_ ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L3146%20while%20displaying,suite%20of%20models%20and%20recipes)). That is good from a trust perspective – it doesn’t suddenly change tone or identity mid conversation. They also presumably ensured the training data kept Moshi’s persona stable (the synthetic conv always had Moshi as a particular style). This avoids confusion and potential catfishing-type issues where the AI alters its style to manipulate user.

**Open-Source vs Proprietary:** Kyutai’s move to open-source Moshi is ethically notable – it allows transparency and community oversight. Anyone can inspect how it was trained and test for biases or issues. This is arguably safer in the long run than a closed model, because problems can be identified and addressed by many. However, open-source also means bad actors could use it for nefarious purposes (like running it on spam calls to scam people, etc.). Mitigating that is challenging. The CC-BY license and community guidelines hopefully discourage misuse. Tools might be developed to detect Moshi’s synthetic voice if used in the wild (maybe spectral signatures). Moshi’s voice currently probably sounds obviously synthetic to a careful listener (a bit monotonic or “AI-ish”), which might actually be a safety feature because it’s identifiable as AI. As the tech improves, synthetic voices become indistinguishable, raising stakes for imposters.

**User Data and Adaptation:** If one were to adapt Moshi with user-specific data (like fine-tuning it on a particular user’s conversations to personalize it), that could raise privacy issues because the model might memorize and regurgitate personal info. Moshi’s base was trained on public data, and fine-tunes on synthetic or public sets (Fisher is public), so it likely doesn’t have private info. But if it’s further trained on logs from real users, care must be taken to anonymize or limit memorization. The team did not mention training on any private or proprietary transcripts, which is good ethically and legally.

In conclusion, while Moshi breaks new ground in conversational AI, those deploying or modifying it must heed these ethical considerations:

- **Ensure the model is used in consented settings** (don’t eavesdrop without permission).
- **Limit misuse potential** (no multi-voice impersonation features by default).
- **Apply safety fine-tuning and filters** to avoid harmful speech.
- **Be transparent** that it’s an AI system to users.
- Continuously **evaluate** its behavior on sensitive prompts and iteratively improve the safety training (red-teaming it to find failure modes).

Kyutai’s initial results show Moshi is reasonably safe – it refuses blatantly problematic requests and doesn’t output toxicity on its own ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)) ([](https://kyutai.org/Moshi.pdf#:~:text=while%20displaying%20satisfying%20levels%20of,suite%20of%20models%20and%20recipes)). However, like any large generative model, it’s not perfect and can have unpredictable outputs. Ongoing monitoring and community feedback (the open-source community can report issues on the GitHub) will be important to address ethical issues as they arise.

---

**Sources:**

1. Alexandre Défossez et al., _“Moshi: a speech-text foundation model for real-time dialogue”_, arXiv preprint 2410.00037 (Oct 2024). ([](https://kyutai.org/Moshi.pdf#:~:text=We%20introduce%20Moshi%2C%20a%20speech,Finally%2C%20they%20rely%20on)) ([](https://kyutai.org/Moshi.pdf#:~:text=Monologue%E2%80%9D%20method%20significantly%20improves%20the,labs%2Fmoshi)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20speech,time)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=altogether%20by%20casting%20spoken%20dialogue,but%20we%20also%20illustrate%20how))

2. Kyutai Labs Blog, _“Moshi open-source release: run Moshi locally!”_ (Sept 18, 2024). ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment))

3. Hugging Face Transformers Documentation – _Moshi model_ (Nov 2024). ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20deals%20with%203%20streams,of%20information)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20streaming%20auto,what%20the%20user%20said%2Fwill%20say))

4. Kyutai Labs GitHub Repository – _Moshi Code_ (2024). ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=2)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=MoshiForConditionalGeneration))

5. Marcello Castellaci, _“Exploring Mimi”_ (Oct 4, 2024) – blog post analyzing Mimi codec. ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=1,5Hz)) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,are%202048%20possible%20audio%20tokens)) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=Although%2C%20here%20we%20find%20another,1kbps%20unreleased%20model))

6. Analytics India Magazine, _“Kyutai Launches Moshi, an Open Source Voice Model…”_ (Sept 2024). ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=How%20Moshi%20Works%3F)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=It%20operates%20as%20a%20full,200%20milliseconds%20on%20L4%20GPUs)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20company%20said%20that%20they,installed%2C%20including%20the%20nvcc%20compiler)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=%E2%80%9CI%20find%20the%20Moshi%20model,worthy%2C%20quipped%20Karpathy))

7. Moshi Technical Report – Appendix and training details. ([](https://kyutai.org/Moshi.pdf#:~:text=of%20our%207B,custom%20dataset%20built%20from%20synthetic)) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)) ([](https://kyutai.org/Moshi.pdf#:~:text=,codebook%20size%20of%20NA)) ([](https://kyutai.org/Moshi.pdf#:~:text=Quantization%20rate,We%20moreover%20follow))

8. Moshi Technical Report – Architecture figures and descriptions. ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20sep%02arate%20encoders%20represents%20a,com%2Fchongo%2Ftech%2Fcomp%2Ffnv%209)) ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021))

9. Moshi Technical Report – Safety analysis. ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2761%20evaluates%20safety,The)) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L3146%20while%20displaying,suite%20of%20models%20and%20recipes))
