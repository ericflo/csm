# Moshi Architecture and Components

**Introduction:** Moshi is a recently introduced speech-to-speech foundation model by Kyutai Labs that enables real-time, full-duplex spoken dialogue ([](https://kyutai.org/Moshi.pdf#:~:text=We%20introduce%20Moshi%2C%20a%20speech,Finally%2C%20they%20rely%20on)) ([](https://kyutai.org/Moshi.pdf#:~:text=also%20illustrate%20how%20it%20can,labs%2Fmoshi)). Unlike traditional voice assistants that pipeline ASR (speech recognition), NLP, and TTS (speech synthesis), Moshi treats conversation as a single end-to-end process: it directly **generates speech from speech**, using text only as an internal intermediate representation ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,the%20modeling%20of%20arbitrary%20conversational)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20speech,time)). This unified approach preserves non-verbal cues (like emotion and intonation) and allows truly interactive conversations with overlapping speech (no fixed turn-taking) ([](https://kyutai.org/Moshi.pdf#:~:text=their%20complex%02ity%20induces%20a%20latency,text%20language%20model%20backbone%2C%20Moshi)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=This%20allows%20for%20the%20removal,of%20160ms%2C%20200ms%20in%20practice)). Moshi’s design is powered by three primary components: a **text language model backbone (Helium)** for language understanding/generation, a **neural audio codec (Mimi)** for compressing/decompressing speech audio into discrete tokens, and a **multi-stream Transformer architecture** (Temporal and Depth Transformers) to model the simultaneous audio streams of user and system in a hierarchical fashion ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)). We provide an in-depth technical breakdown of Moshi’s architecture, training regime, real-time inference process, model topology, implementation details, and safety considerations – all the information an expert ML engineer would need to reimplement Moshi from scratch.

## 1. Understanding Moshi’s Architecture and Components

([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/)) _High-level architecture of Moshi’s full-duplex dialogue model. The Helium Temporal Transformer (7B) autoregressively models time steps (at 12.5 Hz), while a smaller Depth Transformer generates the multiple audio codec tokens per time step (semantic & acoustic tokens). The user’s incoming audio is encoded by Mimi into discrete tokens (purple) fed into the model as one stream, and Moshi’s own outgoing audio tokens (orange) are generated simultaneously on another stream. An “Inner Monologue” text token (if any) may be produced as a prefix to Moshi’s audio tokens, improving linguistic quality (see §3.4.4 in paper). Moshi’s audio output tokens are decoded by Mimi back to speech audio. This architecture enables processing of two audio streams concurrently with minimal latency._ ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=It%20operates%20as%20a%20full,200%20milliseconds%20on%20L4%20GPUs))

## Helium: Text Language Model Backbone (7B LLM)

**Architecture:** Helium is a 7-billion-parameter autoregressive language model that forms the core “brain” of Moshi ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=Moshi%20comprises%20three%20key%20elements%3A,and%20Moshi%20on%20separate%20channels)). It uses a Transformer decoder architecture similar to recent large language models (comparable to LLaMA-7B) with several modern tweaks for efficiency and performance ([](https://kyutai.org/Moshi.pdf#:~:text=Helium%20is%20an%20autoregressive%20language,a%20context%20length)) ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)). Helium adopts **RMSNorm** normalization layers (instead of LayerNorm) at the input of attention/FFN blocks and output, **Rotary Positional Embeddings (RoPE)** for 4,096 token context, and **Gated Linear Unit (GLU)** feed-forward layers with SiLU activation ([](https://kyutai.org/Moshi.pdf#:~:text=Helium%20is%20an%20autoregressive%20language,a%20context%20length)) ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)). These choices follow best practices from contemporary LLM research – e.g. RMSNorm and RoPE help stable training at long context lengths, and GLU (a gated feed-forward such as SwiGLU) improves parameter efficiency and learning capacity. The model dimension is 4096 with 32 Transformer layers and 32 attention heads, and feed-forward hidden size ~11k (11264) ([](https://kyutai.org/Moshi.pdf#:~:text=Hyper,Learning%20rate%203%20%C2%B7%2010%E2%88%924)), consistent with a 7B parameter scale. A SentencePiece unigram tokenizer with a vocabulary of 32,000 subword tokens (primarily English) is used for text; it includes byte fallback to avoid unknown tokens and splits numbers into individual digits to preserve information ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)).

**Training Data & Objectives:** Helium was pretrained on an extremely large text corpus of **2.1 trillion tokens** of English text ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)). Kyutai curated high-quality sources like Wikipedia, StackExchange, and a large collection of scientific articles, then augmented with filtered CommonCrawl web data to reach the required scale ([](https://kyutai.org/Moshi.pdf#:~:text=Training%20data%20is%20one%20of,See%20more%20details%20on%20data)). The data pipeline involved aggressive deduplication (using hash-based filtering and bloom filters), language ID filtering for English, and a **quality filtering** classifier that scored pages (favoring those related to high-quality domains like STEM or humanities) ([](https://kyutai.org/Moshi.pdf#:~:text=obtain%20a%20large%20and%20high,quality)). This ensured Helium’s text training data is rich and diverse while minimizing low-quality or toxic content. Training Helium to convergence required on the order of 500k optimization steps with very large batches (4.2M tokens per batch) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)) – achieved via distributed training on GPU clusters. Optimization used the **AdamW** optimizer with a fixed learning rate and cosine decay schedule ([](https://kyutai.org/Moshi.pdf#:~:text=digits%2C%20and%20use%20byte,7)). Helium’s final perplexity and language understanding are strong, providing Moshi with a solid foundation in linguistic knowledge and reasoning ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%20is%20built%20on%20top,We%20also)).

**Role in Moshi:** In the Moshi architecture, Helium serves as the **Temporal Transformer** – it processes the dialogue history (in text and audio-token form) over time and produces latent context embeddings that will be used to generate new outputs ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)). Essentially, Helium is responsible for understanding the conversational context and deciding _what_ to say next (at the semantic/text level), leveraging its LLM capabilities. However, rather than directly outputting text, Moshi uses Helium’s next-step embedding to condition a secondary model that will produce audio tokens (speech) for that step. This allows the model to **speak in audio** while still thinking in terms of language. Helium’s large capacity enables Moshi to carry on coherent, contentful dialogues – it provides the language backbone that ensures responses are contextually relevant and logically sound ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%20is%20built%20on%20top,We%20also)). Helium was even fine-tuned on dialogue-style data (e.g. OpenHermes synthetic dialogue and real transcripts) to better handle interactive conversations ([](https://kyutai.org/Moshi.pdf#:~:text=bullet%20points%2C%20long%20enumerations%29,then%20synthesize%20them%20with%20our)), which helps Moshi produce realistic conversational behavior.

## Mimi: Streaming Neural Audio Codec

**Purpose:** Mimi is Moshi’s **neural audio codec**, responsible for converting raw speech waveforms into discrete token sequences (and back). It compresses audio at 24 kHz into a low-bitrate sequence of codec tokens that the language model can handle ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Mimi%20is%20a%20neural%20audio,tokens%20for%20a%20streaming%20Transformer)). Without Mimi, Moshi would have to deal with raw audio signals directly, which is intractable for an LLM – Mimi provides a learned discrete representation of audio that is far more compact while preserving speech content and quality.

**Codec Structure:** Mimi follows the general design of neural codecs like **SoundStream/EnCodec** ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,audio%20compression%20at%20low%20bitrates)): it has an **encoder** (which compresses audio to a latent space), a quantization bottleneck, and a **decoder** (which reconstructs audio from quantized latents) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=Being%20an%20audio%20codec%2C%20Mimi,composed%20by%20two%20main%20components)) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=The%20Decoding%20Process)). The encoder is a convolutional neural network that progressively downsamples the input waveform; specifically, Mimi’s encoder uses a stack of residual conv blocks with strides 4×5×6×8×2 (total downsampling factor 1920) to turn 24 kHz audio into a 512-dimensional latent vector at **12.5 Hz frame rate** (i.e. one 512-d vector every 80 ms) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L694%20striding%20fac%02tors,and%20dimension%20D%20%3D%20512)) ([](https://kyutai.org/Moshi.pdf#:~:text=striding%20fac%02tors%20,and%20dimension%20D%20%3D%20512)). All convolutions are causal (no future context) with dilations, ensuring the encoder can run in a streaming fashion (producing output as audio comes in) ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)) ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). The decoder mirrors this (with transpose convolutions) to upsample 12.5 Hz latent back to 24 kHz audio ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)).

At the bottleneck, Mimi uses **Residual Vector Quantization (RVQ)** to discretize the 512-d latent. Importantly, Mimi’s quantizer is _split_ into two parts: one **semantic codebook** and several **acoustic codebooks** ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=3,produces%201%20token%20per%20frame)). In total Mimi uses **Q = 8 quantizers** (codebooks) per frame in the paper’s design ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=3,produces%201%20token%20per%20frame)) ([](https://kyutai.org/Moshi.pdf#:~:text=Quantization%20rate,We%20moreover%20follow)). The first quantizer produces a “semantic token” capturing high-level linguistic content of that 80ms audio frame, while the remaining 7 quantizers produce “acoustic tokens” that capture the detailed voice timbre, prosody, and other low-level audio features ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=3,produces%201%20token%20per%20frame)). Each codebook has 2048 possible entries (11 bits) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,are%202048%20possible%20audio%20tokens)). Thus for each 80ms frame, Mimi yields 8 discrete tokens (1 semantic + 7 acoustic), and the overall bitrate is **12.5 Hz _ 8 tokens _ 11 bits ≈ 1.1 kbps** per audio stream ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,are%202048%20possible%20audio%20tokens)). This is a **very low bitrate** compression – much smaller than traditional codecs – making it feasible for Moshi to model speech with manageable sequence lengths. (For reference, without this hierarchical approach, modeling even 1 second of speech at 50 Hz with purely acoustic tokens would require ~50 tokens/second per stream, which is far more onerous ([](https://kyutai.org/Moshi.pdf#:~:text=with%20Q%20%3D%208%20codebooks,of%20audio%2C%20this%20would%20amount)).)

**Transformer-Enhanced Codec:** A key innovation in Mimi is the inclusion of Transformer layers in the codec bottleneck to improve compression quality. Mimi adds two small Transformer models (8 layers, model dim 512) in the latent pipeline – one inserted _before_ quantization and one _after_ dequantization – to better encode long-range context in the audio representation ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021)) ([](https://kyutai.org/Moshi.pdf#:~:text=which%20preserves%20the%20compatibility%20of,Both%2010)). These are causal Transformers (using RoPE positional encoding, context window ~250 frames = 20s) that process the sequence of latent frames, akin to the “Transformer Encoder/Decoder” in EnCodec’s recent improvements ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021)). By doing this, Mimi can capture semantic information spread over time (e.g. phonetic content spanning multiple 80ms frames) and allocate it efficiently into the first quantizer. The Transformers significantly improved perceived audio quality and the efficacy of semantic token distillation ([](https://kyutai.org/Moshi.pdf#:~:text=Moshi%3A%20a%20speech,parameters%2C%20Mimi%20is%20causal%20and)) ([](https://kyutai.org/Moshi.pdf#:~:text=,67)). To keep training stable, Mimi used **LayerScale** initialization (transformer weights scaled to 0.01 initially) and applied strong regularization (notably weight decay 5e-2 on Transformer parameters only) ([](https://kyutai.org/Moshi.pdf#:~:text=additional%20regular%02ization%20with%20weight%20decay,250%20frames%20before)) ([](https://kyutai.org/Moshi.pdf#:~:text=optimizer,250%20frames%20before%20the%20last)). Mimi’s encoder/decoder still remain mostly convolutional (for speed), but these Transformer modules help bridge the gap between purely semantic encoders (like HuBERT/WavLM) and acoustic codecs.

**Semantic Token Distillation:** The **semantic vs acoustic split** in Mimi’s quantizers is achieved by a **knowledge distillation** technique inspired by Facebook’s SpeechTokenizer ([](https://kyutai.org/Moshi.pdf#:~:text=these%20acous%02tic%20tokens%20provide%20appropriate,conditioning%2C%20by%20using%20semantic%20audio)) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20sep%02arate%20encoders%20represents%20a,com%2Fchongo%2Ftech%2Fcomp%2Ffnv%209)). A pretrained self-supervised speech model (WavLM-Large) was used to provide high-level semantic embeddings of the audio; during Mimi’s training, the encoder’s first quantizer is optimized to produce tokens that approximate WavLM’s embeddings for the same audio segment ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L661%20inspiration%20from,encoding%20and%20decoding%20of%20semantic)) ([](https://kyutai.org/Moshi.pdf#:~:text=inspiration%20from%20previous%20work%20on,encoding%20and%20decoding%20of%20semantic)). Concretely, WavLM generates a 50 Hz sequence of features from the audio (16 kHz), which are downsampled to 12.5 Hz and projected to match Mimi’s latent dimension; Mimi’s first codebook output is trained (via a cosine similarity loss) to mimic these “non-causal” semantic embeddings ([](https://kyutai.org/Moshi.pdf#:~:text=inspiration%20from%20previous%20work%20on,encoding%20and%20decoding%20of%20semantic)) ([](https://kyutai.org/Moshi.pdf#:~:text=Quantization%20rate,We%20moreover%20follow)). This distillation forces Mimi’s first token per frame to encode phonetic/content information (what is being said) in a way that’s useful for a language model, while the remaining tokens focus on reconstructing exact audio quality ([](https://kyutai.org/Moshi.pdf#:~:text=these%20acous%02tic%20tokens%20provide%20appropriate,conditioning%2C%20by%20using%20semantic%20audio)) ([](https://kyutai.org/Moshi.pdf#:~:text=counterpart%2C%20semantic%20tokens%20do%20not,with%20language%20allows%20generating%20intelligible)). Notably, Kyutai found that doing this in a **split RVQ** manner (one dedicated semantic quantizer + 7 residual quantizers) outperformed other strategies – it maintains the **phonetic discriminability** of the first token without overly degrading reconstruction quality in the others ([](https://kyutai.org/Moshi.pdf#:~:text=quality%20metrics%2C%20while%20human%20evaluations,scale%20STFT%20discriminator.%20The%20exact)) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)). In summary, Mimi yields a single stream of tokens that **jointly contain semantic and acoustic information** ([](https://kyutai.org/Moshi.pdf#:~:text=Mimi%20%28Section%203,To)), making it ideal input/output for Moshi’s Transformer (which can treat these tokens similarly to text tokens).

**Training Procedure:** Mimi was trained on a large collection of speech audio (the paper mentions on the order of “millions of hours” of audio, primarily English) ([](https://kyutai.org/Moshi.pdf#:~:text=4,majority%20of%20which%20contains%20English)). Its training objective combined traditional codec losses with adversarial training and the new distillation loss. As a baseline, Mimi started with the same losses as EnCodec: a multi-scale spectrogram reconstruction loss plus a multi-scale STFT discriminator loss (GAN-style) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,scale%20STFT)) ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)). But Kyutai experimented with **“adversarial-only” training**, meaning they _drop the direct reconstruction loss_ and rely purely on the discriminator (and feature matching loss) to drive the generator ([](https://kyutai.org/Moshi.pdf#:~:text=Adversarial,We%20note%20that%20this)) ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). Surprisingly, at such a low bitrate, removing the L1/MSE reconstruction loss improved subjective audio quality (even though objective distortion metrics worsened) ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). They report a “remarkable boost” in audio naturalness using adversarial-only training for Mimi ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)) – an unusual but insightful finding that GAN-only training can prioritize perceptual quality over numerical error. In addition, Mimi’s training used a form of **quantizer dropout** (a.k.a. partial vector quantization): during training, for 50% of audio samples they bypassed quantization for some latents (feeding continuous latent to decoder) ([](https://kyutai.org/Moshi.pdf#:~:text=et%20al,that%20this%20significantly%20improves%20objective)). This technique (also used by EnCodec and others) helps the model learn scalable bitrate – i.e. the codec doesn’t overly rely on any one quantizer and can function if some are dropped ([](https://kyutai.org/Moshi.pdf#:~:text=before%20the%20decoder,%282023%29%2C%20this%20means)) ([](https://kyutai.org/Moshi.pdf#:~:text=et%20al,that%20this%20significantly%20improves%20objective)). Mimi’s final configuration operates fully **causally** with an 80ms latency (it outputs the first 80ms of decoded audio after seeing 80ms of input) ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). This means Mimi can encode or decode streaming audio on-the-fly with only a small lookahead, which is critical for real-time dialogue.

**Efficiency:** Mimi’s design is optimized for integration with an LLM. Operating at **12.5 Hz** and ~1.1 kbps means the sequence of audio tokens is quite slow and sparse compared to real time – only 12–13 tokens per second, which a Transformer can handle in streaming mode. The codec’s fully causal, streaming-friendly design ensures Moshi doesn’t introduce undue delays. In fact, Mimi’s encode/decode steps are light enough to contribute negligible latency (~80ms) relative to the Transformer inference ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). By compressing audio aggressively (while preserving intelligibility through semantic tokens), Mimi effectively **bridges the audio and text worlds**: the audio tokens are analogous to text tokens and can be modeled with similar Transformer architectures ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Similarly%20to%20,batch_size%2C%20num_codebooks%2C%20sequence_length)). This allows Moshi to treat user speech input and system speech output **as token sequences** and apply language modeling techniques to both.

## Multi-Stream Modeling: Temporal & Depth Transformers for Dual Audio Streams

A core challenge in Moshi is modeling **two simultaneous streams** of audio tokens (user and system) along with any text tokens, without flattening everything into an exorbitantly long sequence. To handle this, Moshi introduces a **hierarchical Transformer architecture** called the **RQ-Transformer** ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). It consists of: (a) the large **Temporal Transformer** (Helium) which operates along the time axis, and (b) a smaller **Depth Transformer** which operates along the “codebook depth” axis at each time step ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=as%20illustrated%20in%20Figure%203,by%20the%20Temporal%20Transformer%2C%20and)). This effectively factorizes the generation: Helium predicts the progression of conversation over time (in steps of 80ms increments), and at each time step, the Depth Transformer produces all the required codec tokens for that time frame. By doing so, Moshi’s architecture avoids the need to handle every audio token in a single sequence. Instead of one giant autoregressive sequence of length _T × K_ (time steps × codebook tokens) for a conversation, it can handle _T_ steps with the big model and up to _K_ steps with the smaller model, dramatically reducing computational cost ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)).

**Temporal Transformer (Helium):** The Temporal Transformer operates at the granularity of **discrete time steps (frames)** – roughly 12.5 steps per second. At each step _s_, it ingests the _events/tokens from the previous step_ and outputs a **temporal context vector** z*s ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)). This context vector is an embedding that condenses what should happen at time *s* given all that has come before. Internally, Helium is essentially the same architecture as the base LLM (7B Transformer decoder) but now its “vocabulary” includes not just text tokens but also **audio token inputs** from both speakers ([](https://kyutai.org/Moshi.pdf#:~:text=as%20illustrated%20in%20Figure%203,by%20the%20Temporal%20Transformer%2C%20and)) ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). To represent multiple streams at each step, Moshi uses a clever input representation: it defines a set of sub-sequence values V*{s,k} for each stream _k_ at time _s_ (e.g. specific codebook tokens for user or system) and then projects them into the Temporal Transformer input. In practice, the model has separate learned embedding tables for each type of token (e.g. for each codebook index of each speaker, and for text) – these embeddings are summed or concatenated to form the input for that time step ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L976%20the%20Temporal,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)) ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). For example, suppose at the previous time step _s-1_ the system had produced some audio tokens and the user also was speaking; Helium will take the combination of “user’s token at s-1” and “system’s token at s-1” (and possibly a text token, see Inner Monologue below) by summing their respective embeddings to feed as the context representation of step _s-1_ ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). This way, multiple parallel streams are encoded as a **single composite token per time step** in the Temporal Transformer ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). Helium then attends over the sequence of these composite tokens from 0 up to s-1 to produce z*s = Tr<sub>Temp</sub>(V_0,...,V*{s-1}) ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)), which captures the state of the conversation at time _s_. Notably, Helium runs for **S steps rather than S×K** – e.g. modeling 5 minutes of audio might be S=~3750 steps (at 12.5 Hz) instead of ~30k tokens/steps if flattened ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). This hierarchy is what makes real-time inference feasible.

**Depth Transformer:** Once Helium provides the temporal context vector z*s for the current time step, Moshi uses the **Depth Transformer** to generate the actual audio token outputs (for that time step) in each stream ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=as%20illustrated%20in%20Figure%203,by%20the%20Temporal%20Transformer%2C%20and)). The Depth Transformer is a smaller autoregressive model (6 layers, 1024-d model, 16 heads ([](https://kyutai.org/Moshi.pdf#:~:text=Depth%20Transformer%20Model%20dimension%20,space%20Text%20cardinality%2032000%2032000))) that operates *conditioned on* z_s. Essentially, it takes z_s as a conditioning input (via cross-attention or concatenation with an input embedding) and autoregressively predicts the sequence of **K codebook tokens** for this time frame ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20an%20acoustic%20delay%20%CF%84,At%20inference%20time)). In other words, for time s, the Depth Transformer will generate tokens: e.g. first the semantic token, then acoustic token 2, then token 3, … up to token K, one by one (within that 80ms step) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20an%20acoustic%20delay%20%CF%84,At%20inference%20time)). By the design of Mimi’s codec, K=8 tokens represent the full audio for that frame (for one speaker’s audio). If both the user and system have audio at time s, those can be treated as separate sub-sequences to generate. The paper formalizes this as the Depth Transformer mapping both z_s and the previously generated sub-sequence tokens (V*{s,1}, ..., V\_{s,k-1}) to the next token logits ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). The **important outcome** is that the Depth Transformer **generates all codebook tokens in parallel across streams** for each time step, but its sequence length is limited to K (≤8) instead of the full time length ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L972%20Importantly%2C%20the,In%20practice)) ([](https://kyutai.org/Moshi.pdf#:~:text=Importantly%2C%20the%20number%20of%20steps,In%20practice)). Helium’s context vector acts like a **conditioning variable** that influences the Depth Transformer’s output for that frame. In implementation, z*s might be injected via cross-attention layers or used to initialize the Depth Transformer’s hidden state ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). The separation of duties is clear: Helium (Temporal) decides \_when and generally what* content should be spoken, and the Depth Transformer fills in _how to realize that in codec tokens_ at that time, also ensuring consistency across the multiple codebooks.

**Multi-Stream Integration:** Moshi’s model is **multi-stream** in that it explicitly tracks tokens for two speakers: the user and the system (Moshi itself). At each time step, there can be tokens from the user’s audio stream (if the user is speaking at that moment) and tokens from the system’s audio (if Moshi is speaking), potentially both simultaneously. Moshi handles this by concatenating or stacking the two streams’ tokens as part of the Depth Transformer’s output for the time step ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)). In practice, they arrange the token order such that the model can distinguish the source: e.g., **Moshi’s tokens vs user’s tokens have distinct positions or embedding offsets**. According to the paper, they stack the user and system token sequences for each time: _“multi-stream modeling which stacks the tokens of Moshi and the user for each timestep”_ ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)). For example, one scheme is: at time s, let V<sub>s,1...Q</sub> be Moshi’s Q audio tokens and V<sub>s,Q+1...2Q</sub> be the user’s Q audio tokens (with some fixed ordering). The Temporal Transformer then sees the combined effect of both streams from the previous step. This enables **full-duplex** capabilities – the model can generate Moshi’s response while simultaneously receiving user input tokens, since both are just streams of tokens it conditions on. There is no hard turn-taking; overlapping speech is naturally represented by both streams having tokens at the same time indices ([](https://kyutai.org/Moshi.pdf#:~:text=information%20that%20modifies%20meaning%E2%80%94%20such,of%20the%20user%20into%20parallel)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). Moshi’s training data included many instances of such overlaps, interruptions, and backchannels, teaching the model how to handle them. The **speaker identity** is implicitly encoded by whether a token came from the user’s stream or Moshi’s stream (and possibly the model uses separate embedding tables for each to differentiate). In sum, Moshi’s architecture merges two input/output channels through the Temporal Transformer – a design analogous to multi-channel sequence modeling – letting the model learn the dynamics of conversation (e.g. one person pauses, the other interjects, etc.) without any turn segmentation ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,the%20modeling%20of%20arbitrary%20conversational)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)).

**Inner Monologue (Text Prefix):** One additional component in Moshi’s architecture is the **“Inner Monologue”** mechanism ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=contribution%20is%20the%20Inner%20Monologue%2C,linguistic%20information)). This refers to Moshi predicting **time-aligned text tokens as a prefix to its own audio tokens** when it speaks ([](https://kyutai.org/Moshi.pdf#:~:text=We%20moreover%20extend%20the%20hierarchical,200ms%20in%20practice%2C%20and%20is)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=contribution%20is%20the%20Inner%20Monologue%2C,linguistic%20information)). In other words, before (or while) the Depth Transformer generates Moshi’s acoustic tokens for a time frame, the model first outputs the corresponding text token of what it’s about to say. These text tokens are not spoken aloud; they are an internal representation (hence “inner monologue”) that the model uses. For example, if Moshi is saying “hello” spanning frames s=1 to s=3, it might output the text token “Hello,” at the start, aligned to the audio start, then proceed with the audio tokens that produce the spoken word ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=between%20each%20token%20enunciation)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,are%20synchronized%20with%20the%20audios)). This extension effectively gives the language model a chance to **“think in words”** just ahead of producing speech sounds. It was found to massively improve the linguistic coherence and fluency of the generated speech ([](https://kyutai.org/Moshi.pdf#:~:text=We%20moreover%20extend%20the%20hierarchical,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=This%20allows%20for%20the%20removal,of%20160ms%2C%20200ms%20in%20practice)), since Helium is fundamentally better at next-word prediction than next-audio-frame prediction. The inner text serves as a guiding transcript for the acoustic generation. Notably, Moshi’s inner monologue is **time-aligned**: the text tokens are interwoven into the audio token timeline (with appropriate padding to align them) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=The%20original%20model%20is%20synchronized,in%20between%20each%20token%20enunciation)). Moshi predicts these text tokens only for _its own_ speech, not for the user (the user’s speech is not transcribed by Moshi during normal generation) ([](https://kyutai.org/Moshi.pdf#:~:text=Note%20that%20we%20do%20not,of%20the%20user%2C%20as%20transcribing)). By keeping these text tokens as part of the output sequence (as a special stream or prefix in the multi-stream), Moshi essentially operates in a **speech-to-text-to-speech mode** internally – but it’s all within one unified Transformer model. This is a novel middle ground between pure speech generation and cascaded ASR+TTS: Moshi remains a direct speech generator, but with an internal textual “whisper” that aids it.

**Benefits:** The multi-stream, two-level Transformer architecture is the key enabler of Moshi’s **real-time full-duplex** performance. It reduces the effective sequence length dramatically (Temporal Transformer steps = ~12.5 per second, vs 100 tokens/sec if all codebooks were flattened) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). This means the large 7B model only needs to do ~12.5 forward steps to generate one second of audio ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=previously%20proposed%20for%20discrete%20image,art%20audio%20language%20modeling%20performance)). Indeed, Kyutai highlights that thanks to the Depth Transformer, they “only need 12.5 passes through the 7B backbone for 1s of audio” ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=previously%20proposed%20for%20discrete%20image,art%20audio%20language%20modeling%20performance)). That corresponds to about an 80ms stride, which is how they achieve low latency. The **Depth Transformer** itself is much smaller and thus fast; it can generate the codebook tokens per step with negligible overhead. Also, by stacking user and system streams, the model handles overlap intrinsically – no separate voice activity detection or turn management is needed ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,the%20modeling%20of%20arbitrary%20conversational)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). This architecture, termed RQ-Transformer in the paper, was inspired by RQ-VAE models in image generation and proved effective for audio ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)). It is a major architectural innovation of Moshi, allowing it to be _the first large-scale spoken dialogue model that is truly real-time and simultaneous_ ([](https://kyutai.org/Moshi.pdf#:~:text=Monologue%E2%80%9D%20method%20significantly%20improves%20the,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)).



---

**Navigation**

* [Back to Index](index.md)
* Next: [Training Procedures](training.md)

