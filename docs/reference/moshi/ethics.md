# Ethical and Safety Considerations

Building and deploying a system like Moshi raises important ethical questions and challenges. As an AI that can **listen and speak in real time**, Moshi combines the usual NLP concerns (like harmful content generation, bias, misinformation) with new issues related to voice and audio (such as impersonation, audio deepfakes, privacy of voice data). Kyutai Labs acknowledged these concerns and took several steps to mitigate risks ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)) ([](https://kyutai.org/Moshi.pdf#:~:text=6%20Safety%20In%20parallel%20with,this%20section%2C%20we%20specifically%20consider)).

**Toxic Content and Harassment:** Like any language model, Moshi could produce inappropriate or hateful speech if not properly aligned. To address this, the final fine-tuning phase included **safety-oriented dialogues** – for example, conversations where the user requests disallowed content and Moshi is supposed to refuse ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)). By training on these, Moshi learns to respond with refusals or safe completions when prompted with e.g. hate speech or NSFW queries. The paper mentions generating conversations where _"the user asks unethical or NSFW questions, and Moshi refuses"_ ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)). This aligns Moshi with a kind of content policy similar to ChatGPT’s. Additionally, they evaluated Moshi on a toxicity benchmark (the **ALERT** dataset) across categories like hate, self-harm, violence, etc. ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2761%20evaluates%20safety,The)) ([](https://kyutai.org/Moshi.pdf#:~:text=evaluates%20safety%20under%20multiple%20categories,The)). Moshi’s overall safety score was in the mid range compared to other LLMs – not perfect, but not the worst ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2768%20Overall%20Safety,table%20in%20terms%20of%20rank)) ([](https://kyutai.org/Moshi.pdf#:~:text=Overall%20Safety%20Score%2083,table%20in%20terms%20of%20rank)). The analysis indicates Moshi generally does **not generate toxic content** and stays consistent in persona ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L3146%20while%20displaying,suite%20of%20models%20and%20recipes)). However, users did note in informal testing that Moshi’s personality can be a bit **“rude” or abrupt** at times ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=%E2%80%9CI%20find%20the%20Moshi%20model,worthy%2C%20quipped%20Karpathy)) – this may be an artifact of the training dialogues or the nature of overlapping conversation (it might interrupt more than an ideal polite assistant). This is a subtle safety point: _conversational etiquette_. If Moshi frequently interrupts or ignores user queries, it could lead to user frustration (as one tester commented they “almost lost patience” due to frequent interruptions ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=confusing%20but%20also%20very%20funny,worthy%2C%20quipped%20Karpathy))). While not a catastrophic risk, it shows the challenge of balancing full-duplex freedom with politeness. Further fine-tuning or rules might be needed to refine this behavior (e.g. encourage Moshi to wait slightly longer before cutting in, or to use phrases like “sorry to interrupt”).

**Bias and Fairness:** The underlying training data (text from the web, audio from various sources) likely carries biases (cultural, gender, etc.). Helium’s text corpus was filtered for quality, but not explicitly for bias or ideology, so it might have the same biases as a general web-scraped model. The synthetic fine-tuning might inadvertently amplify certain biases depending on the scripts. For example, if OpenHermes dialogues had certain stereotypes or less diversity, Moshi could reflect that. They did not detail bias testing, but an ethical implementer should be mindful of outputs related to sensitive attributes. It might be wise to do adversarial prompts to see if Moshi exhibits bias (e.g., how does it respond to different accents or dialects? does it assume genders or ethnicities in scenarios?). The **audio domain** adds bias aspects too: if Mimi was trained mostly on certain accents (say American English), Moshi might struggle with others or transcribe them incorrectly internally. The synthetic data tried to include accent variety for the user voice ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=our%20own%20models%3A%20Helium%20writes,sure%20Moshi%20stays%20in%20character)), which helps, but bias could still occur (like preferring certain speech patterns as “normal”). Evaluating Moshi with users of different backgrounds and making adjustments (via further fine-tuning or prompting to be inclusive) is important.

**Privacy:** Since Moshi processes live audio, there are privacy considerations: user speech content is very personal (could include names, health info, etc.). Running Moshi locally (on-device) mitigates many privacy issues because data need not leave the device ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=Overall%2C%20it%20provides%20a%20compelling,still%20has%20room%20for%20improvement)). This is a big advantage over cloud-based voice assistants. However, if Moshi is deployed in a cloud or as a public demo, developers should ensure the audio and transcriptions are handled securely (e.g., encrypted transmission, not stored without consent). Another aspect is **bystander privacy**: Moshi could potentially pick up background voices not intended for it. If running on a phone or smart speaker, it might inadvertently record others. Care must be taken to have a clear “activation” or user consent for recording – albeit Moshi tries to eliminate the need for a wake word by continuously listening, that is double-edged ethically. Continuous listening devices raise privacy flags, so an application might still choose to have a push-to-talk or wake word to delineate when it’s actively listening.

**Impersonation and Deepfake Voice:** Moshi’s voice output is currently a single fixed AI voice. Kyutai intentionally kept Moshi’s voice constant and even in open-source replaced it with two synthetic voices to avoid copying a real person’s voice ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=our%20own%20models%3A%20Helium%20writes,sure%20Moshi%20stays%20in%20character)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)). This is crucial because if Moshi’s voice sounded like a specific individual, one could misuse it to impersonate them. By using an artificial voice (and releasing two obviously synthetic voices), they reduce the immediate risk of voice mimicry. However, the technology could be adapted to clone voices – one could fine-tune Mimi’s decoder to a target voice, or prompt the model to speak in a different style if it had multi-voice capability. That could be used maliciously (fake audio recordings). Thus, limiting voice cloning features and watermarking outputs can be considered. Since Moshi is CC-BY licensed and open, others could attempt to modify it for multi-voice. That’s an ethical risk inherent in open-source speech models. The community should discourage misuse, and perhaps incorporate detectors for synthesized audio (audio deepfake detection is a developing field).

**Non-linguistic sounds and Emotional Content:** Moshi can output not just words but any sound in theory (because acoustic tokens can represent laughter, sighs, etc.). This is powerful but can be misused (e.g., generating realistic sounds of gunfire or screams in a prank scenario). On the other hand, it allows positive uses like more natural, empathetic responses (laughing at a joke, etc.). Ensuring Moshi doesn’t output very startling or dangerous sounds unexpectedly is part of safety. Possibly some filter on output type (like disallow extremely loud or certain wave patterns) could be implemented. Also, emotional manipulation is a risk: with a human-like voice, users might develop trust or attachment (the ELIZA effect). Designers should make clear Moshi is an AI, to avoid deception or over-reliance by vulnerable users. If Moshi gives advice (like medical or legal), that’s another area – it should have been fine-tuned to provide disclaimers or correct itself, but it’s only as good as its data. The paper noted Moshi is not strong at complex tasks or tool use ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=On%20Apple%20Mac%2C%20the%20model,integration%2C%20according%20to%20the%20company)), so deploying it in high-stakes scenarios could be problematic.

**Audio Safety:** The paper points out that **audio safety is less developed** compared to text safety ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2749%20toxicity%20problems,apple%20comparison)). For example, detecting hate speech in audio is harder than in text. Moshi sort of bypasses that by converting audio to tokens (so it can potentially detect if the user’s audio tokens map to a slur or something?), but it wasn’t trained to moderate user input. If a user says hateful things, Moshi likely will respond with something (maybe it refuses if it recognizes the request as unethical). But understanding hate in audio without transcription is tough. Possibly, the inner monologue text acts as a de-facto transcript for Moshi’s own speech, which they used to evaluate toxicity of Moshi’s outputs (they likely transcribed Moshi’s speech via the model itself or external means to check content) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2761%20evaluates%20safety,The)) ([](https://kyutai.org/Moshi.pdf#:~:text=Appendix%20E,Values%20in%20the%20last%20row)). They show an overall safety score table, implying they compared Moshi’s output text to others. It scored moderately (like ~85 out of 100) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2768%20Overall%20Safety,table%20in%20terms%20of%20rank)) ([](https://kyutai.org/Moshi.pdf#:~:text=Overall%20Safety%20Score%2083,table%20in%20terms%20of%20rank)), meaning it does pretty well but not perfect. As implementers, it’s wise to also include a final safety layer: e.g. run Moshi’s intended text output through a filter for banned phrases before synthesizing, to catch anything severe that slipped through. Similarly, for user input, one could have a hotword detector (like if user is screaming an obscenity, maybe Moshi should not cheerfully mirror that).

**Consistency and Voice Stability:** They highlight Moshi remains _consistent in its voice_ ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L3146%20while%20displaying,suite%20of%20models%20and%20recipes)). That is good from a trust perspective – it doesn’t suddenly change tone or identity mid conversation. They also presumably ensured the training data kept Moshi’s persona stable (the synthetic conv always had Moshi as a particular style). This avoids confusion and potential catfishing-type issues where the AI alters its style to manipulate user.

**Open-Source vs Proprietary:** Kyutai’s move to open-source Moshi is ethically notable – it allows transparency and community oversight. Anyone can inspect how it was trained and test for biases or issues. This is arguably safer in the long run than a closed model, because problems can be identified and addressed by many. However, open-source also means bad actors could use it for nefarious purposes (like running it on spam calls to scam people, etc.). Mitigating that is challenging. The CC-BY license and community guidelines hopefully discourage misuse. Tools might be developed to detect Moshi’s synthetic voice if used in the wild (maybe spectral signatures). Moshi’s voice currently probably sounds obviously synthetic to a careful listener (a bit monotonic or “AI-ish”), which might actually be a safety feature because it’s identifiable as AI. As the tech improves, synthetic voices become indistinguishable, raising stakes for imposters.

**User Data and Adaptation:** If one were to adapt Moshi with user-specific data (like fine-tuning it on a particular user’s conversations to personalize it), that could raise privacy issues because the model might memorize and regurgitate personal info. Moshi’s base was trained on public data, and fine-tunes on synthetic or public sets (Fisher is public), so it likely doesn’t have private info. But if it’s further trained on logs from real users, care must be taken to anonymize or limit memorization. The team did not mention training on any private or proprietary transcripts, which is good ethically and legally.

In conclusion, while Moshi breaks new ground in conversational AI, those deploying or modifying it must heed these ethical considerations:

- **Ensure the model is used in consented settings** (don’t eavesdrop without permission).
- **Limit misuse potential** (no multi-voice impersonation features by default).
- **Apply safety fine-tuning and filters** to avoid harmful speech.
- **Be transparent** that it’s an AI system to users.
- Continuously **evaluate** its behavior on sensitive prompts and iteratively improve the safety training (red-teaming it to find failure modes).

Kyutai’s initial results show Moshi is reasonably safe – it refuses blatantly problematic requests and doesn’t output toxicity on its own ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)) ([](https://kyutai.org/Moshi.pdf#:~:text=while%20displaying%20satisfying%20levels%20of,suite%20of%20models%20and%20recipes)). However, like any large generative model, it’s not perfect and can have unpredictable outputs. Ongoing monitoring and community feedback (the open-source community can report issues on the GitHub) will be important to address ethical issues as they arise.

---

**Sources:**

1. Alexandre Défossez et al., _“Moshi: a speech-text foundation model for real-time dialogue”_, arXiv preprint 2410.00037 (Oct 2024). ([](https://kyutai.org/Moshi.pdf#:~:text=We%20introduce%20Moshi%2C%20a%20speech,Finally%2C%20they%20rely%20on)) ([](https://kyutai.org/Moshi.pdf#:~:text=Monologue%E2%80%9D%20method%20significantly%20improves%20the,labs%2Fmoshi)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20speech,time)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=altogether%20by%20casting%20spoken%20dialogue,but%20we%20also%20illustrate%20how))

2. Kyutai Labs Blog, _“Moshi open-source release: run Moshi locally!”_ (Sept 18, 2024). ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Moshi%20is%20made%20of%20three,and%20Moshi%20on%20separate%20channels)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,linguistic%20information)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment))

3. Hugging Face Transformers Documentation – _Moshi model_ (Nov 2024). ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20deals%20with%203%20streams,of%20information)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20streaming%20auto,what%20the%20user%20said%2Fwill%20say))

4. Kyutai Labs GitHub Repository – _Moshi Code_ (2024). ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=2)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=MoshiForConditionalGeneration))

5. Marcello Castellaci, _“Exploring Mimi”_ (Oct 4, 2024) – blog post analyzing Mimi codec. ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=1,5Hz)) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=,are%202048%20possible%20audio%20tokens)) ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=Although%2C%20here%20we%20find%20another,1kbps%20unreleased%20model))

6. Analytics India Magazine, _“Kyutai Launches Moshi, an Open Source Voice Model…”_ (Sept 2024). ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=How%20Moshi%20Works%3F)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=It%20operates%20as%20a%20full,200%20milliseconds%20on%20L4%20GPUs)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20company%20said%20that%20they,installed%2C%20including%20the%20nvcc%20compiler)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=%E2%80%9CI%20find%20the%20Moshi%20model,worthy%2C%20quipped%20Karpathy))

7. Moshi Technical Report – Appendix and training details. ([](https://kyutai.org/Moshi.pdf#:~:text=of%20our%207B,custom%20dataset%20built%20from%20synthetic)) ([](https://kyutai.org/Moshi.pdf#:~:text=Batch%20size%20%28text%29%204,now%20describe%20our%20method%20to)) ([](https://kyutai.org/Moshi.pdf#:~:text=,codebook%20size%20of%20NA)) ([](https://kyutai.org/Moshi.pdf#:~:text=Quantization%20rate,We%20moreover%20follow))

8. Moshi Technical Report – Architecture figures and descriptions. ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)) ([](https://kyutai.org/Moshi.pdf#:~:text=step%201%20%E2%89%A4%20s%20%E2%89%A4,%E2%88%88%20R%20d)) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20sep%02arate%20encoders%20represents%20a,com%2Fchongo%2Ftech%2Fcomp%2Ffnv%209)) ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021))

9. Moshi Technical Report – Safety analysis. ([](https://kyutai.org/Moshi.pdf#:~:text=simple%20factual%20tasks%20like%20adding,refuses%20to%20answer%20these%20requests)) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L2761%20evaluates%20safety,The)) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L3146%20while%20displaying,suite%20of%20models%20and%20recipes))


---

**Navigation**

* [Back to Index](index.md)
* Previous: [Implementation Details](implementation.md)

