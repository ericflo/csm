# Inference and Real-Time Processing

One of Moshi’s headline achievements is being the **first real-time, full-duplex large language model for speech** ([](https://kyutai.org/Moshi.pdf#:~:text=Monologue%E2%80%9D%20method%20significantly%20improves%20the,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)). In deployment, Moshi can listen and speak concurrently, with low latency, enabling natural conversation flow much closer to human-human interaction than prior systems. Here we explain how the inference process works, how streaming is implemented, and what performance/latency characteristics the system has. We also note considerations for deployment and scalability.

##  Full-Duplex Streaming Mechanism

During inference, Moshi operates in a **continuous loop** processing audio in frames (e.g. 40–80ms chunks). It treats the incoming user audio and its own generated audio as parallel streams of tokens that grow over time, and it generates new tokens autoregressively as time advances. A typical real-time session proceeds as follows:

- **Audio Input Encoding:** As the user speaks into the microphone, audio is streamed through the Mimi encoder to produce tokens in real time. Every 80ms of user speech yields one new semantic token and 7 acoustic tokens (once fully quantized) – though Mimi can output the first token slightly earlier due to causal streaming. These user tokens are fed incrementally into Moshi’s Temporal Transformer as they arrive (on the user stream channel). In practice, the system likely buffers a short initial window (e.g. 160ms) to get a couple of frames encoded before Moshi starts responding, but after that it can work frame-by-frame.

- **Autoregressive Generation:** The Moshi model (Temporal+Depth) generates its output tokens step by step. At each time step, it takes into account all tokens so far (user and Moshi’s) and decides whether Moshi should speak (and what). If the model “chooses” to speak, it will produce a semantic token (and an inner text token) followed by acoustic tokens for that time. If the model decides to remain silent (perhaps because the user is talking and it should listen), it might produce a special silence token or simply no Moshi tokens at that step (just padding). This behavior is learned from training where overlapping speech was present – Moshi can generate tokens on its stream while user stream tokens are also present, effectively learning appropriate timing. The inference uses the Depth Transformer to generate all required audio tokens for each time step on the fly. Importantly, this is all done autoregressively **with streaming context**: Moshi doesn’t wait for the user to finish an utterance; it is constantly conditioning on partial user audio and potentially producing partial responses.

- **Output Audio Decoding:** As soon as Moshi generates some audio tokens for its own stream, those tokens are sent to the Mimi decoder to synthesize actual waveform audio that is played back through the speaker. Because generation happens frame by frame, Moshi can begin speaking _while_ the user is still talking, or immediately after the user starts a question (for backchannels like “uh-huh” etc.). The Mimi decoder also works in streaming mode, decoding 80ms at a time with only 80ms latency ([](https://kyutai.org/Moshi.pdf#:~:text=Causality%20and%20streaming,parameters%2C%20Mimi%20is%20causal%20and)). Therefore, the system can output speech with only a brief delay after the tokens are generated.

This pipeline effectively creates a **tight loop**: user audio -> Mimi encode -> Moshi model -> Mimi decode -> system audio. The architecture’s design ensures all pieces are low-latency. The **Temporal Transformer** runs one forward pass per 80ms frame. On modern hardware, a 7B transformer can easily compute one step in far less than 80ms, so it keeps up in real-time. The **Depth Transformer** is much smaller and its computation per step is negligible in comparison. Mimi’s encode/decode each add ~80ms initial latency but then operate continuously streaming. Thus, once the pipeline is filled, Moshi can respond with as little as a single-frame delay relative to the user.

**Latency:** The theoretical minimal latency of the full system is about **160 ms** (as reported by the authors) ([](https://kyutai.org/Moshi.pdf#:~:text=also%20illustrate%20how%20it%20can,labs%2Fmoshi)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)). In practice, they achieved about **200 ms** end-to-end latency on an NVIDIA L4 GPU ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=as%20a%20prefix%20to%20audio,of%20160ms%2C%20200ms%20in%20practice)). This latency budget includes input audio buffering (~80ms frame), model computation (~80ms or less), and perhaps another frame of lookahead or safety margin. 200ms is about the time of a typical human conversational turn-taking pause, so interactions feel quite seamless. For comparison, conventional voice assistants often have 2–3 seconds latency (due to waiting for end-of-speech, performing ASR+NLP, then TTS) ([](https://kyutai.org/Moshi.pdf#:~:text=We%20introduce%20Moshi%2C%20a%20speech,Finally%2C%20they%20rely%20on)) ([](https://kyutai.org/Moshi.pdf#:~:text=their%20complex%02ity%20induces%20a%20latency,text%20language%20model%20backbone%2C%20Moshi)). Moshi’s approach is ~10× faster in response.

Several techniques ensure low latency:

- Moshi does not wait for a full utterance or a “stop speaking” signal; it uses the multi-stream model to handle partial overlap. This eliminates the need for a separate VAD (voice activity detector) to find end-of-user-speech, which typically adds hundreds of ms.
- The **token delay** mechanism can be tuned to trade off latency vs. understanding. For instance, they mention if you delay Moshi’s audio tokens by a couple seconds, Moshi essentially becomes a streaming TTS (getting more text context before synthesizing) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). Conversely, delaying the text tokens yields a streaming ASR ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). In the live system, they likely use a small fixed text delay (the acoustic delay τ=1 corresponds to ~80ms) so that a rough word transcript is formed just slightly after the audio begins, not too far in advance.
- The computational graph is lightweight per step. Running a 7B model for 80ms of conversation is feasible on a single GPU, especially using mixed precision and possibly compiling the model. The authors reported that Moshi can **run in real-time on a single NVIDIA L4 GPU or an Apple M3 MacBook Pro** ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=for%20Helium%20,the%20token%20delay%20of%20MusicGen)), which are relatively accessible devices. L4 is a low-power data center GPU (about 22 TFLOPS), and the M3 with Apple’s Neural Engine can handle the 7B model through their accelerated framework (MLX).

In streaming inference implementation, typically a **scheduler thread** reads microphone audio continuously, chunking it into 80ms frames. Each frame, it encodes new tokens via Mimi and appends them to the user token sequence. Then it invokes Moshi’s generate function for one step (or a few steps) to produce new Moshi tokens. Any new Moshi audio tokens get decoded to sound and immediately played. This loop runs fast enough that it keeps up with real time. Because the model is large, a common strategy is to use a **sliding context window**: after a certain number of steps (say, 3000 steps = ~4 minutes of conversation, which is Moshi’s 4096 token context limit), they might start trimming the context from the beginning to keep the prompt length manageable. Alternatively, because Helium is an LLM with 4096 context, it can handle several minutes of continuous talk without truncation – likely sufficient for most interactions, beyond which some memory of earlier content might be lost. For extended sessions, one could summarize or compress old dialogue into a brief prompt if needed.

##  Real-Time Dialogue Behavior

Moshi’s full-duplex capability means it can engage in **natural conversational behaviors** that earlier systems could not. For example, Moshi can produce **backchannel acknowledgments** (“mm-hmm”, “I see”) while the user is speaking, to show it’s listening – because its user stream is active and it can decide to inject a short response on its stream without waiting ([](https://kyutai.org/Moshi.pdf#:~:text=a%20segmenta%02tion%20into%20speaker%20turns%2C,modeling%20of%20arbitrary%20conversational%20dynamics)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). It can handle **interruptions**: if the user interjects while Moshi is talking, Moshi (being an AI) might stop its speech generation mid-way or adjust on the fly. The model was trained on such scenarios (overlapping speech in Fisher and synthetic data), so it can learn to yield the floor when the user speaks over it. This leads to a more **dynamic, human-like interaction** where both parties don’t strictly alternate with long silences; instead, there can be overlaps and quick interjections, which Moshi can navigate ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)).

In terms of output quality, thanks to Inner Monologue and the strong language backbone, Moshi’s spoken responses are linguistically coherent and contextually appropriate. Evaluations showed that using the inner text prefix dramatically improved word error rates and fluency of the generated speech ([](https://kyutai.org/Moshi.pdf#:~:text=We%20moreover%20extend%20the%20hierarchical,200ms%20in%20practice%2C%20and%20is)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=This%20allows%20for%20the%20removal,of%20160ms%2C%20200ms%20in%20practice)). Essentially, if you transcribe Moshi’s speech, it tends to be as sensible as what a text-only LLM would produce for that query. Also, because Mimi is a high-fidelity codec, the voice output is natural-sounding (not robotic). Moshi’s default voice was trained to be consistent (they kept Moshi’s voice the same in synthetic training) – it’s described as a specific voice/persona. In the open-source release, they even provided two fine-tuned variants with different voices (male “Moshiko” and female “Moshika”) for variety ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=How%20Moshi%20Works%3F)). The user’s voice can be any input; Moshi doesn’t change the user audio except to encode it. The model can understand different accents and noise conditions because the synthetic data was varied on the user side. This makes it robust in practical use (to some extent – extremely heavy noise might still challenge Mimi’s encoding).

**Streaming ASR and TTS modes:** An interesting byproduct of Moshi’s design is that it can be used in one-way modes as well. If you _disable_ Moshi’s audio output stream and only use inner monologue text, Moshi essentially functions as a streaming ASR (transcribing the user in real-time with alignment) by delaying text tokens to line up with audio ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). Conversely, if you input text and have it generate audio tokens (with a slight audio delay), it acts as a streaming TTS, producing speech with low latency ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=An%20interesting%20byproduct%20of%20Inner,a%20streaming%20ASR%20with%20alignment)). While these are side-effects (the main goal is dialogue), it shows the flexibility of having a unified model. Indeed, one could get a text transcription of what Moshi says (from the inner monologue text tokens) or even of what the user said if they force the model to output a guess of user’s words – but that wasn’t a focus (explicit ASR for user is left out in design).

##  Deployment, Scalability, and Hardware

Kyutai Labs made Moshi available with **streaming inference code in PyTorch, Rust, and Apple’s MLX** frameworks ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Today%2C%20we%20release%20several%20Moshi,in%20PyTorch%2C%20Rust%20and%20MLX)) ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20standout%20feature%20of%20Moshi,on%20macOS%2C%20and%20Rust%20implementations)). This means developers can run Moshi locally or on servers with relative ease. The PyTorch implementation is straightforward for GPU servers, while the Rust implementation caters to performance-critical use (and can target different hardware). The MLX (Metal Lightning) version is optimized for Mac GPUs/ANEs, demonstrating Moshi running on an Apple Silicon laptop in real-time ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=for%20Helium%20,the%20token%20delay%20of%20MusicGen)).

For deployment, one needs the Moshi model weights (~≪ a 7B model in 16-bit is around 14 GB, plus ~1 GB for Mimi). Currently, **quantization** is not yet supported in the PyTorch release (as of late 2024), so a full 16-bit model requires a GPU with at least **24 GB VRAM** ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20company%20said%20that%20they,installed%2C%20including%20the%20nvcc%20compiler)). An NVIDIA 3090 or 4090 or A6000 would suffice, as would an L4 (22GB) in slightly compressed form. The Rust version may allow some quantization or use of lower precision, but it requires latest CUDA toolkit for GPU acceleration ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=The%20company%20said%20that%20they,installed%2C%20including%20the%20nvcc%20compiler)). The Mac MLX version likely uses 16-bit on the 64GB M3 Max/Ultra machines; they did mention testing on an M3 MacBook Pro, which presumably worked (possibly using 32-bit ANE accumulation with 16-bit weights). As the ecosystem progresses, we can expect 4-bit or 8-bit quantized Moshi to become available, which would drastically reduce memory and allow even smaller GPUs to run it. In fact, the paper notes that the Depth Transformer was quite robust to quantization in tests ([](https://kyutai.org/Moshi.pdf#:~:text=sizes,is%20reasonably%20robust%20to%20quantization)), and likely Helium could be quantized similarly to LLaMA-7B with minimal loss.

**Scalability:** Moshi is primarily a single-user, single-session model – it wasn’t designed for high-throughput server workloads with multiple parallel inferences on one GPU. To serve multiple users, one would probably run multiple instances or use model parallelism across GPUs if needed. The 7B size is actually relatively lightweight in the LLM world, so serving a few concurrent sessions per GPU is not infeasible (especially if each can run at ~12.5 inferences/sec). The bottleneck is that each session’s state (the KV cache of the transformer) occupies memory proportional to the context length. For a 4096 context of 7B model, that’s a few hundred MB. But since conversation context is often pruned, the memory per session might be manageable. Still, Moshi’s novelty is more about _capability_ than mass deployment scaling – it’s for building an interactive agent.

**Integration:** Developers can integrate Moshi into applications using the open-source code. The GitHub repo provides examples and possibly a demo (they had an online demo at moshi.chat) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=,bf16)). The HuggingFace Transformers library has already integrated Moshi as a model class, so one can load `MoshiForConditionalGeneration` and use `.generate()` to run it, albeit with some caveats due to its multi-stream nature ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20streaming%20auto,what%20the%20user%20said%2Fwill%20say)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=MoshiForConditionalGeneration)). To use it via HuggingFace’s API, you must supply three inputs: `input_ids` (text prompt tokens so far), `user_audio_codes` (tensor of user audio token history), and `moshi_audio_codes` (tensor of Moshi’s own audio token history) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=the%20other%20audio%20stream%20corresponds,what%20the%20user%20said%2Fwill%20say)). These three must be synchronized in length (each time index corresponds across them) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=history)). In a live system, managing these inputs simply means appending new tokens to the appropriate sequence each step. The HF interface is a bit manual, but Kyutai’s provided code likely wraps this into a simpler streaming loop.

**Resource Requirements:** Running Moshi in real-time requires decent compute but not unattainable: a single modern GPU or high-end CPU can do it. On CPU, it would be challenging to hit 12.5 forward passes/sec for 7B parameters (though possible with optimized BLAS and int8 quantization, it might be borderline). The focus has been on GPU and Apple Neural Engine. The Rust version might allow deploying on an edge device with CUDA support (like Jetson) but probably still needs a strong device given the model size.

For training from scratch, of course, the requirements were massive (TPUs/GPUs for weeks, 7M hour audio dataset, etc.). But an engineer reimplementing might leverage the open training recipe and possibly smaller scale to experiment.

In summary, the inference pipeline is well-engineered for low latency. The fact that Andrej Karpathy tested Moshi on a MacBook and found it “cool” and working ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=Karpathy%20shared%20his%20excitement%20about,away%20important%20information%2C%E2%80%9D%20he%20said)) speaks to its practicality. Some early users did note the model sometimes behaves a bit oddly (interrupting too often, etc.), which are things that can be refined with further fine-tuning ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=interrupts%2C%20it%20is%20a%20bit,worthy%2C%20quipped%20Karpathy)). But the ability to locally run a ChatGPT-like voice agent that **hears and speaks simultaneously** is a significant milestone, and Moshi’s design and training solve the key technical hurdles to achieve it.



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Training Procedures](training.md)
* Next: [Model Architecture Details](model_architecture.md)

