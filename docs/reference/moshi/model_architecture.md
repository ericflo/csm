# ML Model Architecture and Layout

In this section, we delve deeper into Moshi’s model topology, providing a detailed breakdown of each component and how they interact. We will also discuss various architectural innovations, parameter counts and efficiency, input/output representations, and how Moshi handles context and prompting. This is effectively a **blueprint** of Moshi’s neural network, suitable for an engineer wanting to implement it.

##  Topology and Component Interaction (Block Diagram Explanation)

Moshi’s overall architecture can be visualized as a **stack of modules** forming a pipeline: **Mimi Encoder -> Temporal Transformer (Helium) -> Depth Transformer -> Mimi Decoder**. The _inputs_ to the system are raw audio waveforms (user speech) and optionally text prompts; the _outputs_ are raw audio (Moshi’s speech) and optionally recognized text. Internally, everything is represented as token sequences:

- **Audio tokens:** Represented as $(batch, \text{num\_codebooks}, \text{seq\_len})$ as noted earlier ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Similarly%20to%20,batch_size%2C%20num_codebooks%2C%20sequence_length)). For ease, one can flatten the codebook dimension when feeding into the model (with distinct embeddings for each codebook slot).

- **Text tokens:** Standard 1D token sequence (batch, seq_len).

**Helium (Temporal Transformer):** This is a Decoder-Only Transformer with 32 layers, 4096-d model, 32 heads (each head ~128-d) – closely resembling a GPT-style or LLaMA model ([](https://kyutai.org/Moshi.pdf#:~:text=Hyper,Learning%20rate%203%20%C2%B7%2010%E2%88%924)). It has a maximum context of 4096 tokens. At any given inference step, Helium takes in a sequence of embeddings representing the history of all streams up to the previous timestep. How are these embeddings formed? For text tokens (like system’s inner monologue text), Helium uses a learned word embedding table of size 32k. For audio tokens, Helium uses a separate embedding mechanism: since each time step can have up to Q=8 tokens per stream, and potentially 2 streams, one approach is to assign each possible “position” a separate embedding matrix. In the HuggingFace implementation, they mention “each timestamp – each codebook – gets its own set of Linear layers and Embeddings” ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=2)). This suggests that for each codebook index (1 through Q) and each speaker (user or system), there is a unique embedding matrix mapping the token ID (0–2047) to a vector. Those vectors (for all tokens present at that time) are summed to form the input for that time step ([](https://kyutai.org/Moshi.pdf#:~:text=the%20Temporal%20Transformer%20receives%20at,%E2%89%A4%20K%2C%20the%20Depth%20Transformer)). Additionally, a positional encoding in time is applied – Helium likely uses Rotary Positional Embeddings which inherently handle temporal positions, so it rotates these summed embeddings by the position index. There may also be a trainable vector for the start token V0 (which they denote as 0) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)).

Inside Helium, multi-head self-attention operates over the sequence of these time-step embeddings, attending up to 4096 back in time. It uses FlashAttention for efficiency (computing softmax attention in optimized kernels) ([](https://kyutai.org/Moshi.pdf#:~:text=model,Kudo%20and%20Richardson)). Feed-forward layers use gated SiLU linear units (effectively doubling the channel then gating), which increases non-linearity. By the end of Helium’s stack, for the current time step _s_, we extract the output embedding for that position (context vector z_s). In a standard Transformer LM, we would multiply that by a language head to get logits over vocabulary; here, instead, we pass z_s to the Depth Transformer as conditioning.

**Depth Transformer:** The Depth Transformer is another decoder-only Transformer but much smaller (6 layers, 1024 model dim). Its “sequence length” is the number of token slots per time step. In the paper, they often call this K (with K=Q plus maybe text) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)). Essentially, if we consider Moshi’s output at time s to be a sequence like [Text_token, Acoustic1, Acoustic2, ..., AcousticQ] (for system) and similarly for user, the Depth Transformer might handle them either in parallel or separately. According to the HuggingFace docs, they implement Depth as generating over the codebook dimension with context length = num_codebooks ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=2)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=On%20its%20own%2C%20it%E2%80%99s%20also,generates%20over%20the%20codebook%20dimension)). This implies the Depth Transformer sees a “position” for each codebook index 1..Q. If inner monologue text is present, that might be treated as position 0 or a separate condition. The Depth Transformer likely employs **cross-attention** to incorporate z_s (the context from Helium). In practice, one can prepend the context vector as if it were a “memory” and allow each layer of Depth to attend to it (similar to how encoder-decoder Transformers work). Alternatively, they could simply add z_s to the input embeddings (through a linear projection and addition) – but cross-attention is more flexible.

The **inputs to Depth** for generation are: initially, perhaps a special start token for the sequence (or simply an all-zero embedding). Then it generates the first token (which should correspond to the semantic code or text). Once the first token is generated, it’s fed back in (with its embedding) to generate token 2, and so on, until Q tokens are generated. In training, all Q tokens at time s are known, so they can be input together with causal masking to ensure token k only attends to < k within that time step. The Depth Transformer attends to z_s (which encodes all prior time steps context) and to already generated tokens of the current step. At token generation time, the Depth Transformer’s final layer outputs logits over the audio codebook vocabulary (size 2048) for that position. If the position is the semantic token (codebook 1), it chooses among 2048 semantic codes; if it’s an acoustic token position, likewise 2048 possibilities. Possibly, they could use different output projections for semantic vs acoustic codebooks, but it’s simpler if they share the same size (which they do – NA=2048 for all). If an inner text token is included as position 0, that would have logits over the text vocab instead, so likely they handle text prefix in the Temporal model rather than via Depth to avoid mixing modalities in one softmax. Indeed, the “text token as prefix to acoustic” is handled by Helium predicting a text token that then just gets aligned to time s (with a pad until audio starts) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=The%20original%20model%20is%20synchronized,in%20between%20each%20token%20enunciation)). So Depth mainly deals with audio codebooks.

Once Depth has generated all needed tokens for the system’s audio at step s, those tokens are handed to Mimi’s decoder to produce sound. Meanwhile, the user’s tokens at step s were given (from Mimi’s encoder), not generated by Depth. However, how does the model incorporate user tokens at the same step s? This is where the **multi-stream stacking** happens. In the simplest approach, one could treat the user tokens as _additional tokens to predict_ as well, but actually, the user tokens are observations, not predictions. In training, they incorporate user tokens into the input context for future steps, but you wouldn’t have the model predict them as a loss (since that would be like trying to do ASR). Instead, they likely feed user tokens as part of the input for the next time steps (s+1 and onward) via the Temporal Transformer embedding, but do not include them in the generated sequence at s. Alternatively, one can imagine an architecture where the model is trying to predict the future of both user and system streams – essentially modeling a joint distribution P(user, system). The paper’s formulation hints they model both streams jointly as well ([](https://kyutai.org/Moshi.pdf#:~:text=only%20models%20semantic%20tokens,is%20full%20duplex%20and%20can)) ([](https://kyutai.org/Moshi.pdf#:~:text=first%20generates%20all%20the%20semantic,semantic%20and%20acoustic%20tokens%20jointly)), which might mean the model _does_ have a loss on user stream tokens too (predicting what the user will say next). However, that would be odd for deployment, since we don’t want the model hallucinating user speech. It might have been done just for pretraining (like modeling general conversation flow). In inference, obviously, we override the user token stream with actual input (so we’re not sampling user tokens, we are feeding them from mic). This approach is akin to treating the user as part of a stochastic process that the model can also simulate if needed (like in synthetic data they _did_ simulate user speech by sampling from the model). For safety, let’s assume at inference we always replace the user stream with real input (if available), ignoring what the model would predict for user tokens. The HuggingFace interface reflects this: `user_audio_codes` are provided as input, not generated by the model, whereas `moshi_audio_codes` are generated ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Moshi%20is%20a%20streaming%20auto,what%20the%20user%20said%2Fwill%20say)).

To summarize the architecture interaction: At each time step, Helium (Temporal) consumes the previous time’s combined token info and outputs context. Depth (with context) generates Moshi’s current audio tokens. User’s current audio tokens (if any) are just fed in externally. Then increment time and repeat. Both Helium and Depth are Transformers with causal masks (Helium masks future in time, Depth masks future in codebook order). They share no parameters except whatever ties might exist (none indicated, Helium and Depth are separate). The whole thing forms a single unified autoregressive model when viewed from a higher level – but implemented as two nested transformers.

##  Helium and Depth Transformer Details

**Helium (Temporal) Transformer Architecture:** As a typical decoder-only Transformer, Helium uses **multi-head self-attention**: $Attention(Q,K,V) = \text{softmax}(QK^T/\sqrt{d})V$. It has 32 attention layers, each with 32 heads of dimension 128 (so d_model=4096). It also has 32 feed-forward layers of dimension 11264 with GLU activation (two linear projections: one 11264→4096 gated by another 11264→4096 via elementwise multiplication after SiLU, per Shazeer 2020) ([](https://kyutai.org/Moshi.pdf#:~:text=of%204%2C096%20tokens%20and%20FlashAttention,We%20split%20all%20numbers%20into)) ([](https://kyutai.org/Moshi.pdf#:~:text=change%20the%20architecture%20of%20the,our%20tokenizer%20does%20not%20lose)). The Transformer uses **pre-normalization** (RMSNorm) – meaning each sub-block (attention or FFN) is preceded by RMSNorm on the input, and there’s an initial RMSNorm on embeddings and a final RMSNorm before output. Positional encoding is done through RoPE which multiplies the $q, k$ vectors by a rotation matrix based on their position index (it encodes position angles implicitly without adding to embeddings) ([](https://kyutai.org/Moshi.pdf#:~:text=original%20architecture%3A%20First%2C%20we%20use,2022%29%20for%20efficient%20training)). Helium’s output at a position is a 4096-d vector. For integration with Depth, they likely project this down if needed. In HuggingFace’s conversion, they indicate that **Moshi’s main decoder (Helium) is “strictly a classic text LLM”** with a language modeling head for text logits ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)). Indeed, during Helium pretraining, they attached an output softmax over the 32k text vocab. In Moshi, however, Helium is not directly producing final logits for audio – instead, its output is consumed by Depth. But Helium still can produce text if needed (e.g. for inner monologue text, Helium could directly output that as a normal LM output).

**Depth Transformer Architecture:** Depth’s 6 layers each have 16 attention heads (d_model=1024, so 16\*64 per head or similar). It also likely uses RMSNorm and RoPE (but context length = 8, which is tiny). However, Depth’s context length is actually the number of codebooks, which is fixed and small, so positional encoding might be trivial (it could even use a static positional embedding for positions 1..8 since that’s so small). The Depth Transformer must incorporate the Helium context vector. If using cross-attention, each Depth layer would have an additional attention that attends to z_s. But z_s is just one vector (or could be extended to one per layer of Helium or something). It might be simpler: some implementations of RQ-Transformer concatenate the context vector to the keys and values of Depth’s self-attention (so Depth heads attend not only to earlier depth tokens but also to z_s which is available as a key/value vector). Since Helium context is one per time step, they could repeat z_s across heads or project it to match head dim. Another approach: at Depth layer 1, add z_s to the self-attention output or feed-forward input in a gated manner. The exact implementation might vary, but conceptually Depth is **conditioned** on Helium’s output.

**Parameter Count & Efficiency:** Helium has ~7B parameters. Depth has much fewer: 6 layers of 1024-d – roughly (attention + FFN) per layer ~ (1024*1024*3*16 + 2*1024\*4096) = on the order of 50M or less. So Depth is ~50 million params, negligible compared to Helium. Mimi’s encoder/decoder conv layers and transformer bottleneck might add another ~50M. So Moshi in total might be ~7.1B parameters. That’s why it fits in ~14GB fp16.

One might wonder if Helium uses any parameter sharing or special structure. The paper doesn’t mention any parameter sharing between layers. They do mention using **LayerScale** in Mimi’s Transformers (with small initial weights) ([](https://kyutai.org/Moshi.pdf#:~:text=after,Both%20Transformers%20use%20causal%20masking)), but not in Helium. Helium’s training was standard next-token, which implicitly gave it world knowledge and reasoning. Depth’s training is tied to Helium’s (they train together in Moshi phases).

**Attention Mechanisms:** Helium’s attention is standard causal self-attention with flash-attn for speed ([](https://kyutai.org/Moshi.pdf#:~:text=model,Kudo%20and%20Richardson)). Depth’s attention might use **causal masking** along depth dimension (so token 2 can attend token 1 but not vice versa) ([](https://kyutai.org/Moshi.pdf#:~:text=with%20an%20acoustic%20delay%20%CF%84,At%20inference%20time)). It likely also uses some mechanism to incorporate temporal context (z_s). Possibly they mask such that all depth tokens can attend an input representing z_s (like a “memory” at position 0 which is not masked). That would ensure every predicted codebook token sees the context.

##  Mimi Audio Codec Architecture

We have described Mimi conceptually; here’s a structured summary of its architecture:

- **Encoder:** 1D ConvNet (fully causal). The exact configuration from EnCodec is used with modifications. They mention cascaded residual blocks with **strides 4,5,6,8,2** at some layers ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L694%20striding%20fac%02tors,and%20dimension%20D%20%3D%20512)). Typically, EnCodec uses something like 5 conv blocks each with downsampling factors that multiply to the total downsample. The **SeaNet** architecture from Tagliasacchi et al. 2020 is referenced ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)) – that means they use residual units with dilated conv (for context) and weight normalization. Each residual block likely uses kernel size like 3 and has multiple dilation rates, similar to WaveNet’s structure, ensuring a receptive field covering ~250ms or more by the last layer. The final output of the encoder is a (time_frames x 512) matrix.

- **Pre-Quantization Transformer:** Before quantization, the 512-d sequence goes through an 8-layer Transformer encoder (causal) which has context of 250 frames (10s) ([](https://kyutai.org/Moshi.pdf#:~:text=after,Both%20Transformers%20use%20causal%20masking)). It uses 8 heads, model dim 512, FFN 2048, GELU activation ([](https://kyutai.org/Moshi.pdf#:~:text=after,Both%20Transformers%20use%20causal%20masking)). This encoder refines the latent representation by capturing long-term dependencies (like phoneme identity or repeated sounds) that convs might not unify.

- **Quantizer:** Instead of doing an 8-level RVQ sequentially, Mimi does a **split: one level + parallel 7-level RVQ**. The first quantizer takes the output of the transformer encoder and produces a 512-d vector which is then vector-quantized to nearest code (out of 2048) – this is the semantic token. Then the _residual_ (difference between the pre-quantization vector and the quantized vector reconstructed) is passed into the next stage, which is a 7-level residual VQ (like SoundStream) to produce 7 more codes ([](https://kyutai.org/Moshi.pdf#:~:text=quality%20metrics%2C%20while%20human%20evaluations,scale%20STFT)) ([](https://kyutai.org/Moshi.pdf#:~:text=parameters%20can%20be%20found%20in,removing%20reconstruction%20losses%20majorly%20degrades)). However, they mention an alternate approach: “we propose a split RVQ: rather than a single RVQ with 8 levels, we distill semantic info into a plain VQ and an RVQ with 7 levels in parallel. We sum their outputs, such that while both can be used for reconstruction, we remove the constraint that acoustic info should be in residual of the semantic quantizer” (paraphrasing from text around page 12 in the paper) ([](https://kyutai.org/Moshi.pdf#:~:text=quality%20metrics%2C%20while%20human%20evaluations,scale%20STFT)) ([](https://kyutai.org/Moshi.pdf#:~:text=parameters%20can%20be%20found%20in,removing%20reconstruction%20losses%20majorly%20degrades)). This implies that after obtaining the semantic quantized vector, they _also_ feed the original latent into a 7-level RVQ that works in parallel, not strictly on the residual. Then they sum the decoded contributions from semantic and acoustic part. This could preserve the semantic info in first codebook without forcing it to also reconstruct some of the signal. The exact implementation might be: let E = encoder output, T = transformer output. Compute Q0 = VQ(T) (semantic codebook). Also compute R = E (or T) fed into a normal 7-level RVQ (with smaller dimension like 256? uncertain). Then the decoder will take both Q0 embedding and the 7 RVQ embeddings summed to reconstruct audio. This would align with their statement that the distillation loss on Q0 conflicted with recon loss if done in the same pipeline, so splitting resolves that ([](https://kyutai.org/Moshi.pdf#:~:text=not%20compati%02ble%20with%20real,negligible%20computational%20burden)) ([](https://kyutai.org/Moshi.pdf#:~:text=first%20generates%20all%20the%20semantic,semantic%20and%20acoustic%20tokens%20jointly)). In any case, the final outcome is 8 tokens per frame.

- **Decoder:** The decoder essentially inverts the encoder. It likely takes the quantized embeddings (8 code vectors of 512/Q each or combined) and first passes them through a **post-quantization Transformer** (the second transformer module they added) with the same config (8 layers, 512-d) ([](https://kyutai.org/Moshi.pdf#:~:text=Transformer,2021)). Then it applies a series of transpose conv layers (stride 2,8,6,5,4 in reverse) with residual connections to upsample back to 24 kHz. The decoder’s conv are also causal (ensuring streaming decode). They mention using WeightNorm and nonlinearities (probably LeakyReLU or GELU) in these conv blocks too ([](https://kyutai.org/Moshi.pdf#:~:text=To%20discretize%20waveforms%20into%20audio,the%20literature%2C%20and%20following%20the)).

Because Mimi is separate from Moshi during inference (it just feeds or takes tokens), one could replace Mimi with another codec if desired, but Mimi was optimized for this use. It runs at 12.5 Hz which conveniently matches typical phoneme rates.

**Computational Optimizations in Mimi:** They used vectorized operations for convs, maybe TorchScript for real-time. Mimi’s transformers use small context (250) so the cost is limited. They trained with a batch size 128 of 12s clips for 4M steps – which is extremely heavy (128*4M*12s = ~170k hours of audio seen) ([](https://kyutai.org/Moshi.pdf#:~:text=,codebook%20size%20of%20NA)) ([](https://kyutai.org/Moshi.pdf#:~:text=weights%20with%20a%20decay%20of,While%20the%20latent%20dimension%20is)) but not the full 7M hours presumably (so they likely did multiple epochs or a subset). The quantizer dropout (50%) means half the time they bypass quantization and feed continuous latents to decoder – implemented likely by randomly not quantizing some samples.

##  Input/Output Representations and Tokenization

**Text Tokenization:** Moshi uses a SentencePiece unigram model with 32k vocabulary trained on English text ([](https://kyutai.org/Moshi.pdf#:~:text=2020,We)). All text (prompts, transcripts, etc.) is encoded with this tokenizer. They handle numbers by splitting digits (so “2025” -> “2 0 2 5”) to ensure each digit is preserved (because a large number might not appear enough to get its own token) ([](https://kyutai.org/Moshi.pdf#:~:text=2020,We)). For Unicode characters or out-of-vocab items, it falls back to byte encoding (byte-level BPE) so nothing is truly out-of-vocab ([](https://kyutai.org/Moshi.pdf#:~:text=tokenizer%20is%20based%20on%20the,We)). This is standard to robustly handle any input text.

**Audio Token representation:** Each audio 80ms frame for each speaker is represented by up to 8 tokens: essentially an 8-dimensional token vector. In Moshi’s data pipeline, these are often handled as separate token IDs with a known grouping. One can imagine flattening it as 8 consecutive tokens in the sequence (which is what you’d do if not using the RQ-Transformer approach). But since they do RQ-Transformer, they treat them differently. The training code likely creates something like: for each time step s, create a structured token object that includes maybe a placeholder for a text token and Q tokens for each codebook for user and Q for system. They then linearize those for feeding into the model with special masking. In the HuggingFace conversion, they actually flatten everything and rely on the model architecture to know how to group them.

**Speaker differentiation:** The user and system audio tokens need to be distinguished. The architecture does this inherently by using separate input embeddings for each stream’s tokens. So, for example, the embedding table for “codebook 3 of Moshi” is distinct from “codebook 3 of User.” Thus, even if the token ID (0-2047) is the same number, the vector added to Helium is different. This effectively acts like a learned speaker embedding. Additionally, they could incorporate a special tag at the start of the sequence indicating who is speaking first, etc., but they didn’t explicitly mention using a role token. Instead, the network learns from context which stream corresponds to the AI vs user. In the synthetic data, they always assigned one stream as Moshi and one as user with random pick who’s first ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L1318%20For%20both,For)), so the model doesn’t overfit to always one pattern (they sometimes swapped which side starts) ([](https://kyutai.org/Moshi.pdf#:~:text=match%20at%20L1318%20For%20both,For)).

**Combining Text and Audio in one sequence:** During inner monologue segments, text tokens (from the same 32k vocab) appear interspersed with audio tokens. To avoid confusion, they likely treated text tokens effectively as another “codebook” or stream in the model. However, since Helium is naturally outputting text, they might simpler: when Moshi is about to speak a word, they output the text token in the sequence. For alignment, they might insert padding tokens in between audio tokens to hold the place of text in the timeline. The HuggingFace guide gave an example: to align “Hello, I’m Moshi” with audio, they might represent it as `"Hello,<pad><unk>I'm Moshi"` ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=The%20original%20model%20is%20synchronized,in%20between%20each%20token%20enunciation)). This indicates they padded between words so that each text token aligns to roughly the moment that word is spoken. Possibly <unk> was used as a timing token. The details are a bit hacky, but the principle is: the model can output a text token at time s and no audio tokens at that exact moment, then audio tokens in subsequent steps. In practice, one can implement inner monologue by having a special “text stream” in parallel that only the system uses. The architecture diagram suggests “Text” comes out of Depth Transformer as well (they showed a separate orange square labeled Text coming from Depth conditioned on context). It’s possible they did extend Depth Transformer to also generate a text token at codebook slot 0 (like before the semantic codebook 1). If so, Depth’s first token could be from a different vocabulary (text vocab). This is tricky to do in one softmax. Alternatively, Helium might generate the text token itself and then they inject it. Given the complexity, perhaps they kept it simple: Helium deals with text, Depth deals with audio only. In fine-tuning with transcripts, they just trained Helium to output those transcript tokens at appropriate times (with some small architectural addition to align them).

Regardless, from an implementation perspective, one could implement inner monologue by having a **third stream** for Moshi’s text, which Depth doesn’t need to generate (since Helium’s output is basically those text tokens). At inference, you could run Helium one step further ahead to get a text token, then feed that as context while generating audio. This area is a bit advanced, but it’s not strictly necessary to replicate unless one is aiming for the exact fidelity – one could also choose to omit inner monologue (the model will still produce audio, just slightly less coherently).

**Prompt Engineering and Context Handling:** When starting a conversation, one can provide an initial text prompt or system message (e.g. “You are Moshi, an AI assistant. Have a conversation…”). This would be tokenized and fed as `input_ids` to Moshi with no audio tokens yet. Moshi was instruction-tuned, so it knows how to behave given an instruction like “Moshi: … User: … etc.” It’s unclear if Kyutai uses explicit role tokens or just relies on separate streams for roles. In text fine-tuning (OpenHermes etc.), they might have included prompts like “[System: ...][User: ...][Moshi: ...]”. If those were included, the model would expect some role indicator tokens. The documentation doesn’t mention special tokens, so they might not use any explicit token like “<|user|>” or such, relying purely on audio vs text channel to delineate roles. For safety, an implementer could include a system prompt text at start such as: “(The user speaks in audio. Moshi hears and responds in a friendly manner.)” – but since the model was fine-tuned on interactive data, it likely doesn’t need much prompting to behave. The open-source model presumably comes with some default persona (Moshi’s voice and style).

**Context management:** Moshi has a 4096 token context which includes both text and audio tokens. Since audio tokens come at 12.5 per second _per stream_, in one second you get up to 25 audio tokens (user+system) plus maybe a couple text tokens. So in one minute (~60 sec), that’s at most 1500 tokens – well within 4096. If a conversation goes on for more than ~4-5 minutes continuously, then the context may hit the limit. At that point, one strategy is **sliding window**: drop the oldest tokens as new ones come in. However, dropping blindly could remove important long-term context (like the conversation topic). Ideally, one would keep some summary of the history. Possibly, since Helium is a strong LLM, one could periodically insert a summary text token that compresses old dialogue (just speculation). The authors haven’t detailed this, but for real implementation, one may implement a rolling buffer. Most likely, 4096 is enough for normal interactions (few mins before a reset or concluding the session).

**Architectural Innovations Recap:**

- The **hierarchical RQ-Transformer** (Temporal + Depth) is a novel application in audio sequence modeling, drastically cutting sequence length and enabling full-duplex token handling ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=We%20then%20augment%20Helium%20with,art%20audio%20language%20modeling%20performance)) ([](https://kyutai.org/Moshi.pdf#:~:text=Figure%203%3A%20Architecture%20of%20the,a%20smaller%20Depth%20Transformer%20over)).
- The integration of **semantic token distillation** into a streaming codec (Mimi) is innovative – prior works like AudioLM used a separate semantic tokenizer that was not causal ([](https://kyutai.org/Moshi.pdf#:~:text=these%20acous%02tic%20tokens%20provide%20appropriate,conditioning%2C%20by%20using%20semantic%20audio)) ([](https://kyutai.org/Moshi.pdf#:~:text=prefix%20to%20predicting%20acoustic%20tokens,into%20the%20tokens%20produced%20by)). Mimi provides a unified token space that carries semantic info in a causal way.
- **Inner Monologue** as implemented is an extension of ideas from AudioLM (which used semantic tokens as intermediate) and SpeechGPT (which did text then speech in a chain) ([](https://kyutai.org/Moshi.pdf#:~:text=removal%20of%20explicit%20speaker%20turns%2C,to)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=contribution%20is%20the%20Inner%20Monologue%2C,linguistic%20information)). Moshi instead folds a text generation step into an otherwise speech model _without_ breaking streaming – this is quite new.
- The **multi-stream modeling** allowing overlapping speech is a significant departure from the turn-based dialogue assumption in virtually all prior dialog systems ([](https://kyutai.org/Moshi.pdf#:~:text=information%20that%20modifies%20meaning%E2%80%94%20such,of%20the%20user%20into%20parallel)) ([Moshi open-source release: run Moshi locally!](https://kyutai.org/2024/09/18/moshi-release.html#:~:text=Our%20main%20contributions%20to%20generative,to)). This required creative training data generation and careful architectural design to avoid interference between streams.
- In terms of trade-offs: Moshi chooses a moderately sized backbone (7B) to balance reasoning and speed. Larger LLMs (like 70B) might produce more accurate answers but would be far too slow to run 12.5 steps/sec on current hardware. So 7B was a sweet spot for real-time; indeed they mention Moshi’s knowledge is somewhat limited on complex tasks or tool use ([Kyutai Launches Moshi, an Open Source Alternative to OpenAI's Advanced Voice Model](https://analyticsindiamag.com/global-tech/kyutai-releases-moshi-an-open-source-voice-model-ahead-of-openai/#:~:text=On%20Apple%20Mac%2C%20the%20model,integration%2C%20according%20to%20the%20company)), but it handles casual dialogue well. Another trade-off is the **audio codec bitrate**. They targeted 1.1 kbps, which is ultra-low. This keeps token rates low but means some audio quality loss (e.g., maybe the voice can sound a bit artificial or muffled at times). They did find training tricks to maximize quality at that bitrate, but still, to an audiophile, 1.1 kbps is low. In fact, the open model apparently runs at 4.4 kbps (32 codebooks) for better audio, at the cost of 4x more tokens ([marcodsn.me](https://marcodsn.me/posts/exploring-mimi#:~:text=Although%2C%20here%20we%20find%20another,1kbps%20unreleased%20model)). So one could trade latency for quality by adjusting codebooks. The architecture allows that scaling: increasing Q (codebooks) improves fidelity linearly but also increases Depth gen time and token rate. They stuck with Q=8 for research, but open users might use Q=32 for nicer sound if their GPU can handle ~50 tokens/sec instead of 12.5.

- Another innovation: **adversarial-only training for codec** – not directly an architecture thing, but a training innovation that yielded very natural sound at low bitrates ([](https://kyutai.org/Moshi.pdf#:~:text=Tagliasacchi%20et%20al,a%20remarkable%20boost%20in%20audio)). And the use of **quantizer dropout** to effectively train one model that can operate at multiple bitrates (just drop some quantizers and it still works) means Mimi could be used in variable bandwidth scenarios.

In conclusion, Moshi’s architecture is a complex orchestration of components, each specialized yet working together. Implementing it from scratch would involve constructing the Helium and Depth Transformers as described, integrating Mimi or an equivalent codec for I/O, and carefully handling the multi-modal sequence formatting. The clear modularity (separate classes for Helium LM, Depth LM, Mimi codec) makes it feasible. HuggingFace’s integration demonstrates that it can be broken into `MoshiForCausalLM` (the Helium part), a `DepthDecoder`, and `MimiModel` for the codec ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=1,in%20the%20paper)) ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=3)). They even provide a conversion script, meaning the architecture is now understood to the point it can be reimplemented in standard transformer libraries ([Moshi](https://huggingface.co/docs/transformers/en/model_doc/moshi#:~:text=Tips%3A)).



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Inference and Processing](inference.md)
* Next: [Implementation Details](implementation.md)

