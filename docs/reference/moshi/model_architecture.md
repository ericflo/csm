# Model Architecture Details

In this section, we delve deeper into Moshi's model topology, providing a detailed breakdown of each component and how they interact. We discuss various architectural innovations, parameter counts and efficiency, input/output representations, and how Moshi handles context and prompting. This is effectively a **blueprint** of Moshi's neural network, suitable for an engineer wanting to implement it.

## Topology and Component Interaction

Moshi's overall architecture can be visualized as a **stack of modules** forming a pipeline:

```
Mimi Encoder → Temporal Transformer (Helium) → Depth Transformer → Mimi Decoder
```

### Inputs and Outputs

- **Inputs**: Raw audio waveforms (user speech) and optionally text prompts
- **Outputs**: Raw audio (Moshi's speech) and optionally recognized text

### Internal Representations

Internally, everything is represented as token sequences:

- **Audio tokens**: Represented as `(batch, num_codebooks, seq_len)`
  - For ease, the codebook dimension can be flattened when feeding into the model
  - Each codebook position has distinct embeddings

- **Text tokens**: Standard 1D token sequence `(batch, seq_len)`

## Helium (Temporal Transformer)

Helium is a Decoder-Only Transformer designed to process the temporal sequence of dialogue.

### Architecture Details

- **Size**: 32 layers, 4096-dimensional model, 32 attention heads (each head ~128-dimensional)
- **Design**: Closely resembles a GPT-style or LLaMA model
- **Context**: Maximum context window of 4096 tokens

### Input Embedding Mechanism

At any inference step, Helium takes a sequence of embeddings representing the history of all streams up to the previous timestep. These embeddings are formed from:

1. **Text Tokens**: A learned word embedding table of size 32k
   
2. **Audio Tokens**: A separate embedding mechanism where:
   - Each time step can have up to Q=8 tokens per stream (potentially 2 streams)
   - Each codebook index (1 through Q) and each speaker (user or system) has a unique embedding matrix
   - These matrices map token IDs (0–2047) to vectors
   - Vectors for all tokens present at a time step are summed to form the input for that step

3. **Positional Encoding**: Rotary Positional Embeddings are applied to handle temporal positions
   - A trainable vector may be used for the start token (V0)

### Internal Processing

- **Self-Attention**: Multi-head self-attention operates over the sequence of timestep embeddings
  - Uses FlashAttention for efficiency (optimized softmax attention kernels)
  - Attends up to 4096 positions back in time

- **Feed-Forward**: Uses gated SiLU linear units
  - Effectively doubles the channel width then applies gating
  - Increases non-linearity in the model

- **Output**: For the current time step _s_, the output embedding for that position (context vector z_s) is extracted
  - Instead of multiplying by a language head for logits (as in standard Transformer LMs)
  - z_s is passed to the Depth Transformer as conditioning

## Depth Transformer

The Depth Transformer is another decoder-only Transformer specialized for generating token sequences at a single time step.

### Architecture Details

- **Size**: Much smaller than Helium (6 layers, 1024 model dimension)
- **Sequence Length**: Equal to the number of token slots per time step (K)
- **Function**: Generates tokens within a single time step conditioned on the temporal context

### Operation

The Depth Transformer generates over the codebook dimension with context length = num_codebooks. This implies:

- Each codebook index 1..Q is treated as a "position"
- Inner monologue text (if present) might be treated as position 0 or a separate condition
- **Cross-attention** is likely employed to incorporate z_s (the context from Helium)
  - Context vector can be prepended as a "memory" for each layer to attend to
  - Similar to how encoder-decoder Transformers work

### Generation Process

1. **Initial Input**: A special start token or zero embedding
2. **First Token Generation**: Produces the semantic code or text token
3. **Subsequent Tokens**: The first token's embedding is fed back to generate token 2, and so on
4. **Completion**: Process continues until Q tokens are generated for the time step

### Training vs. Inference

- **Training**: All Q tokens at time s are known, so they can be input together with causal masking
  - Token k only attends to tokens < k within that time step
  
- **Inference**: The Depth Transformer:
  - Attends to z_s (which encodes all prior context)
  - Attends to already generated tokens of the current step
  - Outputs logits over the audio codebook vocabulary (size 2048) for each position

## Multi-Stream Handling

Moshi's architecture uniquely handles parallel audio streams for both user and system.

### Stream Management

- **System Stream**: Generated by the model (Temporal + Depth Transformers)
  - Tokens are handed to Mimi's decoder to produce sound

- **User Stream**: Provided as input, not generated by the model
  - User tokens are fed as part of the input context for future steps
  - They aren't included in the generated sequence at the current time step
  
### Joint Modeling Approach

The model can be configured to model both streams jointly:
- This enables prediction of a joint distribution P(user, system)
- During pretraining, this allows modeling of general conversation flow
- In inference, user token predictions are overridden with actual input from the microphone

### Practical Implementation

As reflected in the HuggingFace interface:
- `user_audio_codes` are provided as input, not generated
- `moshi_audio_codes` are generated by the model
- For safety, actual user input always replaces what the model would predict for user tokens

## Architecture Summary

The architecture interaction can be summarized as:

1. At each time step:
   - Helium (Temporal) consumes the previous time's combined token information
   - Helium outputs context vector z_s
   - Depth (conditioned on z_s) generates Moshi's current audio tokens
   - User's current audio tokens (if any) are fed in externally

2. Time is incremented and the process repeats

3. Both Transformers use causal masking:
   - Helium masks future in time
   - Depth masks future in codebook order

4. The components share no parameters (Helium and Depth are separate)

The whole system forms a unified autoregressive model implemented as two nested transformers.

## Helium and Depth Transformer Details

### Helium (Temporal) Transformer Specifications

As a typical decoder-only Transformer, Helium includes:

- **Attention Mechanism**: Multi-head self-attention with formula:
  ```
  Attention(Q,K,V) = softmax(QK^T/√d)V
  ```

- **Layer Configuration**:
  - 32 attention layers
  - 32 heads per layer (dimension 128 each)
  - Model dimension (d_model) = 4096
  - 32 feed-forward layers of dimension 11264

- **Activation Function**: GLU activation with SiLU
  - Two linear projections: 11264→4096
  - One projection is gated by the other via elementwise multiplication after SiLU

- **Normalization**: Pre-normalization with RMSNorm
  - Each sub-block (attention or FFN) is preceded by RMSNorm
  - Initial RMSNorm on embeddings and final RMSNorm before output

- **Positional Encoding**: Rotary Position Embedding (RoPE)
  - Multiplies q, k vectors by a rotation matrix based on position
  - Encodes position angles implicitly without adding to embeddings

- **Output**: 4096-dimensional vector at each position
  - In Moshi, this output is consumed by the Depth Transformer
  - Helium can still produce text directly if needed (e.g., for inner monologue)

### Depth Transformer Specifications

The Depth Transformer has a more compact architecture:

- **Layer Configuration**:
  - 6 layers
  - 16 attention heads per layer
  - Model dimension (d_model) = 1024
  - Each head is approximately 64 dimensions

- **Context Handling**: The Depth Transformer incorporates Helium's context vector z_s
  - May use cross-attention in each layer to attend to z_s
  - Alternatively, might concatenate z_s to the keys and values of self-attention
  - Another approach: add z_s to self-attention output in a gated manner

- **Positional Encoding**: Context length equals the number of codebooks (8)
  - May use simple positional embeddings due to the small fixed sequence length

- **Conditioning**: Conceptually, Depth is conditioned on Helium's output
  - This enables the hierarchical generation of audio tokens

## Input/Output Representations and Tokenization

Moshi's effectiveness depends heavily on how it represents and processes different types of data.

### Text Tokenization

- **Tokenizer**: SentencePiece unigram tokenizer
- **Vocabulary Size**: 32,000 subword tokens
- **Special Features**:
  - Byte fallback to avoid unknown tokens
  - Numbers split into individual digits for better preservation of information
  - Speaker identifiers encoded directly in the text

### Audio Tokenization (Mimi Codec)

- **Frame Rate**: 12.5 Hz (80ms per frame)
- **Codebook Structure**:
  - 1 semantic codebook (high-level linguistic content)
  - 7 acoustic codebooks (fine-grained audio details)
  - Each codebook has 2048 possible values (11 bits per token)

- **Representation**: Audio is represented as a sequence of:
  ```
  [c₀,₁, c₁,₁, ..., c₇,₁, c₀,₂, c₁,₂, ..., c₇,₂, ...]
  ```
  Where c₍ᵢ,ⱼ₎ is the token for codebook i at time frame j

### Multi-Stream Representation

- **User Stream**: Represented as a sequence of audio tokens
  ```
  U = [u₀,₁, u₁,₁, ..., u₇,₁, u₀,₂, u₁,₂, ...]
  ```

- **System Stream**: Represented as audio tokens plus optional text
  ```
  S = [t₁, s₀,₁, s₁,₁, ..., s₇,₁, t₂, s₀,₂, ...]
  ```
  Where t₍ᵢ₎ represents optional inner monologue text tokens

### Embedding Space

- **Audio Tokens**: Each position in each codebook has its own embedding matrix
  - Maps discrete token IDs (0-2047) to continuous vectors
  - Different embeddings for user vs. system tokens

- **Text Tokens**: Standard embeddings from a lookup table
  - 32,000 vocabulary size × embedding dimension
  - Same embedding mechanism as used in base LLaMA or similar models

These detailed representations allow Moshi to effectively process and generate both text and speech in a unified framework, handling the complexities of real-time dialogue with high fidelity.

---

## Navigation

* [Back to Index](index.md)
* Previous: [Inference and Processing](inference.md)
* Next: [Implementation](implementation.md)