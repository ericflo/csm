## 1. Introduction to Sesame AI Lab and CSM

Sesame AI Lab is a research team focused on advancing AI-driven voice technology to achieve what they call “voice presence” – the feeling that a machine’s voice is as genuine and engaging as a human’s ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Achieving%20voice%20presence)). Traditional voice assistants and text-to-speech systems often sound neutral and lack emotional depth, which makes interactions feel flat over time ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Today%E2%80%99s%20digital%20voice%20assistants%20lack,the%20initial%20novelty%20wears%20off)). To address this, Sesame AI Lab set out to create AI companions that can carry _interactive, context-aware conversations_ with natural prosody and emotion. Their Conversational Speech Model (CSM) is the centerpiece of this effort, designed to generate speech that not only has high audio quality but also responds appropriately to the conversational context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)).

CSM’s development was driven by key innovations in conversational speech generation. First, it frames speech synthesis as an **end-to-end multimodal problem**: unlike conventional two-stage pipelines (where a model predicts a transcript or prosody tokens and a separate vocoder produces audio), CSM directly generates audio tokens from text and audio context in a single integrated process ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)). This _single-stage_ approach improves efficiency and expressivity by allowing the model to jointly consider linguistic content and acoustic context when producing speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)). Second, CSM is explicitly designed to leverage **conversation history** – it looks at previous dialogue (both what was said and how it was said) to inform the speech it generates ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)). This helps solve the “one-to-many” mapping problem in speech generation: a given sentence can be spoken in many ways, and by providing context (tone, tempo, recent dialogue), CSM can choose a rendition that fits naturally ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)). In practice, CSM can adapt its speaking style to the situation – for example, sounding sympathetic if the prior user turn was sad, or more energetic if the conversation is upbeat. Another innovation is Sesame’s focus on new evaluation metrics and realistic benchmarks for conversational speech. They found that common metrics like word error rate or speaker similarity are saturated (modern TTS models already score near-human) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,human%20performance%20on%20these%20metrics)), so they introduced context-specific tests (like disambiguating homographs through context, and maintaining pronunciation consistency in dialogue) to measure CSM’s understanding of context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=faithfulness%20to%20text%2C%20context%20utilization%2C,study%20using%20the%20Expresso%20dataset)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20better%20assess%20pronunciation%20and,based%20benchmarks)). These efforts underscore that CSM isn’t just about generating **audio** – it’s about generating the **right** audio for a given conversational moment.

## 2. Machine Learning Model Architecture and Layout

**Overall Architecture:** CSM uses a _dual-Transformer_ architecture that operates on interleaved text and speech token inputs ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). It consists of two autoregressive Transformer models working in tandem: a large **Backbone Transformer** and a smaller **Audio Decoder Transformer** ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). The backbone is a multimodal model that processes both textual tokens (from the conversation transcript) and audio tokens (from previous speech, represented as discrete codes) in a single sequence. Its role is to model the high-level content and context – effectively understanding “what to say and how to say it” at a coarse level. The output of the backbone at each time step is not directly speech, but the first level of an audio code (the **zeroth codebook token**) which captures the semantic and prosodic content of the next chunk of speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). The second model, the audio decoder, then takes this predicted code and generates the remaining audio codebook tokens needed to produce the final speech waveform for that time step ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). In essence, the backbone produces a _context-aware linguistic+prosodic summary_ of the next bit of speech, and the decoder “fills in” the fine acoustic details to make it sound realistic ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). This division of labor allows CSM to remain end-to-end (the whole system is trained jointly) while being efficient: the heavy context modeling is done in one place, and the fine waveform reconstruction in another, smaller model ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)).

([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice)) _CSM’s dual-Transformer architecture at inference. Text tokens (orange `T`) and audio tokens (green `A`) from the conversation history are interleaved and fed into the large Backbone Transformer, which predicts the next semantic audio token (codebook 0). The smaller Audio Decoder then generates the remaining acoustic tokens (codebooks 1 through N−1) needed for that frame ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). The full set of audio tokens (`A`) for that frame is decoded to speech, and also fed back into the backbone for predicting subsequent tokens, until an end-of-utterance token is produced._ ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation))

**Dual-Transformer Framework:** The backbone and decoder operate in a loop to produce conversational speech. During generation, the model maintains a sequence of tokens representing the dialogue so far: this includes text tokens for things that were said, plus audio tokens for how they were said. At a given step, backbone takes in the recent text prompt (the new sentence it needs to speak) along with prior context tokens, and autoregressively emits the next audio code (semantic code) for the response ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)). The decoder immediately takes that code and generates the detailed acoustic codes (e.g. timbre, fine intonation) for the corresponding audio frame ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)). Once the decoder outputs are obtained, they are assembled into a complete audio frame which can be converted to a short waveform segment. Crucially, that audio token (`A` in the diagram) is fed back into the backbone’s input sequence for the next iteration ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20model%20inference%20process,audio%20and%20text%20transcription%20tokens)). This means the backbone always has access to the _up-to-date audio history_ of the conversation, including the very speech it has generated so far. The process repeats token by token until the model produces a special end-of-utterance token, indicating the response is complete ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20model%20inference%20process,audio%20and%20text%20transcription%20tokens)). Because the decoder is much smaller and faster, this two-stage autoregressive generation is still low-latency – the backbone does the complex part only once per frame, and the decoder’s work is quick to compute ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). The result is an integrated but efficient pipeline: by splitting at the semantic code (codebook 0), the system avoids having a single massive model generate _all_ waveform details sequentially (which would be slow) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=RVQ,time%20scenario)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). Instead, the backbone focuses on long-range coherence and context, while the decoder focuses on high-fidelity speech synthesis.

**Encoder-Decoder Analogy:** Although CSM’s architecture is autoregressive rather than a traditional encoder-decoder, you can think of the backbone+decoder division as analogous to an encoder-decoder system. The backbone “encodes” the text and prior audio context into a semantic audio representation, and the decoder “decodes” that into actual speech tokens. However, both are generative Transformers running in sequence each time step, rather than one encoding a fixed input and the other generating an output all at once. This design proved advantageous for modeling conversational speech: the backbone Transformer can attend over a long history of both modalities (text _and_ audio) to decide the manner of speaking, while the decoder Transformer, conditioned on that decision, ensures the speech comes out sounding natural and in the target voice ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). All components are trained together, so the backbone learns to produce semantic codes that are easy for the decoder to reconstruct into audio.

**Comparison to Moshi (Kyutai Labs):** The Moshi system by Kyutai Labs also uses a dual-transformer concept, but with a different layout and objectives. Moshi is built as a _full-duplex spoken dialogue model_, meaning it can listen and speak simultaneously in real time ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=More%20fundamentally%2C%20Moshi%20is%20an,integrated%20multimodal%20modeling%20of%20Moshi)). It employs a **global-local transformer architecture**, sometimes referred to as a Temporal vs. Depth transformer structure ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). In Moshi, a large 7B-param model (codenamed “Helium”) handles temporal sequence modeling (the global structure of the conversation over time), and a separate small “Depth” transformer handles the inter-codebook dependencies _within each audio frame_ ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)). This is conceptually similar to CSM’s backbone vs. decoder split, but Moshi’s design is even more specialized: the Depth transformer in Moshi is responsible for producing multiple acoustic codebooks in parallel for each time step (reducing the need to generate them one-by-one), while the Temporal transformer looks after the content of what’s being said and when ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). Additionally, Moshi integrates speech recognition and text generation into its architecture – it actually predicts text tokens (transcription) for the AI’s own speech as it generates it, as a form of _internal language modeling_ to guide the speech generation ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=Moshi%20models%20two%20streams%20of,to%20its%20own%20speech%2C%20its)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). CSM does not do this; it doesn’t generate any text internally (it only consumes text input and produces speech) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). As a result, Moshi can operate without a separate NLP brain – it can decide the dialogue and speak it, whereas CSM is usually paired with a separate large language model for deciding what to say ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). In terms of latency and real-time performance, Moshi’s architecture is highly optimized for streaming. By using the Mimi codec at 12.5 Hz and parallel codebook generation, it achieves as low as ~200 ms response latency on appropriate GPU hardware ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)), even allowing interruptions mid-speech. CSM, while efficient for a non-streaming model, typically achieves ~380 ms latency on a high-end GPU for an utterance (it generates slightly slower and in a turn-based fashion) ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). Both architectures share the principle of splitting high-level content modeling from low-level acoustic modeling, and both leverage the same audio codec (Mimi) – but Moshi blurs the line between “speech model” and “language model,” whereas CSM cleanly separates them (CSM strictly focuses on speech generation, delegating language understanding to other systems).

## 3. Technical Components of CSM

**Backbone Transformer (LLaMA-based):** The backbone of CSM is a Transformer network based on the LLaMA architecture (a GPT-style decoder-only Transformer) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). It serves as the main _sequence model_ that reads the combined text and audio token sequence. In the largest “Medium” configuration, the backbone has around 8 billion parameters ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,Medium%3A%208B%20backbone%2C%20300M%20decoder)) (smaller variants with 1B or 3B params were also trained for research). Architecturally, it inherits typical features of LLaMA/GPT: multiple self-attention layers, each with many heads, and feed-forward networks, optimized for autoregressive token prediction. Sesame did **not** initialize this transformer from any pre-trained text model – it was trained from scratch on the conversational speech data ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)). This means the backbone had to learn linguistic patterns, contextual cues, and basic world knowledge directly from the audio transcripts (which is feasible given the massive 1M hour dataset). The backbone uses a SentencePiece tokenizer (or similar) for text, likely the same as LLaMA’s tokenizer, to convert input text to tokens ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). Audio, on the other hand, is tokenized by the Mimi codec (described below). To allow the model to distinguish modalities, special embeddings or token IDs are used so that the backbone knows which tokens are text and which are audio. Additionally, Sesame encodes **speaker identities** directly into the text token sequence as markers ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). For example, they might prepend a token indicating “Speaker A” vs “Speaker B” before each segment’s text, so the model can learn to speak in different voices or styles for different speakers. The backbone’s primary job is to output the correct _zeroth codebook token_ for the next audio frame given all the preceding context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). In doing so, it must integrate conversational context (what has been said) and paralinguistic context (how it was said) to decide the appropriate prosody, timing, intonation, etc., for the upcoming speech. It also handles long-range coherence – with a context window of 2048 tokens (~2 minutes of audio/text) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)), the backbone can retain conversational memory and style over extended dialogues. This long context capability is crucial for maintaining consistency in things like tone or persona over the course of a chat. (For instance, if a user asked a question 1.5 minutes ago and the assistant is still answering, CSM can remember that far back to keep the answer relevant and in the same voice.) The backbone uses positional encodings (likely rotary position embeddings as in LLaMA) to manage such a long sequence. Overall, this transformer acts as the **brain of CSM**, figuring out _what_ audio should sound like before the finer details are filled in by the decoder.

**Contextual Understanding & Long-Form Dialogue Retention:** A standout feature of CSM is its ability to utilize **long conversational context**. Each training sample fed to the model was a sequence of up to 2048 tokens, corresponding to roughly 2 minutes of a conversation ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)). These sequences were constructed by alternating between speakers: e.g., [Speaker0’s text + audio tokens][Speaker1’s text + audio tokens][Speaker0’s text + audio tokens]… and so on ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). By training on such data, the backbone learns to interpret not just the last user utterance, but the entire history of what both it and the user have said. It retains information like the _dialogue state_ (has a question been answered already?), the _emotional tone_ of each speaker (was the user sounding frustrated or pleased?), and even speaking quirks or pace. Mechanisms like self-attention allow the model to attend to relevant parts of the history. For example, if the assistant is about to clarify something, the backbone might attend to the earlier part of the conversation where that topic was first raised, ensuring the clarification tone matches the context. This goes beyond what typical TTS models do – CSM isn’t generating speech in isolation, but as a turn in an ongoing exchange. As a result, it can exhibit behaviors like **dialogue coherence** (sounding hesitant when appropriate, or using a firmer tone if repeating an answer). The long context also facilitates _long-form synthesis_ (up to 2 minutes straight) without resetting style. In testing, the Medium CSM model showed that including conversational context significantly improved human preference for its responses’ appropriateness ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=The%20graph%20above%20shows%20the,However%2C%20when%20context%20is%20included)). Essentially, by remembering up to 2 minutes of dialogue, CSM provides continuity in multi-turn interactions that makes the voice assistant feel more “present” and aware. From an implementation perspective, handling 2048 token sequences with an 8B model is non-trivial (it’s a lot of computation), but Sesame’s training strategies (discussed later) and model optimizations made it feasible.

**Residual Vector Quantization (RVQ) Tokenizer – Mimi:** Instead of producing raw audio waveforms, CSM outputs **discrete audio tokens** thanks to a technique called Residual Vector Quantization. The team uses _Mimi_, a state-of-the-art neural audio codec developed by Kyutai (and adopted by Sesame for CSM) to convert audio into tokens and back ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)). Mimi works by encoding 24 kHz audio into a sequence of code vectors at 12.5 Hz (i.e., one set of codes every 80 ms of audio) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=dialogue%20framework,3kbps)). Each 80 ms frame of audio is represented by a stack of codebook indices. In Mimi’s case, there is **1 semantic codebook** (often referred to as codebook 0) and **N−1 acoustic codebooks** (codebooks 1 through N−1 for finer detail) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). For example, Mimi might use a total of 8 or 10 codebooks – one of them (the first) captures high-level features of the sound, and the rest capture the residual nuances. The semantic code (zeroth codebook) is trained to represent the content and broad prosody of speech in a compact way. In fact, Mimi’s training uses a form of **distillation from a self-supervised speech model (WavLM)** so that codebook 0 tokens carry linguistic/semantic information akin to a transcript, but also some prosodic cues ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). The remaining acoustic codebooks encode the additional information needed to reconstruct the original waveform with high fidelity – things like the exact voice timbre of the speaker, background noise, subtle inflections, etc. ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=phonetic%20features,specific%20identity%20and%20timbre)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=2,specific%20identity%20and%20timbre)). This RVQ scheme means that any piece of audio can be turned into a sequence of discrete tokens: one token per codebook per 80 ms frame. Conversely, given those tokens, a decoder (the Mimi vocoder) can synthesize speech that sounds very close to the original. For CSM, using Mimi tokens has several advantages. _First_, it transforms the continuous audio generation problem into a _discrete sequence prediction_ problem suitable for Transformers ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=One%20approach%20to%20modeling%20audio,two%20types%20of%20audio%20tokens)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Residual%20Vector%20Quantization%20%28RVQ%29%20,specific%20identity%20and%20timbre)). _Second_, by splitting the information across a semantic token and acoustic tokens, it allows the model to focus on high-level correctness first and detail second. The semantic token acts almost like a “pseudo-word” that the backbone can predict, representing what needs to be conveyed in sound ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=A%20common%20strategy%20first%20models,this%20during%20training%20is%20challenging)). _Third_, Mimi is highly efficient – it compresses speech down to about 1.1 kbps (kilobits per second) while preserving quality ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)). That low bitrate translates to far fewer tokens per second of audio (e.g., 12.5 tokens/sec for each codebook, so if there are ~8 codebooks, that’s on the order of 100 tokens/sec in total, whereas a naive 16 kHz PCM would be 16,000 samples/sec). Fewer tokens means the Transformers have less work to do to model a given duration of speech. Additionally, Mimi is _streaming-capable_ and causal by design (it encodes and decodes one frame after another without needing future context) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=dialogue%20framework,3kbps)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)), which aligns with CSM’s real-time goals. In summary, the Mimi RVQ tokenizer is the foundation that makes CSM possible: it provides the “vocabulary” of audio tokens. CSM’s training included teaching the model to predict these Mimi tokens directly. By operating on this discrete audio representation, CSM can handle aspects like speaker identity and audio style transfer inherently – e.g., if the context contains a certain speaker’s tokens, the model learns to continue using similar acoustic tokens to maintain that voice ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=2,specific%20identity%20and%20timbre)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). The semantic token (codebook 0) is especially important for bridging text and speech: since it captures phonetic content, the backbone essentially learns a mapping from text+context to a pseudo-phoneme representation. The subsequent acoustic tokens then ensure the phonemes are rendered in the correct voice and style.

**Audio Decoder and Multi-Codebook Generation:** The Audio Decoder in CSM is a smaller Transformer network dedicated to predicting the acoustic codebooks (levels 1 through N−1) given the backbone’s output. In the Medium model, the decoder has about 300 million parameters (much smaller than the 8B backbone) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,Medium%3A%208B%20backbone%2C%20300M%20decoder)). Despite its size, it’s crucial for producing high-quality speech because it generates the fine detail tokens. The decoder is conditioned on two things: (1) the backbone’s hidden representation or output embedding for the current time step (which conveys the semantic code and context info), and (2) any already-predicted lower-level codes for the current frame. One way to implement this is to feed the decoder the backbone’s predicted code0 along with an indication of which codebook level it’s currently predicting. Sesame’s design uses a **distinct linear output head for each codebook level** in the decoder ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). In other words, the decoder has multiple output projections sharing the same Transformer body – one head is responsible for predicting code1, another for code2, etc. During inference, the decoder can iterate through code1 to codeN−1, each time using the corresponding head to output a token ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). Likely, the decoder is run autoregressively across the codebooks (often referred to as running across “depth”), meaning it predicts code1, then takes that prediction as input to predict code2, and so on, until all acoustic codes for that frame are generated. (This sequential approach within a frame ensures that each higher codebook can depend on the lower codebooks, capturing the residual nature of RVQ ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=RVQ,suitable%20for%20offline%20applications%20like)).) However, because N−1 is relatively small (e.g., if N=8, then 7 acoustic levels), this process is very fast and could even be unrolled in a single forward pass if implemented cleverly. Training of the decoder is done jointly with the backbone: the decoder learns to minimize the error in predicting the true acoustic tokens given the true code0 (during teacher forcing) and any previously predicted lower-level codes. An interesting aspect is that the decoder’s **time dimension** is basically the number of codebooks, not the length of the utterance – it models a short sequence (the stack of codes for one frame) at a time, conditioned on the backbone output for that frame. This is why it can be much smaller; it doesn’t need to model long temporal dependencies, those are handled by the backbone. By keeping the decoder lightweight, CSM achieves low latency generation: once code0 is available for a frame, the decoder can very quickly spit out the rest of the codes ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). This design is what makes CSM **single-stage** yet efficient – older two-stage TTS would first generate a mel-spectrogram or semantic tokens with one model and then invoke a completely separate model to vocode the entire utterance. CSM instead interleaves these steps frame by frame, with a tiny “vocoder” step (the decoder) that’s intimately guided by the context-aware backbone. The outcome is audio that is both high-fidelity and contextually appropriate. The use of multi-codebook modeling ensures that _natural speech characteristics_ like speaker idiosyncrasies, accents, and emotions are preserved in the output ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=2,specific%20identity%20and%20timbre)). Because the acoustic tokens explicitly encode things like voice timbre, the decoder can reproduce the unique sound of the speaker’s voice (or even mimic a style) as long as it has seen tokens from that voice in context. This plays into CSM’s ability to do _voice switching_ in multi-speaker conversations. It’s worth noting that training a multi-codebook decoder can be challenging – models must learn the right dependency order and not confuse the codebooks. Sesame’s approach of splitting at code0 is a clever simplification: it removes the need for the backbone to explicitly output prosody into a separate “spectrogram” representation, and instead uses the same Transformer to handle text and prosody. The decoder’s existence then doesn’t bottleneck the expressivity; it simply ensures the technical reconstruction of audio is accurate. In summary, the audio decoder component can be seen as **CSM’s neural vocoder**, one that is tightly integrated into the language modeling process (unlike traditional vocoders that operate independently after text processing).

**Expressive Speech Modeling and Emotion Classification:** One of Sesame’s goals was to imbue the model with **emotional intelligence** and expressive range, so that it can react to the subtleties of human speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Key%20components)). To this end, CSM incorporates an explicit emotion recognition component and training signals to handle expressive speech. According to Sesame’s technical disclosures, CSM features a **6-layer emotion classifier** network as part of its pipeline ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). This is likely a small Transformer or feed-forward model that takes as input either the audio tokens or the hidden state of the backbone corresponding to an utterance, and predicts an emotion category (e.g., neutral, happy, sad, angry, etc.). By training this classifier on the conversation data (possibly with labels inferred from an external model or certain dataset), the system gains the ability to **classify the emotion** in a speaker’s voice. This information can then be used to influence generation. One way Sesame could integrate this is by feeding the detected emotion of the user’s last utterance into the backbone (for example, as an additional token or embedding that the backbone attends to when producing its next outputs). That would allow the model to adjust its speaking style in response – for instance, speaking more softly or soothingly if the user’s emotion was detected as upset. Another possibility is multi-task training: the backbone might be trained not only to predict the next token, but also to predict the emotion of the current speaker from its hidden state, effectively learning representations that are sensitive to emotional tone. The presence of _emotion tokens_ in the Mimi codec’s semantic representation could also help – if the first codebook token encodes some emotional nuance (since Mimi compresses prosody), the backbone is inherently getting a signal of emotion from the audio context. The 6-layer classifier would refine that. In generating expressive speech, CSM benefits from its large training corpus which likely contained a wide range of natural emotions and speaking styles (e.g., excited storytelling, angry debates, empathetic responses). The model, seeing these in context, learned to mimic them. For example, in the demo, the voice can laugh, sigh, or change pacing – these are emergent capabilities from training on real conversational data and optimizing the right objectives. The **prosody** of CSM’s outputs is notably human-like (as external testers have pointed out, sometimes to an eerie degree). It injects pauses, emphases, and intonation patterns that fit the context. If a question is asked, CSM’s voice will likely end on a rising pitch. If it’s conveying sad news, it may speak in a lower, slower tone. All this is achieved without explicit rule-based controls, but through the model’s internalization of patterns. In summary, the technical components dealing with expressivity include (a) the model architecture’s ability to ingest and produce prosodic signals (through audio tokens), and (b) auxiliary components like the emotion classifier that guide the model to _understand_ and appropriately _produce_ emotional cues. This combination allows a machine learning engineer or developer using CSM to get a voice that isn’t just intelligible, but **expressively appropriate** to the conversation at hand. (In practical use, one might even select a desired emotion for the AI’s response by tweaking input prompts or using a fine-tuned version of CSM, although the base model doesn’t have a user-facing emotion control beyond what context implies.)

## 4. Training Pipeline and Data Processing

**Data Collection and Preprocessing:** Training CSM required an enormous amount of conversational speech data. Sesame assembled a custom dataset of approximately **1 million hours of audio** (predominantly English) drawn from public sources ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)). They have not named specific datasets, but likely candidates include audiobook collections, podcast archives, YouTube videos with dialogues, call center recordings made public for research, etc. The key was to get diverse, **multi-speaker conversational** audio. Each audio source was processed with automated speech recognition (to get transcripts) and **diarization** (to identify speaker turns) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)). Diarization is important because the model needs to know _who_ is speaking when, so that it can learn to assign consistent voices to each speaker ID and follow turn-taking structure. The data was also segmented into manageable chunks – rather than feeding the model unbroken hours, they cut it into dialogue segments roughly a couple of minutes long or shorter, to fit the 2048-token window. Filtering was applied to ensure quality: for instance, segments with bad ASR transcripts or extremely noisy audio might be removed ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)). The resulting training examples each contain a sequence of text and audio tokens. A single example might look like: _[Speaker0: “Hello, how are you?” + audio of that]_, _[Speaker1: “I’m doing well, thanks.” + audio]_, _[Speaker0: “Great to hear.” + audio]_, etc., all concatenated. By structuring the data this way, the model learns to predict each speaker’s next audio given the ongoing conversation. Moreover, using a million hours ensures a vast coverage of speaking styles (different ages, accents, emotions) and linguistic content (from casual chats to narrative storytelling). This massive scale – orders of magnitude larger than typical TTS datasets – is what empowers the large backbone model to generalize and produce human-like dialogue. We can infer some sources: public audiobooks (like LibriVox) could contribute thousands of hours of read speech; switching between narrator and character dialogues might simulate multi-speaker. Podcasts or talk shows provide more natural conversational cadence. The dataset likely also included **Expressive TTS datasets** (like Blizzard, VCTK, etc.) and **Emotion datasets** merged in, to boost the expressive range. All audio was presumably normalized to 24 kHz and processed through the Mimi tokenizer to convert into discrete codes for training. The text was probably normalized (lowercased, no punctuation maybe) to match ASR style transcripts. In sum, Sesame’s data pipeline created a training corpus that pairs _text+audio at a very large scale_, well-aligned for the model to learn speech generation in context.

**Training Framework and Hyperparameters:** Training a model as complex as CSM demanded careful design of the training loop and substantial compute resources. The team trained CSM end-to-end using the paired data, treating it as a **language modeling task** over an extended token vocabulary (which includes both text subword tokens and audio code tokens). Concretely, at training time the model is fed a long token sequence (2048 tokens) and it tries to predict the next token at each position. These tokens could be text or audio, so the model is learning to handle both. Whenever a text token is next in sequence (e.g., the next words of a transcript), the model’s job is effectively like that of a language model predicting text. Whenever an audio token is next (e.g., the next code0 or code1 of a response), the model’s job is to output the correct code index. The loss function is the sum of the cross-entropy losses for predicting all these next tokens. Because audio tokens far outnumber text tokens in the sequence (continuous speech has many frames, whereas text might be shorter), a large portion of the training optimizes predicting the audio tokens correctly. Training was done for **5 epochs** over the 1M-hour dataset ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)). One epoch here means the model saw each hour of audio once (though likely shuffled in short segments). Five epochs means effectively 5 million hours worth of audio-equivalent training. Even at batch sizes of e.g. one hour per GPU, this is an immense number of iterations. To handle this, the training was almost certainly distributed across many GPUs (or TPUs). The backbone being 8B parameters and the context 2048 tokens means memory per example is huge – distributed data parallelism with gradient checkpointing and possibly model parallelism (sharding layers across devices) would be needed. The training utilized mixed precision (FP16 or bfloat16) to speed up computation and fit in memory. While exact hyperparameters aren’t public, we can surmise they used an AdamW optimizer (common for transformers) with a learning rate schedule (probably warming up then decaying). The batch size (in terms of tokens) might have been chosen to maximize throughput without overflow. One aspect mentioned in the GitHub is that **the open-source release is the 1B variant** ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM)). That suggests the full 8B model might have been more of an internal or demo model, whereas they made a smaller one easier to retrain or run by the community. The 1B model training would be easier (still hundreds of thousands of hours, but can be done on fewer GPUs). For replicating the model from scratch, one should plan for _industrial-scale_ training if targeting the full size and data – i.e. multiple weeks on a large GPU cluster. It’s worth noting that unlike some speech models that first pre-train on audio alone or text alone, CSM was trained _from scratch on the joint task_. They did not first train the backbone as a text LM (and indeed they mention it does not use pre-trained LLM weights) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)), nor did they pre-train an audio-only model except for the tokenizer. This joint training is enabled by the huge data: the model effectively learns ASR (speech recognition) internally to map audio context to an internal text-like understanding, and TTS internally to map text to audio tokens – all as part of one sequence modeling objective. It’s an ambitious training setup, but the end result is a single model that “knows” speech and text modalities together.

**Compute Amortization for Efficiency:** One of the most novel training techniques used in CSM is what Sesame calls a _compute amortization scheme_, which addresses the heavy memory and compute load introduced by the multi-codebook audio decoder ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=This%20design%20introduces%20significant%20infrastructure,which%20are%20crucial%20for%20performance)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). The problem is that if the model had to calculate loss for every single audio token (including all N codebooks for every frame in a 2-minute sequence), the amount of computation would be enormous – the decoder would backpropagate through maybe tens of thousands of tokens per sample. To make this feasible, Sesame **sparsified the training of the audio decoder**. In each training batch, for each training example, they randomly select only a small fraction of audio frames for the decoder to learn on ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). Specifically, they used _1/16 of the frames_ (about 6.25%) and computed the decoder’s loss on those frames, ignoring the others ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). The backbone, however, still learns on every frame (predicting code0 for each frame in the sequence) – so the semantic modeling is always fully trained ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). This approach dramatically reduces gradient computations for the decoder. Essentially, they “amortize” the decoder training over multiple batches: any given batch updates the decoder on a subset of frames, but over many batches (on expectation after 16 batches) the decoder sees all frames. Importantly, they reported that this did **not hurt the decoder’s performance** – there was no noticeable difference in decoder loss or output quality when using this 1/16th frame sampling ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). Likely the redundancy in speech (lots of frames are similar) and the strong conditioning from the backbone makes it possible for the decoder to learn effectively even with sparse feedback. This trick is a big reason they could scale to an 8B backbone with an RVQ decoder. It mitigates the memory bottleneck: since the decoder is smaller, and now it’s also only active for a few frames per sequence on the backward pass, the peak GPU memory usage and computation time per iteration drop significantly. From an engineering perspective, implementing this means when preparing each training sample, you mark most audio frame positions as “don’t compute decoder loss”. You still run the decoder during the forward pass for continuity (or you might even skip forward passes on some frames), but you only compute gradients for the chosen subset. Over the course of training, this random sampling ensures all frames and positions influence the decoder weights eventually. Another benefit is faster iteration – more batches per second – allowing more epochs or larger batches, which can improve convergence. This technique could be seen as a form of _stochastic layer skipping_ during training, and it might inspire similar approaches in other multi-stage models. For someone replicating CSM, adopting this amortized training method would be crucial to reach the same scale without a prohibitive compute budget.

**Fine-Tuning for Contextual Memory and Quality:** After the main training, the base CSM model can generate speech given text and context. Sesame then applied fine-tuning for specific improvements and uses. They mention that a _fine-tuned variant_ of CSM was used to power their interactive voice demo ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)). This suggests that after the general training on 1M hours, they performed additional training passes on targeted data. Possible fine-tuning steps include:

- **Conversation fine-tuning:** They might fine-tune the model on actual _conversations_ (as opposed to random segments) to reinforce dialogue coherence. If the initial training data included a lot of monologue or audiobook style speech, an extra fine-tune on dialogue-heavy data (like customer service conversations or multi-turn chat transcripts) could help the model handle back-and-forth interactions even better, especially to maintain _2048-token conversation retention_ in a stable way.
- **Style/Persona fine-tuning:** The demo voices were noted to be “optimized for friendliness and expressivity” ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Building%20a%20digital%20companion%20with,the%20potential%20of%20our%20approach)). To achieve this, Sesame could fine-tune CSM on a subset of data where the assistant speaks in a friendly tone. They might have picked training examples with a certain emotional tone or even recorded custom dataset for the assistant’s persona. This would adjust model weights to prefer a particular style when appropriate.
- **Safety and appropriateness:** In an interactive setting, they’d want the model not to use offensive tones or mimic sensitive voices. Fine-tuning (or instruction via the prompt) could be used to imbue the model with a “polite filter” on how it speaks.
- **Multi-speaker consistency:** The base model can speak in many voices (since it wasn’t tuned to one). They might fine-tune separate _voice profiles_ by conditioning on a single speaker’s data to get a more consistent voice if needed for certain deployments. However, the open-sourced model **does not** come with specific voices baked in – it’s a _base model_ capable of many voices ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=Does%20this%20model%20come%20with,any%20voices)). That implies no single-voice fine-tune was done for the release (which is good for generality).
- **Emotion recognition integration:** If the emotion classifier was trained after the base model, they could fine-tune the backbone to better utilize the classifier’s output. Alternatively, train the classifier alongside the model in a multi-task fine-tune phase. This would sharpen the model’s sensitivity to emotional context.

In comparing to **Moshi’s training approach**, Moshi likely went through an analogous two-step process: first training on raw audio/text, then **multimodal instruction tuning** to make it behave as a conversational agent ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=Developing%20Moshi%20required%20significant%20contributions,a%20live%20demonstration%20on%20stage)). The big difference is Moshi used a pre-trained language model (the 7B “Helium”) as a starting point for text understanding ([Moshi: a speech-text foundation model for real-time dialogue](https://www.researchgate.net/publication/384563750_Moshi_a_speech-text_foundation_model_for_real-time_dialogue#:~:text=In%20this%20work%2C%20we%20introduce,is%20Helium%2C%20a%207B)), whereas CSM did not leverage a pre-trained text model ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20currently%20trained%20on,trained%20language%20models)). Moshi’s team reported going from scratch to a working model in 6 months with 8 researchers, suggesting they used transfer learning and heavy existing components, whereas Sesame spent more time curating data and training from ground up. Additionally, Moshi had to coordinate ASR and TTS together (since it’s full-duplex), possibly training in stages (first ASR, first TTS, then combined). CSM’s training pipeline is simpler in that it is one unified sequence task (albeit huge). In terms of scale, both likely used several hundred GPUs – the details aren’t public, but training multi-billion parameter models on ~million-hour data is at the cutting edge of what’s being done in industry research.

To summarize, training CSM from scratch involved: building a massive dual-modal dataset, designing a special architecture and tokenizer, using a clever training trick to handle the load, and possibly fine-tuning for performance and style. Each of these steps required careful engineering but they are documented at a high level in Sesame’s blog and code release, providing a roadmap for replication.

## 5. Inference and Real-Time Processing

**Low-Latency Inference Strategies:** Despite its size, CSM is optimized to deliver quick responses suitable for real-time interactions. There are several strategies and design choices that enable this. First, as discussed, the decision to use the Mimi codec at 12.5 Hz greatly reduces the number of generation steps required per second of speech. If a response is, say, 5 seconds long, the backbone will produce on the order of 5 sec \* 12.5 tokens/sec ≈ 63 semantic tokens, and the decoder will produce the accompanying acoustic tokens. 63 steps of the large model is very manageable, even if the model is large, especially since self-attention at each step is only over at most 2048 context tokens (many of which can be cached from the past) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). Indeed, the Transformer’s autoregressive nature means that at inference, one does not reprocess the entire 2048 tokens at every step – one can cache the key/value vectors from the self-attention for the past context and only compute new ones for the new token. CSM takes full advantage of this standard Transformer caching to avoid recomputation. This means the cost per step is roughly constant (and much lower than the cost of processing the whole history in one go).

Another factor in latency is the model’s **two-stage generation**. By splitting tasks, the heavy backbone runs less frequently. The backbone essentially runs one forward pass per audio frame. The decoder runs N−1 forward passes for that frame’s acoustic tokens, but it’s small. If N is, say, 8, the decoder does 7 quick passes. This is still likely faster than if the backbone had to generate all 8 tokens sequentially by itself. Moreover, the decoder’s operations could be parallelized or fused – since it’s predicting a fixed small sequence of tokens, one could configure it to output all acoustic codes in one go (for example, by unrolling those 7 steps within the decoder with a single multi-head attention that outputs a sequence of length 7). It’s not explicitly stated if Sesame did this, but the phrase “distinct linear head for each codebook” ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)) hints that they might predict all acoustic tokens in parallel (each head produces its codebook’s token) once the decoder attends to the backbone output. If that’s the case, decoder latency is nearly constant (one pass for all levels). Even if it’s sequential, it’s a small constant factor per frame. So _time-to-first-frame_ of audio is just one backbone forward + decoder forwards. This contrasts with older approaches where an autoregressive model might have to generate, say, 50 coarse tokens before any audio could be produced (the “delay pattern” problem) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=RVQ,time%20scenario)). CSM can start outputting audio after generating the very first frame’s tokens.

Additionally, Sesame likely employed standard **efficient inference practices**: using half-precision (FP16) or even 8-bit quantization for weights to speed up matrix multiplies, using optimized kernels (FlashAttention for long context, etc.), and possibly doing small batch inference to amortize overheads. They achieved an average end-to-end latency of ~380 ms for a response, which likely includes the text input already in hand ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). (It’s not entirely clear if that 380 ms is just for TTS or includes some pipeline around it. But given <500 ms is reported, we assume it’s TTS on GPU.) Running the 8B backbone on a GPU like an RTX 4090 is feasible in real-time because 8B isn’t huge by modern standards – it fits in 24 GB VRAM with room to spare for cache, especially if weights are 8-bit. The 300M decoder is trivial in comparison. The self-attention mechanism might be the bottleneck due to 2048 context length, but efficient implementations handle that with tiling.

Another trick for low latency is **incremental audio playback**. CSM can generate audio frame by frame. In a live demo, the system doesn’t necessarily wait to finish generating the entire sentence before starting to play audio. Instead, as soon as a few frames are ready (e.g., 0.5 seconds of speech), it can be sent to the audio output. Because Mimi’s codec and decoder are streaming, the user can start hearing the speech while the tail end is still being generated. This pipelining hides some of the inference time. For example, the model might take 0.3 seconds to generate 2 seconds of speech, but the user hears those 2 seconds in real-time; by the time those 2 seconds have played, the model would have generated perhaps the next chunk if needed. This way the perceived latency is just the initial wait before speech starts (maybe a few hundred milliseconds). Full-duplex systems like Moshi go even further, but even in CSM’s turn-based scenario, chunked streaming output is beneficial.

**Sub-500ms Response Time Optimizations:** Achieving under half a second latency required optimizing every part of the pipeline. On the software side, they likely used PyTorch’s GPU acceleration and possibly TensorRT or ONNX Runtime for deployment to squeeze out extra speed. On the hardware side, an NVIDIA A100 or RTX 4090 can easily handle 8B param models with FP16 at many tokens per second. The reported average ~380 ms latency suggests that in many cases the model is faster (perhaps around 200–300 ms) for shorter utterances, but they account for some longer ones too. The <500 ms figure matches the typical expectation for an interactive voice assistant (it feels instantaneous). There is mention that Moshi’s design yields ~200 ms on an NVIDIA L4 GPU (which is a datacenter GPU) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)). CSM on a 4090 at 380 ms is in a similar ballpark; the difference might be due to Moshi not needing an external LLM step and some parallelism.

One specific challenge is that CSM must handle **multi-speaker context** without exploding latency or memory. The context could include another speaker’s audio. But note: during the assistant’s response generation, the user’s last utterance (audio) is already converted to tokens and is part of the context. There’s no _active_ processing of user audio at that moment; it’s just tokens to attend to. So multi-speaker doesn’t inherently slow down generation, it just adds to the context length which is fixed at max 2048. The heavy lifting of ASR (to get user’s text) is presumed external. So CSM basically always deals with one active speaker at generation time (itself), which simplifies things – it’s not generating two voices simultaneously, except in concatenated form in context.

Memory-wise, to deploy CSM one needs enough GPU memory for the model weights and the KV cache. The backbone self-attention cache for 2048 tokens _ 32 layers (if LLaMA-32) _ 8 heads (just guessing) could be a few hundred MB. It fits in 24GB along with weights. The decoder’s cache is negligible. If memory were tighter, one could reduce context length or use a lower-precision cache (FP16 to FP8 perhaps). But the recommended **hardware is an RTX 4090 or better** for running CSM ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=Parameter%20Details%20Training%20Data%201M,Support%20RTX%204090%20or%20higher)), indicating ~24GB VRAM is needed for comfortable real-time use. This aligns with the 8B model size and large context.

**Handling Multi-Speaker Scenarios:** CSM was built to naturally handle dialogues with multiple speakers. At inference, the user can specify a sequence of `Segment` objects as context, where each segment contains a piece of text, an optional audio sample, and a speaker ID ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=speakers%20%3D%20,%5D%20audio_paths%20%3D)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)). The model will process these in order. Speaker IDs allow the model to maintain distinct voice characteristics for each participant in the conversation. If we want the assistant to have a consistent voice across turns, we use the same speaker ID (say, 0) for all its turns. The user could be speaker ID 1. During training, the diarization provided these IDs, so the model learned that tokens from speaker 0 should sound like one voice and speaker 1 like another within a single conversation. The open model wasn’t explicitly fine-tuned to any specific voice, but it can **mimic a voice given a sample**. For example, if you have a waveform of speaker 1 talking, you feed those audio tokens in the context with speaker=1. Then when the model generates a reply as speaker 1 in the future, it will tend to use the acoustic codes that match that voice timbre (because it “remembers” the recent audio tokens from that same ID). This way, CSM can carry multiple voices in one dialogue – essentially doing voice cloning on the fly for any speaker ID that it has audio for. In practice, the demo may have a preset voice for the AI (without needing external audio every time), and for the user it might treat the input audio as speaker 1’s exemplar. The generation call in the code example shows providing separate audio files for each utterance in context ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=audio_paths%20%3D%20%5B%20,)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)), which suggests the model was indeed given the actual audio of prior turns, not just the transcribed text. This is how it captures things like the user’s speaking style or the background tone, enabling it to respond appropriately (or even speak _as_ that user if needed).

When it comes to **real-time audio synthesis**, once the model has generated the audio code tokens for its response, those tokens must be converted to a waveform. The Mimi codec includes a decoder (which likely is a separate model or algorithm, possibly a small neural net) that takes the sequence of tokens and produces the audio samples. This decoding is very fast – typically neural codecs can decode faster than real time by a factor of 20-50x even on CPU. So the bottleneck is the Transformer generation, not the final waveform reconstruction. In a deployed system, one would run the CSM model on GPU to get the RVQ codes, and then run the Mimi decoder either on GPU or CPU to get actual sound. Since the model outputs 80 ms of audio at a time, it could potentially stream those 80 ms chunks through the decoder and start playing them out through speakers immediately. This streaming decoder approach ensures that the user hears the voice with minimal delay.

**Memory Efficiency and Deployment Optimizations:** For deployment, in addition to quantizing weights, one can also **optimize the context management**. For example, if the conversation exceeds 2048 tokens, one might start dropping the oldest context (in a chatbot scenario after a while). Or compress older audio context by keeping only semantic tokens from far-back turns instead of all acoustic tokens (to save space). The open-source project might not have implemented context compression, but a replicator could consider it. Another optimization is using a **smaller model for inference** if some quality can be traded for speed – the 1B CSM runs faster and could possibly achieve <100 ms latency, which might be valuable for certain applications, albeit with reduced voice naturalness.

In the interactive demo setting, the system likely works as follows: (1) User speaks, (2) ASR transcribes to text and also provides audio tokens, (3) CSM takes ASR text and prior context (including user audio tokens) as input and generates response audio tokens, (4) Mimi decoder produces speech waveform, (5) play audio. Steps 2-4 each add a bit of latency, but through concurrency (ASR on CPU, TTS on GPU, streaming, etc.) they manage a smooth experience. The target of sub-500 ms means the user perceives almost no lag.

To conclude, CSM’s inference pipeline is designed for **responsive, real-time performance**. It leverages the efficiency of discrete audio tokens, caching in Transformers, a small second-stage model, and possibly parallelization to ensure that even though it’s doing a lot under the hood (considering context and generating rich speech), it operates within the tight timing constraints of natural conversation. For a machine learning engineer, reproducing these latency numbers will involve carefully profiling the model, using GPU acceleration to the fullest, and possibly trimming model size or context where necessary. The good news is that Sesame’s results show it’s very achievable to have an 8B parameter model driving a realistic voice that responds in under half a second – a milestone that opens the door to truly interactive voice AI.

## 6. Code Implementations and Publicly Available Resources

Sesame AI Labs has made significant portions of CSM available to the public, which is a boon for anyone looking to understand or replicate the model. The core resource is the **CSM GitHub repository** (`SesameAILabs/csm`) which contains code and documentation for the model ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM)). As of March 13, 2025, they have released the **1B-parameter variant** of CSM (1B backbone + 100M decoder) along with a pre-trained checkpoint for that model on Hugging Face Hub ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=2025%2F03%2F13%20,checkpoint%20is%20hosted%20on%20HuggingFace)). The larger models (3B, 8B) were not immediately open-sourced, presumably due to their size and the sensitivity of extremely high-quality voices, but the 1B model still demonstrates the system’s capabilities and provides a reference for the architecture.

**GitHub Repository (SesameAILabs/csm):** The repository includes the model definition, training and inference code, and usage examples. The model architecture is implemented in Python (likely using PyTorch given the context) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=%2A%20%20Python%20100.0)). The code is organized to load the backbone and decoder models and run generation. For instance, there’s a `generator.py` that provides a high-level `generate` function to produce audio from text ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=from%20huggingface_hub%20import%20hf_hub_download%20from,generator%20import%20load_csm_1b%20import%20torchaudio)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=audio_paths%20%3D%20%5B%20,)). Using the repo is straightforward: after installation, you can do something like:

```python
from generator import load_csm_1b
generator = load_csm_1b("path/to/ckpt.pt", device="cuda")
audio = generator.generate(text="Hello from Sesame.", speaker=0, context=[])
```

This would output a PyTorch tensor with the waveform for the spoken sentence ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=from%20huggingface_hub%20import%20hf_hub_download%20from,generator%20import%20load_csm_1b%20import%20torchaudio)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=generator%20%3D%20load_csm_1b%28model_path%2C%20,speaker%3D0%2C%20context%3D%5B%5D%2C%20max_audio_length_ms%3D10_000%2C)). The `context` parameter can be used to pass in a list of prior segments (each segment containing text, speaker ID, and possibly raw audio or audio tokens) to ground the generation in conversation ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20sounds%20best%20when%20provided,for%20each%20speaker%20utterance)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)). The repository likely defines a `Segment` class or similar to structure that data. The example in the README shows how to load audio files for previous utterances and prepare segments with `Segment(text=..., speaker=..., audio=...)` ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=audio_paths%20%3D%20%5B%20,)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)). By studying this code, one can learn how the model expects inputs and how it formats outputs.

The GitHub also contains an **Apache 2.0 license**, meaning you can use and modify the code freely in your own projects ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=License)). The README provides important information such as the fact that the model is _not_ a general-purpose language model and cannot generate arbitrary text (so users shouldn’t try to have CSM invent new dialogue – it’s purely for speech generation from given text) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). It also addresses language support (mostly English) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=Does%20it%20support%20other%20languages%3F)) and some ethical guidelines for using the model responsibly ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=This%20project%20provides%20a%20high,we%20explicitly%20prohibit%20the%20following)).

For replication, the GitHub repository includes useful components beyond just the model code. It has (as per the ComfyUI Wiki summary) a **“complete architecture whitepaper”** and possibly design docs ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)) – this might be the blog post or an extended PDF describing CSM’s internals. It also references **REST API examples** ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)), which could help integrate CSM into applications (like a server that takes text and returns audio). An **audio preprocessing toolkit** is mentioned ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)), which likely contains scripts to tokenize audio with Mimi (so if you have your own audio, you can produce the RVQ codes) and perhaps to decode tokens back to audio. This is extremely useful if one wants to train or fine-tune the model on new data – you’d need to encode that data into tokens first using the same tokenizer. The **model quantization guide** implies they provide instructions or tools to convert the model to lower precision for faster inference or smaller memory footprint ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)). All these resources lower the barrier to entry for experimentation.

**Hugging Face Models and Demos:** Sesame has uploaded the 1B model checkpoint to Hugging Face (`sesame/csm_1b`) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=2025%2F03%2F13%20,checkpoint%20is%20hosted%20on%20HuggingFace)). While the weights are gated (requiring acceptance of terms) ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm_1b#:~:text=You%20need%20to%20agree%20to,information%20to%20access%20this%20model)), it makes it convenient to download and use via the `huggingface_hub` library as shown in their example code ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=from%20huggingface_hub%20import%20hf_hub_download%20from,generator%20import%20load_csm_1b%20import%20torchaudio)). The model card on Hugging Face reiterates the architecture: a LLaMA-based backbone with a smaller decoder producing Mimi audio codes ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). It also links back to Sesame’s site for the blog post and to the GitHub repo for code ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=smaller%20audio%20decoder%20that%20produces,Mimi%20audio%20codes)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)). Hugging Face also hosts a **CSM demo Space** (likely a Gradio or Streamlit app) where users can input text and get audio out, or even simulate a conversation by inputting multi-turn dialogues ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=smaller%20audio%20decoder%20that%20produces,Mimi%20audio%20codes)). This is a quick way to test the model’s outputs without setting up code locally ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)).

Additionally, the **Mimi codec** itself is available as an open model: Kyutai Labs has a repository on Hugging Face for Mimi (`kyutai/mimi`) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=kyutai)) with a model card describing its function and even instructions to use it with Transformers or with Moshi ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Usage%20with%20)). The Mimi model card provides technical details like the frame rate (12Hz) and bitrate (1.1 kbps) and notes that Mimi’s first codebook is aligned with WavLM features ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). It also links to a **GitHub repo for Mimi** (which might be part of the Moshi repository) and a paper reference ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Model%20Sources)). If one wanted to replicate CSM’s tokenizer, they could use the Mimi code and pre-trained model directly instead of training a new codec from scratch – this saves a lot of effort and ensures compatibility (Sesame chose Mimi presumably because it was excellent and open). The Mimi HF model can encode audio to tokens and decode tokens to audio. This means if you generate tokens with CSM, you can synthesize them to sound using Mimi’s decoder (the CSM code likely wraps this process so you might not need to call Mimi directly).

**Other Resources and Discussions:** The research community has been actively discussing CSM and similar models. There are references to CSM in articles and forums. For example, Ars Technica wrote about the demo’s realism and the concept of “voice presence,” quoting Sesame’s goals ([Eerily realistic AI voice demo sparks amazement and discomfort online](https://arstechnica.com/ai/2025/03/users-report-emotional-bonds-with-startlingly-realistic-ai-voice-demo/#:~:text=Eerily%20realistic%20AI%20voice%20demo,)). This and other media (e.g., YouTube demos, the Product Hunt listing ([ Sesame - Conversational speech model that achieves voice presence | Product Hunt](https://www.producthunt.com/posts/sesame-4#:~:text=Upvote%20181))) provide qualitative insight and third-party impressions which can be valuable in understanding how humans perceive the model. Kyutai Labs’ Moshi project provides a point of comparison; their GitHub (`kyutai-labs/moshi`) is open-source and includes not only code but also a preprint on arXiv detailing Moshi’s architecture ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=,Face)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). By reading Moshi’s code (which is in the same domain of speech-text modeling), an engineer can glean ideas about implementing streaming ASR, integrating a text model, etc., which could be complementary to what CSM does. Moshi’s code might also contain a PyTorch implementation of Mimi (since it uses Mimi as well) which could double-check how to do tokenization properly.

In terms of replication, these resources mean one does not have to start from zero. You can:

- Use the open-source **CSM 1B model** to experiment and familiarize yourself with the model’s behavior. This gives a baseline for voice quality and helps validate your environment (tokenization, model forward pass, etc.).
- Leverage the **code** to see how the model class is built. The architecture details like number of layers, hidden size, attention heads, etc., might be explicitly defined in the code or config. (If not in the readme, one might have to dive into the Python code in the repo to find, for example, a module that creates a `LlamaModel` or similar for the backbone, and a `DecoderModel` for the audio part.)
- Use the **preprocessing scripts** to prepare any custom data. For instance, if you want to feed a new audio file as context, you can convert it to tokens with Mimi so that it’s in the exact form the model was trained on.
- Follow the **evaluation scripts or metrics** if provided, to test output quality against references if you do any fine-tuning. The mention of homograph tests and CMOS in the blog indicates they have evaluation code internally – it might or might not be in the repo, but at least they described the procedures.

One thing to note: at the time of writing, the **full training code** for CSM had not been released, though the wiki suggests it may come later in 2025 ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=%E2%9A%A0%EF%B8%8F%20Limitations%3A)). This means that while we have the model architecture and an inference code path, setting up the exact same training (with distributed data loading, etc.) might require some work and guesswork. The open code likely allows fine-tuning on smaller data out of the box (since they expect researchers might try adapting the model). If one is replicating from scratch, they could reimplement training in PyTorch fairly readily using the model definitions as a guide. The biggest missing piece from code might be the _training schedule/hyperparams_, but those can be tuned or inferred from similar projects (e.g., LLaMA training recipes).

In addition to Sesame’s and Kyutai’s resources, it’s wise to look at related research like **AudioLM, VALL-E, and SpeechLM**. Some of those projects have released code or models that, while not identical in purpose, share components (e.g., AudioLM’s use of SoundStream tokens, VALL-E’s approach to zero-shot voice cloning with codebooks). OpenAI’s **Whisper** ASR and Microsoft’s **VALL-E X** (if code available) could provide further context for voice cloning and multilingual aspects. However, given that Sesame’s CSM is open and quite bleeding-edge, it likely represents the current state-of-the-art one can directly study.

To summarize, the publicly available resources for CSM include: the open-source **code repository**, a ready-to-use **1B model checkpoint**, the underlying **Mimi codec code/model**, a detailed **technical blog post**, and various **evaluation results and discussions** shared by the team. Together these give a comprehensive picture that a machine learning engineer can use to fully understand CSM’s design. By leveraging the code and models, one can experiment hands-on, which is invaluable for learning or attempting a reimplementation of the model from scratch.

## 7. Comparison to Moshi by Kyutai Labs

CSM and Moshi are both cutting-edge conversational speech generation models, but they were developed by different teams with somewhat different goals, leading to differences in architecture and capabilities. Here we’ll compare them in several aspects:

**Architectural Design:** Both CSM and Moshi utilize dual transformer architectures, but the way they partition tasks is different. CSM splits by _function_ (context modeling vs. acoustic rendering) – its large backbone vs. small decoder ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). Moshi splits by _time scale_ – it has a Temporal (or “global”) transformer that handles the sequence over time, and a Depth (or “local”) transformer that handles the parallel prediction of codebooks at each time step ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). Another way to view it: CSM’s backbone is analogous to Moshi’s Temporal transformer (both model long sequences of a “primary” token stream: code0 for CSM, combined audio-text for Moshi), and CSM’s decoder is analogous to Moshi’s Depth transformer (both model dependencies among codebooks per frame). However, Moshi’s architecture is a bit more complex because it is a _speech-text_ model: it doesn’t just predict audio tokens, it also predicts text tokens for its own output concurrently ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=Moshi%20models%20two%20streams%20of,to%20its%20own%20speech%2C%20its)). Moshi essentially learned to _speak and listen in the same model_, integrating an ASR component and a TTS component. It uses a form of next-token prediction where the next token could be from either the user’s audio stream or the AI’s own audio stream or the AI’s own transcript ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=Moshi%20models%20two%20streams%20of,to%20its%20own%20speech%2C%20its)). CSM, by contrast, is only a TTS model (multimodal in that it takes audio context, but it doesn’t produce transcripts or take audio input on the fly) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). This means Moshi’s training and architecture had to account for two simultaneous audio streams (full-duplex) and a text stream, making it a _three-stream model (user audio, AI audio, AI text)_, whereas CSM is a _two-stream model (past audio, upcoming audio, plus text as annotations)_.

**Full-duplex vs. Turn-based:** A key practical difference is that Moshi is designed for **full-duplex dialogue** – it can handle speaking and listening at the same time without explicit turn-taking ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=More%20fundamentally%2C%20Moshi%20is%20an,integrated%20multimodal%20modeling%20of%20Moshi)). In Moshi’s demo, the AI can interject “mm-hm” while the user is still speaking or start formulating an answer before the user fully stops. This is achieved by Moshi’s architecture continuously processing input audio and generating output audio tokens with minimal delay (only 80 ms frame delay) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). It doesn’t wait for an “end of utterance”; it treats the conversation like two audio streams flowing concurrently. CSM, on the other hand, currently works in a **turn-based** manner. It assumes one speaker talks, then stops, then the other responds. It needs the text (and ideally the audio) of a user utterance as input context, then it produces the response. If the user were to interrupt while CSM is speaking, CSM has no mechanism to incorporate that mid-response – it would likely continue until completion (unless an external system stops it). Therefore, Moshi is more suitable for _natural, overlapping conversation_, whereas CSM is more aligned with the typical voice assistant interaction pattern (speak -> wait -> respond). Achieving full-duplex with CSM would require integrating an ASR to listen during output and perhaps cutting off generation if interruption is detected, but that would be a higher-level system solution, not something CSM inherently does.

**Integration of Language Modeling:** Moshi is, at its heart, a **speech-dialogue foundation model** – it not only generates speech, but also decides _what_ to say (like ChatGPT with a voice). In fact, the first component of Moshi (“Helium”) is a fine-tuned LLM that was trained to generate conversational content, and it outputs both text and some representation that guides the speech output ([Moshi: a speech-text foundation model for real-time dialogue](https://www.researchgate.net/publication/384563750_Moshi_a_speech-text_foundation_model_for_real-time_dialogue#:~:text=In%20this%20work%2C%20we%20introduce,is%20Helium%2C%20a%207B)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)). Moshi was instruction-tuned on dialogue data to follow prompts and produce appropriate answers, similar to how ChatGPT was tuned, but with audio in the loop ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=Developing%20Moshi%20required%20significant%20contributions,a%20live%20demonstration%20on%20stage)). CSM deliberately does _not_ include a language generation component – it requires an external text input for the content of speech ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). Sesame’s design philosophy was to separate concerns: use the best text-based LLM for language understanding and use CSM for rendering speech from that text. This has pros and cons. With CSM+LLM, you can upgrade the text brain independently (e.g., use GPT-4 or a specialized chatbot and then have CSM voice it). With Moshi, the text and speech are entwined; you have one model that does both, which is elegant but potentially less flexible (if the language part of Moshi is weaker than the latest ChatGPT, it’s not trivial to swap it out). However, Moshi’s integrated approach could yield more coherence between wording and prosody – since it knows exactly what it’s saying as it says it, it might place emphasis more appropriately. For instance, Moshi’s internal text prediction allows it to ensure that its spoken output matches the intended text exactly (because it literally generates the text as a token sequence too, reducing risk of say, mispronouncing a word or skipping a word) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). CSM doesn’t generate text tokens for its output, so it must rely on the provided text – if the text has a word it’s never heard, it might struggle (though presumably it can spell it out via phonemes if needed).

**Model Size and Efficiency:** Moshi’s main Transformer is about 7 billion parameters (plus whatever the Depth transformer adds, which is relatively small, maybe a few hundred million). CSM’s largest is 8B + 0.3B decoder. So in raw size they’re comparable. However, Moshi’s architecture might demand more compute at inference because it’s essentially performing ASR and TTS in one model – although the ASR part is mostly just ingesting audio tokens, which, thanks to Mimi, is also a 12.5 Hz stream, so it’s not that heavy. Moshi’s advantage is it does fewer autoregressive steps because of parallel codebook prediction and because it doesn’t wait for an end-of-utterance. In terms of **latency**, Moshi achieves a remarkable ~200 ms end-to-end latency (including the 80 ms frame delay) in a streaming scenario ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)). CSM, while very fast for a TTS system, is used in a call-response scenario where you likely have an additional ASR delay (maybe 200 ms) plus the CSM generation (~380 ms) – so the total user-perceived latency might be ~0.5-1.0 s if you include ASR. In a fair measure just of TTS, CSM might be ~0.3-0.4 s vs Moshi’s ~0.2 s, but Moshi doesn’t have the ASR step because it’s built-in. For practical purposes, both are low enough to feel interactive, but Moshi edges out if you need immediate back-and-forth.

**Voice Naturalness and Quality:** Both models produce highly natural voices, nearly human-like in many cases. Subjective evaluations (like CMOS scores) for CSM showed that without context, people could hardly tell it apart from real speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=The%20graph%20above%20shows%20the,However%2C%20when%20context%20is%20included)). Moshi’s public demos also astonished listeners with how human and spontaneous it sounded (to the point of some being unsettled). CSM places emphasis on using context to produce _appropriate_ prosody – e.g., its evaluations showed that when given conversational context, human evaluators preferred real human speech slightly over CSM, meaning CSM is extremely close but perhaps not perfect yet in contextual expressiveness ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=The%20graph%20above%20shows%20the,However%2C%20when%20context%20is%20included)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20currently%20trained%20on,trained%20language%20models)). Moshi, with its ability to “um” and interrupt, demonstrates a very high degree of conversational realism. One difference is that CSM’s output is deterministically tied to the input text. If the input text is poorly phrased, CSM will still read it in a fluent way, but it won’t _rewrite_ or improvise. Moshi can actually choose wording on the fly, leading to very fluid dialogues. This can make Moshi seem more _alive_ as a personality, whereas CSM is more like a highly advanced _reader/actor_. In terms of **speaker similarity** or cloning, both can impersonate voices given a sample. Moshi presumably can do zero-shot voice based on a short prompt (similar to how Vall-E does), and CSM can do it if provided a snippet in context. The underlying tokenization being the same (Mimi) means if both see the same voice token sequence, they should both be able to render that voice. If anything, CSM might do better at maintaining a specific voice across a long conversation because it was explicitly trained with speaker IDs over long contexts (ensuring consistency), whereas Moshi, by not using explicit turn tokens, has to implicitly keep track of which voice to use (but it obviously was designed to do so across its two streams).

**Context Handling:** CSM can handle 2048 tokens of context (roughly 2 minutes, could be many turns) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)). Moshi, being streaming, effectively can handle indefinite context – it doesn’t have a set context length in the same sense, because it processes input continuously. However, Moshi’s internal architecture might still have some window for attention for its temporal model. If it’s 7B, it might be similar to a GPT-3 sized model which often have context lengths 1K or 2K tokens. But since it’s processing at 12.5 Hz, a 2K token context in Moshi would correspond to 160 seconds of audio (if counting one token per frame per stream perhaps), which is actually longer than CSM’s 2 minutes. It’s unclear if Moshi uses relative positioning to allow infinite context or if it also has some limit. Moshi also uses the technique of predicting its own transcript, which provides a textual summary of the conversation as part of the context (the “inner monologue”). This likely helps it maintain context in long dialogues by relying on text memory (which is compact) rather than pure audio memory. CSM relies on having both text and audio tokens fed in for context – text gives it the words, audio gives it tone, as we discussed. Both approaches have merit. For replication purposes, Moshi’s approach is more complex to implement (because of multi-task training for ASR+TTS), whereas CSM’s approach is straightforward if you have the transcripts and audio tokens.

**Tokenization and Audio Compression:** Both CSM and Moshi use the **Mimi tokenizer**. This means they share the same fundamental audio representation. There might be minor differences: perhaps the exact number of codebooks or codebook sizes. If one of them tweaked Mimi for their use-case, that could mean slight differences in quality or compatibility. The Moshi paper mentions Mimi has an adversarial training loss and matches or beats other codecs at similar bitrates ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). If Sesame used Mimi as-is, CSM benefits from that as well. If Sesame developed “Mimi 2” or modified it, they didn’t say so explicitly – it sounds like they directly used Mimi (they even call it Mimi in the blog) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). Therefore, on the tokenization front, the approaches are identical. Both output one semantic token and multiple acoustic tokens per frame. Where they differ is how those tokens are generated: CSM’s decoder vs. Moshi’s depth transformer. Moshi’s depth transformer might generate all tokens at once for a frame (non-autoregressively, using a training scheme akin to parallel decoding). It’s mentioned that Moshi’s approach reduces the number of autoregressive steps thanks to Mimi’s low frame rate ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=dialogue%20framework,3kbps)), implying that for each frame it doesn’t do a lot of sequential work. It could be using something like a mask-and-predict for codebooks or a learned order. CSM’s approach is partly sequential (the codebooks are generated in order, albeit quickly). In theory, Moshi’s method could be slightly faster in generation within each frame, but both are so fast at that level it’s negligible compared to the big transformer passes.

**Model Efficiency and Footprint:** Moshi, by being end-to-end, requires one model to do everything. This means if one wanted to deploy Moshi on a device, that device must handle the 7B model. CSM could be split: ASR on device (maybe a small Whisper model), LLM in the cloud, CSM TTS in the cloud, etc. For strictly TTS usage, CSM’s 1B model is already out and much smaller to run. Moshi has not released weights yet (as far as known); only the demo is available. So from an open-source standpoint, CSM currently is more accessible. Efficiency also includes training efficiency: CSM’s training used the amortization trick to reduce memory, whereas Moshi’s training complexity came from multitask learning. It’s not obvious which was more efficient – both likely pushed GPUs to their limits. But since CSM’s team could train an 8B model on 1M hours, and Moshi’s team did a 7B model on (likely less data, perhaps tens of thousands of hours?), CSM probably consumed more raw compute to train (Google-sized effort), whereas Moshi was a startup effort with limited time.

**Differences in Naturalness and Context Handling:** In terms of _natural interactive behaviors_, Moshi can do things like incremental speech, backchanneling (“uh-huh”, “right…”) and quick turn-taking. CSM can produce very natural _speaking style_ but doesn’t inherently decide to inject backchannels unless the text explicitly says “uh-huh” (since it doesn’t generate discourse behaviors by itself). So Moshi feels more like a live conversational partner that can surprise you, while CSM is an extremely high-quality voice for an assistant that speaks when it’s supposed to. Another subtle point: Moshi’s integrated text+speech modeling might help with _pronunciation_. If Moshi encounters a word it’s not sure how to say, it might derive it from spelling via its language model knowledge. CSM relies on the training data and possibly the semantic token to get pronunciation right. Both claim to handle tricky cases (CSM tested homograph disambiguation using context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20better%20assess%20pronunciation%20and,based%20benchmarks)), which is typically a language understanding problem – CSM solved it by using textual context to choose correct pronunciation). So in that test, CSM is using context in a similar way as an integrated model would, except it’s leveraging that the text context disambiguated the word meaning and thus it knew which phoneme code to produce.

**Summary of Approach Differences:** In short, **CSM = LLM architecture + audio tokens; Moshi = joint LLM + speech architecture**. CSM chooses modularity (separate text generation, focus on speech rendering), while Moshi chooses unity (one model to rule them all). For replicating, CSM’s approach might be easier to start with, and indeed we have full resources for it, whereas Moshi’s approach might be more ambitious but potentially the future direction (since eliminating the boundary between deciding and speaking can yield more fluent interaction). Both achieve impressive efficiency with Mimi codec, but use slightly different tricks to overcome the multi-codebook challenge (CSM with a sequential decoder but skipping frames in training, Moshi with a parallel codebook prediction using a second transformer).

**Moshi vs CSM in Efficiency & Quality:** The ComfyUI Wiki’s summary suggests CSM achieved a 68% improvement in first-frame latency over prior methods ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=Official%20benchmarks%20show%3A)), highlighting how single-stage (CSM’s method) improves over two-stage pipelines. Moshi’s stats aren’t directly compared, but we know Moshi’s latency is extremely low by design. On quality benchmarks, if we had both side by side, they’d likely be close. Each might have slight edge in certain areas: e.g., CSM perhaps better at long monologues since it trained on 2-minute samples, Moshi better at rapid dialogue. On context, Moshi inherently uses whatever it heard to inform reply, while CSM can be paired with a strong text model to use context (the text model would handle remembering facts etc., whereas Moshi must rely on its internal weights for world knowledge).

In conclusion, while CSM and Moshi share the vision of natural conversational AI voices and even share technical building blocks (Mimi codec, Transformer modeling), they diverge in integration. CSM is **modular and focused on speech synthesis** with contextual awareness, whereas Moshi is **integrated and focused on full dialogue** (speech in and out). An engineer learning from both could consider a hybrid: e.g., using CSM as the TTS component in a Moshi-like full-duplex system, or adding a lightweight text generation head to CSM to give it some dialogue autonomy. The comparison highlights that CSM’s approach is somewhat more conservative (leveraging known LLM externally) and Moshi’s is more end-to-end (which is bold and impressive). Depending on the application (closed-loop assistant vs. open-ended chatbot), one or the other might be preferable. For replicating CSM, one can focus on the speech part and interface it with existing NLP systems, whereas replicating Moshi requires tackling ASR, NLP, and TTS all together.

## 8. Final Considerations for Recreating CSM

Recreating CSM from scratch is a complex but achievable project, given the information and resources now available. Here we outline a step-by-step roadmap and important considerations for an engineer attempting this, as well as the compute requirements and potential challenges.

**Step-by-Step Development Plan:**

1. **Obtain/Prepare a Large Conversational Speech Dataset:** As a foundation, gather a substantial amount of audio data with transcriptions. Aim for diverse, multi-speaker conversation data. Public datasets can be combined: for example, LibriSpeech/LibriLight for sheer hours of speech, Switchboard and Fisher for telephone conversations, Common Voice or VoxPopuli for crowd-sourced speech, podcast datasets for natural dialogue, etc. You’ll need transcripts and speaker labels. If not readily available, use an ASR like Whisper to transcribe audio and a diarization tool (like Pyannote or SpeakerNet) to label speaker turns. The goal is to produce a training corpus of token sequences where text and audio tokens alternate by speaker. If replicating at smaller scale, you might start with say 1–10k hours instead of 1M, but more data will noticeably improve naturalness and reduce overfitting. Ensure audio is standardized (e.g., 16 or 24 kHz mono) and segmented into chunks (e.g., 20–120 seconds segments) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)).

2. **Train or Use an Audio Tokenizer (RVQ-VAE):** CSM relies on the Mimi tokenizer, which you can either reuse or reimplement. The easiest path is to use the **pre-trained Mimi codec** from Hugging Face ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)). It provides an encoder that converts wav audio into a series of discrete codes and a decoder that does the reverse. If you want to train your own tokenizer, you’d train an autoencoder on audio: encoder produces a sequence of latent vectors (one per 80 ms frame), then have multiple quantization layers (codebooks) that progressively quantize the latent with residuals, and a decoder reconstructing the waveform. You’d use an adversarial + reconstruction loss as in SoundStream ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). This is a significant endeavor itself. Given Mimi exists, you can save time by adopting it – treat the Mimi encoder as a black box that gives you code0..N-1 for each frame. You might still want to verify its performance on your data (Mimi was trained on speech, so it should generalize to most voices). Using Mimi, encode all your training audio into token sequences and store them (preferably in a compressed form or on-the-fly compute). These tokens will form the “audio portion” of your training samples ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)).

3. **Define the Model Architecture:** Implement the dual-transformer model. You can use an existing Transformer framework (like HuggingFace Transformers) to create a GPT-like model, but you will need to extend it to handle two types of inputs and two transformers. Specifically:

   - Create the **Backbone** model as a decoder-only Transformer with a causal self-attention mask. The backbone’s vocabulary is the union of text tokens and audio tokens (code0 specifically). One approach: assign a range of token IDs for text (e.g., using a standard tokenizer like SentencePiece for text) and another range for audio code0 tokens. You might treat code0 tokens as additional vocabulary symbols. Alternatively, you can treat them as a separate embedding matrix and combine via a modality embedding. But simpler is one hot vocabulary with size = text vocab size + size_of_code0_vocab. The code0 vocab size is the number of entries in the semantic codebook (Mimi’s first codebook, which might be 1024 or similar). The backbone will be trained to predict the next token which could be either a text subword or a code0.
   - The backbone needs to be able to attend across a long history (so implement positional embeddings for up to 2048 or more positions). Follow LLaMA’s hyperparameters for layer count, hidden size, heads, etc., if aiming to replicate Sesame’s quality. For example, an 8B LLaMA-like model might have 32 layers, 4096-dim feedforward layers, and 32 attention heads. The open 1B model might have around 16 layers, 2048-dim FFN, etc.
   - Create the **Audio Decoder** model. This can be a smaller Transformer. One design: treat each codebook prediction as a “time step” in this decoder. For instance, if there are 8 codebooks (0-7), the decoder will produce codebooks 1 through 7. Code0 is given by backbone. So you can feed backbone’s output embedding as a start token to the decoder, then have the decoder run for 7 steps (each step predicts one deeper code). Use a causal mask so each step can only attend to previous ones (ensuring code2 sees code1, etc.). Or, to use the distinct head approach, you could simplify this: have the decoder attend to backbone output and itself (maybe 1 self-attn layer is enough since there’s a fixed small sequence) and then have multiple output heads that directly predict each codebook level in parallel ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). During training, you’d provide the ground-truth code1..7 to compute loss on each head. During inference, you input the backbone output, and have the decoder output all required codes. Implement whichever approach is simpler for you – sequential decoding of 7 tokens is fine (7 isn’t much). The decoder should be significantly smaller: e.g., Sesame’s 300M decoder might be 6 layers, 1024 hidden size, 16 heads, something in that ballpark. It also needs an embedding for each codebook level it’s predicting (to know which level it’s doing if sequential).
   - Integrate the two: The backbone will output something (could be a probability distribution for code0). In training, you have the true code0 as next token, so the backbone loss is straightforward cross-entropy on code0 prediction. Then, conditioned on the _actual_ code0 (teacher forcing), you run the decoder to predict code1..N-1 for that frame, and compute decoder loss. In inference, you would sample backbone’s code0 (or take argmax) then feed it to decoder to sample the rest. Managing these pieces requires custom code beyond a standard language model training loop, but it’s manageable with PyTorch: you’ll write a forward function that does backbone(x) and decoder(backbone_output).
   - Don’t forget to prepend special tokens or embeddings for speaker IDs in the input. For example, you can reserve an ID in the text vocabulary for <speaker0>, <speaker1>, etc., and insert them accordingly in the text sequence. CSM used this to condition voices ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). You may decide how many distinct speaker embeddings you allow (Sesame might have used a generic scheme or a fixed small set and reused them, since they had arbitrary many speakers in data). You could also encode the speaker info in the audio tokens themselves (less direct). The simplest: add a token like “[S0]” before each speaker’s utterance text in the training data. The model will learn association of [S0] with certain audio that follows.

4. **Training Setup:** Set up a training loop that can feed the data to the model. Each training example is a sequence of tokens representing an exchange. Build batches of these (they should all be padded to the same length in the batch). Use a masking to ensure loss is only computed on real tokens, not padding. Use an optimizer like AdamW with a learning rate schedule. You will likely need distributed training (DDP) if using GPUs – at least if training the full 8B. Start with a smaller model first (for example, a 100M backbone + 20M decoder on maybe 1000 hours of data) to verify the pipeline works and the model can overfit a small sample, etc. Monitor training losses for both backbone and decoder. The backbone should learn to predict text and code0 tokens; the decoder learns code1+ tokens. Implement the **compute amortization**: when computing decoder loss, randomly choose a subset of frames in each sequence to apply decoder loss. For example, out of all frame positions in the sequence, sample ~6% and only compute loss for decoder’s outputs at those frames ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). For other frames, you can ignore decoder loss (and optionally even skip running the decoder to save compute, or run it but detach it to not backprop – whichever is easier). Ensure code0 loss is still computed at every frame to train backbone fully. This will require careful indexing, but it’s doable: you know which positions correspond to code0 tokens (every time a new frame starts). You can tag those, and then randomly mask out most of them for decoder training each step.  
   As for hyperparameters: likely a few hundred thousand steps with a batch size such that each step covers perhaps a couple of minutes of audio per GPU. Sesame did 5 epochs on 1M hours ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)), which is effectively 5e6 hours processed. If you have less data, you might do more epochs. Use gradient accumulation or micro-batching if needed to simulate a large batch size (to stabilize training for these large models, a total batch on the order of 0.5 to 2 hours of audio worth of tokens might be good). Keep an eye on validation – you can set aside some hours of dialogue as a dev set to see if the model isn’t overfitting and if the losses for text vs audio tokens behave well.

5. **Scaling Up:** Once the training pipeline works on smaller scale, scale the model to larger sizes gradually and the dataset accordingly. This will require multiple GPUs – consider using model parallelism or sequence parallelism if memory is an issue. The compute amortization will help memory, but the 8B backbone still will be heavy. You might utilize mixed precision and gradient checkpointing on the Transformer layers to manage memory. Training could be done on cloud TPUs as well (since the model is similar to a large LM, JAX/Flax or DeepSpeed frameworks might help). This stage is the most resource-intensive: an 8B model on say 100k hours might take several weeks on 16 GPUs. To reach 1M hours you likely need tens of GPUs for multiple weeks (Sesame likely used a specialized hardware setup or a lot of compute). Ensure to shuffle data well (so each epoch is random order) and periodically evaluate the model’s outputs for intelligibility and quality.

6. **Fine-Tuning and Specialty Training:** After base training, you may fine-tune the model on specific sub-tasks:

   - Fine-tune on **emotion-rich data**: If you have an emotion-labeled dataset (like CREMA-D, RAVDESS, or some internal set), you can fine-tune (or continue training) the model with an auxiliary loss to classify emotion from the context or from its own generated tokens. For example, add a small classifier head on the backbone’s final layer to predict an emotion label for the current utterance, and train with a multitask loss (LM loss + emotion classification loss). This mimics what Sesame’s 6-layer emotion classifier might do ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). This will encourage the backbone to encode emotional state in its hidden vectors. You can then condition generation subtly by emotion (or just let it be implicit).
   - Fine-tune for **voice consistency**: If your base model’s voices are too variable, you might fine-tune on a subset where one speaker (the assistant) uses a consistent voice (this could be done by narrowing down speaker embedding or injecting a target voice sample always for speaker 0). This will reduce variation and give you a stable persona voice for the AI.
   - Fine-tune to reduce **verbal filler**: Depending on data, model might generate some “um” or stutters if it heard them. If unwanted, filter those from training data or fine-tune on cleaner speech.
   - If you integrate with an LLM, you might fine-tune the _text style_ the model expects. For instance, if your LLM outputs very formal text but you want a conversational tone, you could have the LLM produce a hidden tag like “[casual]” and include that in text for CSM, and fine-tune CSM to alter its tone accordingly.

7. **Inference Pipeline Development:** With the model trained (or while training smaller versions), develop the inference code. This includes implementing the generation loop: feeding context tokens, using cached attention states for efficiency, and generating until an end-of-utterance token is produced. You should test that given some prompt text and some context, the model produces reasonable speech tokens. Then incorporate the Mimi decoder to turn those tokens into audio. Verify the audio quality. Often there might be a need for some **post-processing**: for example, inserting a short pause between sentences (the model might output tokens that correspond to silence if needed, or you can manually splice small silence if desired). Also ensure that the sampling strategy is right: CSM likely does _greedy or deterministic decoding_ for most cases because it’s aiming to render the given text accurately (not much stochastic freedom except maybe in how to express something). They might sample code0 from a small distribution to add variation in tone, but likely they pick the most probable tokens to match the transcript exactly. Experiment with this – if you sample too much, you might get prosody or even pronunciation drifting. Greedy might be safest to replicate their results of intelligibility. The decoder can be run in streaming mode – generate a few tokens, decode to audio frames, output – to reduce user perceived latency. Ensure the system can handle multiple speakers: test that if you provide a user utterance with one speaker ID and prompt a response with another, it indeed speaks differently. The GitHub example with `Segment` objects can guide how to do this in code ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=speakers%20%3D%20,%5D%20audio_paths%20%3D)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)).

8. **Hardware and Optimization:** When deploying the model, consider using quantization (8-bit or 4-bit) to shrink model size with minimal quality drop. Projects like GPTQ or Nvidia’s quantization can be used on the backbone. The decoder is small anyway, but quantizing it too won’t hurt. For 8B model, 8-bit quantization can cut memory roughly in half, allowing possibly a single GPU deployment. Use high-performance inference libraries if available; e.g., ONNX Runtime or FasterTransformer for the backbone (these can handle GPT models efficiently). Since CSM must often run alongside an ASR and an NLP model, optimizing each is important. A 4090 or A6000 GPU can run CSM 8B easily under 500 ms per sentence as shown ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)); for scaling to many users, you’d use multiple GPUs or a server cluster, or drop to the 1B model for faster, albeit slightly lower-quality, results.

9. **Testing and Evaluation:** After you have a working system, evaluate it thoroughly. Use objective metrics: WER (of re-transcribed output) to see if it pronounces words correctly, MOS (mean opinion score) or CMOS via listening tests to gauge naturalness, and the contextual tests like those Sesame did (homograph disambiguation ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20better%20assess%20pronunciation%20and,based%20benchmarks)), pronunciation consistency ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=model%20correctly%20pronounced%20different%20words,of%20the%20speaker%20and%20context))). Also test **dialogue coherence**: have multi-turn exchanges and see if the voice remains consistent and appropriate. Since you likely won’t reach the scale of Sesame’s training immediately, expect some gaps: perhaps the model might still have minor audio glitches or less stability in very long contexts. Iteratively refine by adjusting training (more data, or fine-tuning on problematic cases).

10. **Iterative Improvement:** Identify any shortcomings. For example, maybe your model struggles with a certain accent or with very fast speech in context. You could then incorporate more data of that type or adjust the model (maybe the acoustic model needs more capacity for those cases). Another area: if the model sometimes produces odd intonation for questions, you might fine-tune with more clear question examples. This process might be needed to approach the polish of CSM. Additionally, consider implementing the **evaluation suite** Sesame mentioned to quantify improvements. Over time, with enough data and tuning, your replica should approach human-level naturalness and strong contextual adaptivity, just like CSM.

**Compute and Hardware Requirements:** Building CSM from scratch is computationally intensive. To train an 8B parameter transformer on 1M hours (which is ~\*30 billion\* tokens if we roughly estimate tokens at 0.5 sec each) is a project that likely requires a supercomputer-grade setup. In practical terms, one would use parallel GPUs. For example, if using NVIDIA A100 40GB GPUs, one might use 32 of them and train for several weeks, using gradient accumulation to handle large sequences. If reducing scope, a 1B model on perhaps 100k hours could be trained on 8 GPUs in a few weeks. It’s advisable to start with something manageable like that to prove it out. A realistic hardware minimum for a serious attempt might be: 8 GPUs (each with 24+ GB VRAM), 100 terabytes of disk for storing data and intermediate tokens (if needed), and a lot of patience. Using cloud TPU v4 pods or multi-node GPU clusters with good interconnect can significantly speed things up. Inference, on the other hand, is much lighter: the final 8B model can run on a single high-end GPU in real-time, or on two for added ease. If you plan to serve many simultaneous users, you’d deploy multiple instances or use a model with fewer parameters or quantized weights.

**Potential Challenges:**

- _Data Noise and Quality:_ Training on a million hours of found data will include errors (ASR transcript errors, speaker label errors, etc.). The model might learn some unwanted things like disfluencies or mimic ASR mistakes. Sesame mitigated this with filtering ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)) and presumably the sheer scale washes out individual errors. Still, careful data preprocessing is crucial. An overly noisy dataset can lead to muffled or slurred speech generation or incorrect pronunciations.

- _Long-range dependencies:_ Training with 2048-token context is heavy. Some tricks like FlashAttention and efficient positional embeddings (rotary) help, but the model might still struggle to perfectly utilize very long contexts (some attention heads might effectively ignore far context if not trained well). Ensuring the model really uses that capability (for example, by including full conversations, not just disjoint utterances, in training) is important. Also, the model might get confused if context is too long and contains many different speakers or topics – managing that (maybe truncating irrelevant old context) could be needed in deployment.

- _Balancing text vs. audio learning:_ The model has to learn both to be a decent language model (to predict text tokens correctly) and an acoustic model. If one dominates (e.g., too high weight on audio token loss), it might sacrifice language understanding. Conversely, if language part is overweighted, it might produce correct transcripts but with dull prosody. In training, keep an eye on this balance. You might empirically find you need to scale losses from text vs. audio differently. Sesame’s results suggest they got a good balance (since word error rate is low and prosody is good) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Objective%20metrics)).

- _Reproducing emotion and style:_ It’s challenging to make a model truly emotionally nuanced. Sesame had top talent and massive data; a smaller replication might end up more monotone. If that happens, you might need to incorporate explicit emotion modeling (as discussed) or add more expressive data (like acted emotional speech). Also, some subtle prosody elements (like laughter, gasps) are hard to get unless those appear in training data sufficiently. Sesame showcased laughter in samples, meaning their model somehow captured it (maybe from talk show data or such). That’s a high bar to reach.

- _Deployment issues:_ If you aim to deploy widely, consider runtime optimizations: distillation (compress the model by training a smaller model to mimic the big one’s outputs), or using knowledge transfer to separate models. For example, one could train a smaller 2B model that achieves nearly the same quality and runs on mobile devices. These are separate research efforts though. Also, be mindful of the **ethical and legal aspects**: generating voices that sound real can cross into deepfake territory. It’s good to implement safeguards (maybe a classifier to detect if the voice being generated is the same as a known public figure and block it, etc.) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=This%20project%20provides%20a%20high,we%20explicitly%20prohibit%20the%20following)). Sesame explicitly prohibits misuse like impersonation ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=explicitly%20prohibit%20the%20following%3A)). As a replicator, you should do the same for responsible AI practice.

- _Multilinguality:_ If you care about languages beyond English, note that CSM was primarily English-trained and didn’t do well in other languages ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)). To support other languages, you’d need to incorporate multi-lingual data. That might mean more code switching in text and maybe a larger text vocabulary (or byte-level encoding). This complicates training but is doable. Alternatively, train separate models per language or fine-tune an English model on other languages (the acoustic part of Mimi likely can handle other languages’ sounds given enough data, but semantic code may need retraining if language differs a lot).

In summary, replicating CSM entails combining expertise in **NLP (language modeling)**, **speech processing (audio coding and TTS)**, and **large-scale distributed training**. It’s a prime example of modern AI systems that straddle multiple domains. The publicly released code and model provide a blueprint, and by following the outlined steps – from data prep to model design to training tricks – a determined ML engineer or research group can build a system with similar capabilities. It’s important to set realistic milestones, leverage existing open-source components like Mimi, and iterate. The final outcome, if successful, is a powerful conversational speech model that can bring human-like voice interactions to life, marking a significant achievement in the intersection of speech and language AI.

**Sources:** ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Achieving%20voice%20presence)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=A%20common%20strategy%20first%20models,this%20during%20training%20is%20challenging)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)) ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)) ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=More%20fundamentally%2C%20Moshi%20is%20an,integrated%20multimodal%20modeling%20of%20Moshi)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=This%20project%20provides%20a%20high,we%20explicitly%20prohibit%20the%20following)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)) ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=%E2%9A%A0%EF%B8%8F%20Limitations%3A))
