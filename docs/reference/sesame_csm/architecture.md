# Machine Learning Model Architecture and Layout

**Overall Architecture:** CSM uses a _dual-Transformer_ architecture that operates on interleaved text and speech token inputs ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). It consists of two autoregressive Transformer models working in tandem: a large **Backbone Transformer** and a smaller **Audio Decoder Transformer** ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). The backbone is a multimodal model that processes both textual tokens (from the conversation transcript) and audio tokens (from previous speech, represented as discrete codes) in a single sequence. Its role is to model the high-level content and context – effectively understanding “what to say and how to say it” at a coarse level. The output of the backbone at each time step is not directly speech, but the first level of an audio code (the **zeroth codebook token**) which captures the semantic and prosodic content of the next chunk of speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). The second model, the audio decoder, then takes this predicted code and generates the remaining audio codebook tokens needed to produce the final speech waveform for that time step ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). In essence, the backbone produces a _context-aware linguistic+prosodic summary_ of the next bit of speech, and the decoder “fills in” the fine acoustic details to make it sound realistic ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). This division of labor allows CSM to remain end-to-end (the whole system is trained jointly) while being efficient: the heavy context modeling is done in one place, and the fine waveform reconstruction in another, smaller model ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)).

([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice)) _CSM’s dual-Transformer architecture at inference. Text tokens (orange `T`) and audio tokens (green `A`) from the conversation history are interleaved and fed into the large Backbone Transformer, which predicts the next semantic audio token (codebook 0). The smaller Audio Decoder then generates the remaining acoustic tokens (codebooks 1 through N−1) needed for that frame ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). The full set of audio tokens (`A`) for that frame is decoded to speech, and also fed back into the backbone for predicting subsequent tokens, until an end-of-utterance token is produced._ ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation))

**Dual-Transformer Framework:** The backbone and decoder operate in a loop to produce conversational speech. During generation, the model maintains a sequence of tokens representing the dialogue so far: this includes text tokens for things that were said, plus audio tokens for how they were said. At a given step, backbone takes in the recent text prompt (the new sentence it needs to speak) along with prior context tokens, and autoregressively emits the next audio code (semantic code) for the response ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)). The decoder immediately takes that code and generates the detailed acoustic codes (e.g. timbre, fine intonation) for the corresponding audio frame ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)). Once the decoder outputs are obtained, they are assembled into a complete audio frame which can be converted to a short waveform segment. Crucially, that audio token (`A` in the diagram) is fed back into the backbone’s input sequence for the next iteration ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20model%20inference%20process,audio%20and%20text%20transcription%20tokens)). This means the backbone always has access to the _up-to-date audio history_ of the conversation, including the very speech it has generated so far. The process repeats token by token until the model produces a special end-of-utterance token, indicating the response is complete ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20model%20inference%20process,audio%20and%20text%20transcription%20tokens)). Because the decoder is much smaller and faster, this two-stage autoregressive generation is still low-latency – the backbone does the complex part only once per frame, and the decoder’s work is quick to compute ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). The result is an integrated but efficient pipeline: by splitting at the semantic code (codebook 0), the system avoids having a single massive model generate _all_ waveform details sequentially (which would be slow) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=RVQ,time%20scenario)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). Instead, the backbone focuses on long-range coherence and context, while the decoder focuses on high-fidelity speech synthesis.

**Encoder-Decoder Analogy:** Although CSM’s architecture is autoregressive rather than a traditional encoder-decoder, you can think of the backbone+decoder division as analogous to an encoder-decoder system. The backbone “encodes” the text and prior audio context into a semantic audio representation, and the decoder “decodes” that into actual speech tokens. However, both are generative Transformers running in sequence each time step, rather than one encoding a fixed input and the other generating an output all at once. This design proved advantageous for modeling conversational speech: the backbone Transformer can attend over a long history of both modalities (text _and_ audio) to decide the manner of speaking, while the decoder Transformer, conditioned on that decision, ensures the speech comes out sounding natural and in the target voice ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). All components are trained together, so the backbone learns to produce semantic codes that are easy for the decoder to reconstruct into audio.

**Comparison to Moshi (Kyutai Labs):** The Moshi system by Kyutai Labs also uses a dual-transformer concept, but with a different layout and objectives. Moshi is built as a _full-duplex spoken dialogue model_, meaning it can listen and speak simultaneously in real time ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=More%20fundamentally%2C%20Moshi%20is%20an,integrated%20multimodal%20modeling%20of%20Moshi)). It employs a **global-local transformer architecture**, sometimes referred to as a Temporal vs. Depth transformer structure ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). In Moshi, a large 7B-param model (codenamed “Helium”) handles temporal sequence modeling (the global structure of the conversation over time), and a separate small “Depth” transformer handles the inter-codebook dependencies _within each audio frame_ ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)). This is conceptually similar to CSM’s backbone vs. decoder split, but Moshi’s design is even more specialized: the Depth transformer in Moshi is responsible for producing multiple acoustic codebooks in parallel for each time step (reducing the need to generate them one-by-one), while the Temporal transformer looks after the content of what’s being said and when ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). Additionally, Moshi integrates speech recognition and text generation into its architecture – it actually predicts text tokens (transcription) for the AI’s own speech as it generates it, as a form of _internal language modeling_ to guide the speech generation ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=Moshi%20models%20two%20streams%20of,to%20its%20own%20speech%2C%20its)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). CSM does not do this; it doesn’t generate any text internally (it only consumes text input and produces speech) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). As a result, Moshi can operate without a separate NLP brain – it can decide the dialogue and speak it, whereas CSM is usually paired with a separate large language model for deciding what to say ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). In terms of latency and real-time performance, Moshi’s architecture is highly optimized for streaming. By using the Mimi codec at 12.5 Hz and parallel codebook generation, it achieves as low as ~200 ms response latency on appropriate GPU hardware ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)), even allowing interruptions mid-speech. CSM, while efficient for a non-streaming model, typically achieves ~380 ms latency on a high-end GPU for an utterance (it generates slightly slower and in a turn-based fashion) ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). Both architectures share the principle of splitting high-level content modeling from low-level acoustic modeling, and both leverage the same audio codec (Mimi) – but Moshi blurs the line between “speech model” and “language model,” whereas CSM cleanly separates them (CSM strictly focuses on speech generation, delegating language understanding to other systems).



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Introduction](introduction.md)
* Next: [Technical Components](components.md)

