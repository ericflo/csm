# Comparison to Moshi by Kyutai Labs

CSM and Moshi are both cutting-edge conversational speech generation models, but they were developed by different teams with somewhat different goals, leading to differences in architecture and capabilities. Here we’ll compare them in several aspects:

**Architectural Design:** Both CSM and Moshi utilize dual transformer architectures, but the way they partition tasks is different. CSM splits by _function_ (context modeling vs. acoustic rendering) – its large backbone vs. small decoder ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). Moshi splits by _time scale_ – it has a Temporal (or “global”) transformer that handles the sequence over time, and a Depth (or “local”) transformer that handles the parallel prediction of codebooks at each time step ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). Another way to view it: CSM’s backbone is analogous to Moshi’s Temporal transformer (both model long sequences of a “primary” token stream: code0 for CSM, combined audio-text for Moshi), and CSM’s decoder is analogous to Moshi’s Depth transformer (both model dependencies among codebooks per frame). However, Moshi’s architecture is a bit more complex because it is a _speech-text_ model: it doesn’t just predict audio tokens, it also predicts text tokens for its own output concurrently ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=Moshi%20models%20two%20streams%20of,to%20its%20own%20speech%2C%20its)). Moshi essentially learned to _speak and listen in the same model_, integrating an ASR component and a TTS component. It uses a form of next-token prediction where the next token could be from either the user’s audio stream or the AI’s own audio stream or the AI’s own transcript ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=Moshi%20models%20two%20streams%20of,to%20its%20own%20speech%2C%20its)). CSM, by contrast, is only a TTS model (multimodal in that it takes audio context, but it doesn’t produce transcripts or take audio input on the fly) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). This means Moshi’s training and architecture had to account for two simultaneous audio streams (full-duplex) and a text stream, making it a _three-stream model (user audio, AI audio, AI text)_, whereas CSM is a _two-stream model (past audio, upcoming audio, plus text as annotations)_.

**Full-duplex vs. Turn-based:** A key practical difference is that Moshi is designed for **full-duplex dialogue** – it can handle speaking and listening at the same time without explicit turn-taking ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=More%20fundamentally%2C%20Moshi%20is%20an,integrated%20multimodal%20modeling%20of%20Moshi)). In Moshi’s demo, the AI can interject “mm-hm” while the user is still speaking or start formulating an answer before the user fully stops. This is achieved by Moshi’s architecture continuously processing input audio and generating output audio tokens with minimal delay (only 80 ms frame delay) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). It doesn’t wait for an “end of utterance”; it treats the conversation like two audio streams flowing concurrently. CSM, on the other hand, currently works in a **turn-based** manner. It assumes one speaker talks, then stops, then the other responds. It needs the text (and ideally the audio) of a user utterance as input context, then it produces the response. If the user were to interrupt while CSM is speaking, CSM has no mechanism to incorporate that mid-response – it would likely continue until completion (unless an external system stops it). Therefore, Moshi is more suitable for _natural, overlapping conversation_, whereas CSM is more aligned with the typical voice assistant interaction pattern (speak -> wait -> respond). Achieving full-duplex with CSM would require integrating an ASR to listen during output and perhaps cutting off generation if interruption is detected, but that would be a higher-level system solution, not something CSM inherently does.

**Integration of Language Modeling:** Moshi is, at its heart, a **speech-dialogue foundation model** – it not only generates speech, but also decides _what_ to say (like ChatGPT with a voice). In fact, the first component of Moshi (“Helium”) is a fine-tuned LLM that was trained to generate conversational content, and it outputs both text and some representation that guides the speech output ([Moshi: a speech-text foundation model for real-time dialogue](https://www.researchgate.net/publication/384563750_Moshi_a_speech-text_foundation_model_for_real-time_dialogue#:~:text=In%20this%20work%2C%20we%20introduce,is%20Helium%2C%20a%207B)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)). Moshi was instruction-tuned on dialogue data to follow prompts and produce appropriate answers, similar to how ChatGPT was tuned, but with audio in the loop ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=Developing%20Moshi%20required%20significant%20contributions,a%20live%20demonstration%20on%20stage)). CSM deliberately does _not_ include a language generation component – it requires an external text input for the content of speech ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). Sesame’s design philosophy was to separate concerns: use the best text-based LLM for language understanding and use CSM for rendering speech from that text. This has pros and cons. With CSM+LLM, you can upgrade the text brain independently (e.g., use GPT-4 or a specialized chatbot and then have CSM voice it). With Moshi, the text and speech are entwined; you have one model that does both, which is elegant but potentially less flexible (if the language part of Moshi is weaker than the latest ChatGPT, it’s not trivial to swap it out). However, Moshi’s integrated approach could yield more coherence between wording and prosody – since it knows exactly what it’s saying as it says it, it might place emphasis more appropriately. For instance, Moshi’s internal text prediction allows it to ensure that its spoken output matches the intended text exactly (because it literally generates the text as a token sequence too, reducing risk of say, mispronouncing a word or skipping a word) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). CSM doesn’t generate text tokens for its output, so it must rely on the provided text – if the text has a word it’s never heard, it might struggle (though presumably it can spell it out via phonemes if needed).

**Model Size and Efficiency:** Moshi’s main Transformer is about 7 billion parameters (plus whatever the Depth transformer adds, which is relatively small, maybe a few hundred million). CSM’s largest is 8B + 0.3B decoder. So in raw size they’re comparable. However, Moshi’s architecture might demand more compute at inference because it’s essentially performing ASR and TTS in one model – although the ASR part is mostly just ingesting audio tokens, which, thanks to Mimi, is also a 12.5 Hz stream, so it’s not that heavy. Moshi’s advantage is it does fewer autoregressive steps because of parallel codebook prediction and because it doesn’t wait for an end-of-utterance. In terms of **latency**, Moshi achieves a remarkable ~200 ms end-to-end latency (including the 80 ms frame delay) in a streaming scenario ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)). CSM, while very fast for a TTS system, is used in a call-response scenario where you likely have an additional ASR delay (maybe 200 ms) plus the CSM generation (~380 ms) – so the total user-perceived latency might be ~0.5-1.0 s if you include ASR. In a fair measure just of TTS, CSM might be ~0.3-0.4 s vs Moshi’s ~0.2 s, but Moshi doesn’t have the ASR step because it’s built-in. For practical purposes, both are low enough to feel interactive, but Moshi edges out if you need immediate back-and-forth.

**Voice Naturalness and Quality:** Both models produce highly natural voices, nearly human-like in many cases. Subjective evaluations (like CMOS scores) for CSM showed that without context, people could hardly tell it apart from real speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=The%20graph%20above%20shows%20the,However%2C%20when%20context%20is%20included)). Moshi’s public demos also astonished listeners with how human and spontaneous it sounded (to the point of some being unsettled). CSM places emphasis on using context to produce _appropriate_ prosody – e.g., its evaluations showed that when given conversational context, human evaluators preferred real human speech slightly over CSM, meaning CSM is extremely close but perhaps not perfect yet in contextual expressiveness ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=The%20graph%20above%20shows%20the,However%2C%20when%20context%20is%20included)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20currently%20trained%20on,trained%20language%20models)). Moshi, with its ability to “um” and interrupt, demonstrates a very high degree of conversational realism. One difference is that CSM’s output is deterministically tied to the input text. If the input text is poorly phrased, CSM will still read it in a fluent way, but it won’t _rewrite_ or improvise. Moshi can actually choose wording on the fly, leading to very fluid dialogues. This can make Moshi seem more _alive_ as a personality, whereas CSM is more like a highly advanced _reader/actor_. In terms of **speaker similarity** or cloning, both can impersonate voices given a sample. Moshi presumably can do zero-shot voice based on a short prompt (similar to how Vall-E does), and CSM can do it if provided a snippet in context. The underlying tokenization being the same (Mimi) means if both see the same voice token sequence, they should both be able to render that voice. If anything, CSM might do better at maintaining a specific voice across a long conversation because it was explicitly trained with speaker IDs over long contexts (ensuring consistency), whereas Moshi, by not using explicit turn tokens, has to implicitly keep track of which voice to use (but it obviously was designed to do so across its two streams).

**Context Handling:** CSM can handle 2048 tokens of context (roughly 2 minutes, could be many turns) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)). Moshi, being streaming, effectively can handle indefinite context – it doesn’t have a set context length in the same sense, because it processes input continuously. However, Moshi’s internal architecture might still have some window for attention for its temporal model. If it’s 7B, it might be similar to a GPT-3 sized model which often have context lengths 1K or 2K tokens. But since it’s processing at 12.5 Hz, a 2K token context in Moshi would correspond to 160 seconds of audio (if counting one token per frame per stream perhaps), which is actually longer than CSM’s 2 minutes. It’s unclear if Moshi uses relative positioning to allow infinite context or if it also has some limit. Moshi also uses the technique of predicting its own transcript, which provides a textual summary of the conversation as part of the context (the “inner monologue”). This likely helps it maintain context in long dialogues by relying on text memory (which is compact) rather than pure audio memory. CSM relies on having both text and audio tokens fed in for context – text gives it the words, audio gives it tone, as we discussed. Both approaches have merit. For replication purposes, Moshi’s approach is more complex to implement (because of multi-task training for ASR+TTS), whereas CSM’s approach is straightforward if you have the transcripts and audio tokens.

**Tokenization and Audio Compression:** Both CSM and Moshi use the **Mimi tokenizer**. This means they share the same fundamental audio representation. There might be minor differences: perhaps the exact number of codebooks or codebook sizes. If one of them tweaked Mimi for their use-case, that could mean slight differences in quality or compatibility. The Moshi paper mentions Mimi has an adversarial training loss and matches or beats other codecs at similar bitrates ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). If Sesame used Mimi as-is, CSM benefits from that as well. If Sesame developed “Mimi 2” or modified it, they didn’t say so explicitly – it sounds like they directly used Mimi (they even call it Mimi in the blog) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). Therefore, on the tokenization front, the approaches are identical. Both output one semantic token and multiple acoustic tokens per frame. Where they differ is how those tokens are generated: CSM’s decoder vs. Moshi’s depth transformer. Moshi’s depth transformer might generate all tokens at once for a frame (non-autoregressively, using a training scheme akin to parallel decoding). It’s mentioned that Moshi’s approach reduces the number of autoregressive steps thanks to Mimi’s low frame rate ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=dialogue%20framework,3kbps)), implying that for each frame it doesn’t do a lot of sequential work. It could be using something like a mask-and-predict for codebooks or a learned order. CSM’s approach is partly sequential (the codebooks are generated in order, albeit quickly). In theory, Moshi’s method could be slightly faster in generation within each frame, but both are so fast at that level it’s negligible compared to the big transformer passes.

**Model Efficiency and Footprint:** Moshi, by being end-to-end, requires one model to do everything. This means if one wanted to deploy Moshi on a device, that device must handle the 7B model. CSM could be split: ASR on device (maybe a small Whisper model), LLM in the cloud, CSM TTS in the cloud, etc. For strictly TTS usage, CSM’s 1B model is already out and much smaller to run. Moshi has not released weights yet (as far as known); only the demo is available. So from an open-source standpoint, CSM currently is more accessible. Efficiency also includes training efficiency: CSM’s training used the amortization trick to reduce memory, whereas Moshi’s training complexity came from multitask learning. It’s not obvious which was more efficient – both likely pushed GPUs to their limits. But since CSM’s team could train an 8B model on 1M hours, and Moshi’s team did a 7B model on (likely less data, perhaps tens of thousands of hours?), CSM probably consumed more raw compute to train (Google-sized effort), whereas Moshi was a startup effort with limited time.

**Differences in Naturalness and Context Handling:** In terms of _natural interactive behaviors_, Moshi can do things like incremental speech, backchanneling (“uh-huh”, “right…”) and quick turn-taking. CSM can produce very natural _speaking style_ but doesn’t inherently decide to inject backchannels unless the text explicitly says “uh-huh” (since it doesn’t generate discourse behaviors by itself). So Moshi feels more like a live conversational partner that can surprise you, while CSM is an extremely high-quality voice for an assistant that speaks when it’s supposed to. Another subtle point: Moshi’s integrated text+speech modeling might help with _pronunciation_. If Moshi encounters a word it’s not sure how to say, it might derive it from spelling via its language model knowledge. CSM relies on the training data and possibly the semantic token to get pronunciation right. Both claim to handle tricky cases (CSM tested homograph disambiguation using context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20better%20assess%20pronunciation%20and,based%20benchmarks)), which is typically a language understanding problem – CSM solved it by using textual context to choose correct pronunciation). So in that test, CSM is using context in a similar way as an integrated model would, except it’s leveraging that the text context disambiguated the word meaning and thus it knew which phoneme code to produce.

**Summary of Approach Differences:** In short, **CSM = LLM architecture + audio tokens; Moshi = joint LLM + speech architecture**. CSM chooses modularity (separate text generation, focus on speech rendering), while Moshi chooses unity (one model to rule them all). For replicating, CSM’s approach might be easier to start with, and indeed we have full resources for it, whereas Moshi’s approach might be more ambitious but potentially the future direction (since eliminating the boundary between deciding and speaking can yield more fluent interaction). Both achieve impressive efficiency with Mimi codec, but use slightly different tricks to overcome the multi-codebook challenge (CSM with a sequential decoder but skipping frames in training, Moshi with a parallel codebook prediction using a second transformer).

**Moshi vs CSM in Efficiency & Quality:** The ComfyUI Wiki’s summary suggests CSM achieved a 68% improvement in first-frame latency over prior methods ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=Official%20benchmarks%20show%3A)), highlighting how single-stage (CSM’s method) improves over two-stage pipelines. Moshi’s stats aren’t directly compared, but we know Moshi’s latency is extremely low by design. On quality benchmarks, if we had both side by side, they’d likely be close. Each might have slight edge in certain areas: e.g., CSM perhaps better at long monologues since it trained on 2-minute samples, Moshi better at rapid dialogue. On context, Moshi inherently uses whatever it heard to inform reply, while CSM can be paired with a strong text model to use context (the text model would handle remembering facts etc., whereas Moshi must rely on its internal weights for world knowledge).

In conclusion, while CSM and Moshi share the vision of natural conversational AI voices and even share technical building blocks (Mimi codec, Transformer modeling), they diverge in integration. CSM is **modular and focused on speech synthesis** with contextual awareness, whereas Moshi is **integrated and focused on full dialogue** (speech in and out). An engineer learning from both could consider a hybrid: e.g., using CSM as the TTS component in a Moshi-like full-duplex system, or adding a lightweight text generation head to CSM to give it some dialogue autonomy. The comparison highlights that CSM’s approach is somewhat more conservative (leveraging known LLM externally) and Moshi’s is more end-to-end (which is bold and impressive). Depending on the application (closed-loop assistant vs. open-ended chatbot), one or the other might be preferable. For replicating CSM, one can focus on the speech part and interface it with existing NLP systems, whereas replicating Moshi requires tackling ASR, NLP, and TTS all together.



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Code Implementation](implementation.md)
* Next: [Implementation Considerations](considerations.md)

