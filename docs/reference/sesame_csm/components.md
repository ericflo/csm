# Technical Components of CSM

**Backbone Transformer (LLaMA-based):** The backbone of CSM is a Transformer network based on the LLaMA architecture (a GPT-style decoder-only Transformer) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). It serves as the main _sequence model_ that reads the combined text and audio token sequence. In the largest “Medium” configuration, the backbone has around 8 billion parameters ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,Medium%3A%208B%20backbone%2C%20300M%20decoder)) (smaller variants with 1B or 3B params were also trained for research). Architecturally, it inherits typical features of LLaMA/GPT: multiple self-attention layers, each with many heads, and feed-forward networks, optimized for autoregressive token prediction. Sesame did **not** initialize this transformer from any pre-trained text model – it was trained from scratch on the conversational speech data ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)). This means the backbone had to learn linguistic patterns, contextual cues, and basic world knowledge directly from the audio transcripts (which is feasible given the massive 1M hour dataset). The backbone uses a SentencePiece tokenizer (or similar) for text, likely the same as LLaMA’s tokenizer, to convert input text to tokens ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). Audio, on the other hand, is tokenized by the Mimi codec (described below). To allow the model to distinguish modalities, special embeddings or token IDs are used so that the backbone knows which tokens are text and which are audio. Additionally, Sesame encodes **speaker identities** directly into the text token sequence as markers ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). For example, they might prepend a token indicating “Speaker A” vs “Speaker B” before each segment’s text, so the model can learn to speak in different voices or styles for different speakers. The backbone’s primary job is to output the correct _zeroth codebook token_ for the next audio frame given all the preceding context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)). In doing so, it must integrate conversational context (what has been said) and paralinguistic context (how it was said) to decide the appropriate prosody, timing, intonation, etc., for the upcoming speech. It also handles long-range coherence – with a context window of 2048 tokens (~2 minutes of audio/text) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)), the backbone can retain conversational memory and style over extended dialogues. This long context capability is crucial for maintaining consistency in things like tone or persona over the course of a chat. (For instance, if a user asked a question 1.5 minutes ago and the assistant is still answering, CSM can remember that far back to keep the answer relevant and in the same voice.) The backbone uses positional encodings (likely rotary position embeddings as in LLaMA) to manage such a long sequence. Overall, this transformer acts as the **brain of CSM**, figuring out _what_ audio should sound like before the finer details are filled in by the decoder.

**Contextual Understanding & Long-Form Dialogue Retention:** A standout feature of CSM is its ability to utilize **long conversational context**. Each training sample fed to the model was a sequence of up to 2048 tokens, corresponding to roughly 2 minutes of a conversation ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)). These sequences were constructed by alternating between speakers: e.g., [Speaker0’s text + audio tokens][Speaker1’s text + audio tokens][Speaker0’s text + audio tokens]… and so on ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). By training on such data, the backbone learns to interpret not just the last user utterance, but the entire history of what both it and the user have said. It retains information like the _dialogue state_ (has a question been answered already?), the _emotional tone_ of each speaker (was the user sounding frustrated or pleased?), and even speaking quirks or pace. Mechanisms like self-attention allow the model to attend to relevant parts of the history. For example, if the assistant is about to clarify something, the backbone might attend to the earlier part of the conversation where that topic was first raised, ensuring the clarification tone matches the context. This goes beyond what typical TTS models do – CSM isn’t generating speech in isolation, but as a turn in an ongoing exchange. As a result, it can exhibit behaviors like **dialogue coherence** (sounding hesitant when appropriate, or using a firmer tone if repeating an answer). The long context also facilitates _long-form synthesis_ (up to 2 minutes straight) without resetting style. In testing, the Medium CSM model showed that including conversational context significantly improved human preference for its responses’ appropriateness ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=The%20graph%20above%20shows%20the,However%2C%20when%20context%20is%20included)). Essentially, by remembering up to 2 minutes of dialogue, CSM provides continuity in multi-turn interactions that makes the voice assistant feel more “present” and aware. From an implementation perspective, handling 2048 token sequences with an 8B model is non-trivial (it’s a lot of computation), but Sesame’s training strategies (discussed later) and model optimizations made it feasible.

**Residual Vector Quantization (RVQ) Tokenizer – Mimi:** Instead of producing raw audio waveforms, CSM outputs **discrete audio tokens** thanks to a technique called Residual Vector Quantization. The team uses _Mimi_, a state-of-the-art neural audio codec developed by Kyutai (and adopted by Sesame for CSM) to convert audio into tokens and back ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)). Mimi works by encoding 24 kHz audio into a sequence of code vectors at 12.5 Hz (i.e., one set of codes every 80 ms of audio) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=dialogue%20framework,3kbps)). Each 80 ms frame of audio is represented by a stack of codebook indices. In Mimi’s case, there is **1 semantic codebook** (often referred to as codebook 0) and **N−1 acoustic codebooks** (codebooks 1 through N−1 for finer detail) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). For example, Mimi might use a total of 8 or 10 codebooks – one of them (the first) captures high-level features of the sound, and the rest capture the residual nuances. The semantic code (zeroth codebook) is trained to represent the content and broad prosody of speech in a compact way. In fact, Mimi’s training uses a form of **distillation from a self-supervised speech model (WavLM)** so that codebook 0 tokens carry linguistic/semantic information akin to a transcript, but also some prosodic cues ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). The remaining acoustic codebooks encode the additional information needed to reconstruct the original waveform with high fidelity – things like the exact voice timbre of the speaker, background noise, subtle inflections, etc. ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=phonetic%20features,specific%20identity%20and%20timbre)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=2,specific%20identity%20and%20timbre)). This RVQ scheme means that any piece of audio can be turned into a sequence of discrete tokens: one token per codebook per 80 ms frame. Conversely, given those tokens, a decoder (the Mimi vocoder) can synthesize speech that sounds very close to the original. For CSM, using Mimi tokens has several advantages. _First_, it transforms the continuous audio generation problem into a _discrete sequence prediction_ problem suitable for Transformers ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=One%20approach%20to%20modeling%20audio,two%20types%20of%20audio%20tokens)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Residual%20Vector%20Quantization%20%28RVQ%29%20,specific%20identity%20and%20timbre)). _Second_, by splitting the information across a semantic token and acoustic tokens, it allows the model to focus on high-level correctness first and detail second. The semantic token acts almost like a “pseudo-word” that the backbone can predict, representing what needs to be conveyed in sound ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=A%20common%20strategy%20first%20models,this%20during%20training%20is%20challenging)). _Third_, Mimi is highly efficient – it compresses speech down to about 1.1 kbps (kilobits per second) while preserving quality ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)). That low bitrate translates to far fewer tokens per second of audio (e.g., 12.5 tokens/sec for each codebook, so if there are ~8 codebooks, that’s on the order of 100 tokens/sec in total, whereas a naive 16 kHz PCM would be 16,000 samples/sec). Fewer tokens means the Transformers have less work to do to model a given duration of speech. Additionally, Mimi is _streaming-capable_ and causal by design (it encodes and decodes one frame after another without needing future context) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=dialogue%20framework,3kbps)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)), which aligns with CSM’s real-time goals. In summary, the Mimi RVQ tokenizer is the foundation that makes CSM possible: it provides the “vocabulary” of audio tokens. CSM’s training included teaching the model to predict these Mimi tokens directly. By operating on this discrete audio representation, CSM can handle aspects like speaker identity and audio style transfer inherently – e.g., if the context contains a certain speaker’s tokens, the model learns to continue using similar acoustic tokens to maintain that voice ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=2,specific%20identity%20and%20timbre)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). The semantic token (codebook 0) is especially important for bridging text and speech: since it captures phonetic content, the backbone essentially learns a mapping from text+context to a pseudo-phoneme representation. The subsequent acoustic tokens then ensure the phonemes are rendered in the correct voice and style.

**Audio Decoder and Multi-Codebook Generation:** The Audio Decoder in CSM is a smaller Transformer network dedicated to predicting the acoustic codebooks (levels 1 through N−1) given the backbone’s output. In the Medium model, the decoder has about 300 million parameters (much smaller than the 8B backbone) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,Medium%3A%208B%20backbone%2C%20300M%20decoder)). Despite its size, it’s crucial for producing high-quality speech because it generates the fine detail tokens. The decoder is conditioned on two things: (1) the backbone’s hidden representation or output embedding for the current time step (which conveys the semantic code and context info), and (2) any already-predicted lower-level codes for the current frame. One way to implement this is to feed the decoder the backbone’s predicted code0 along with an indication of which codebook level it’s currently predicting. Sesame’s design uses a **distinct linear output head for each codebook level** in the decoder ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). In other words, the decoder has multiple output projections sharing the same Transformer body – one head is responsible for predicting code1, another for code2, etc. During inference, the decoder can iterate through code1 to codeN−1, each time using the corresponding head to output a token ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). Likely, the decoder is run autoregressively across the codebooks (often referred to as running across “depth”), meaning it predicts code1, then takes that prediction as input to predict code2, and so on, until all acoustic codes for that frame are generated. (This sequential approach within a frame ensures that each higher codebook can depend on the lower codebooks, capturing the residual nature of RVQ ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=RVQ,suitable%20for%20offline%20applications%20like)).) However, because N−1 is relatively small (e.g., if N=8, then 7 acoustic levels), this process is very fast and could even be unrolled in a single forward pass if implemented cleverly. Training of the decoder is done jointly with the backbone: the decoder learns to minimize the error in predicting the true acoustic tokens given the true code0 (during teacher forcing) and any previously predicted lower-level codes. An interesting aspect is that the decoder’s **time dimension** is basically the number of codebooks, not the length of the utterance – it models a short sequence (the stack of codes for one frame) at a time, conditioned on the backbone output for that frame. This is why it can be much smaller; it doesn’t need to model long temporal dependencies, those are handled by the backbone. By keeping the decoder lightweight, CSM achieves low latency generation: once code0 is available for a frame, the decoder can very quickly spit out the rest of the codes ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)). This design is what makes CSM **single-stage** yet efficient – older two-stage TTS would first generate a mel-spectrogram or semantic tokens with one model and then invoke a completely separate model to vocode the entire utterance. CSM instead interleaves these steps frame by frame, with a tiny “vocoder” step (the decoder) that’s intimately guided by the context-aware backbone. The outcome is audio that is both high-fidelity and contextually appropriate. The use of multi-codebook modeling ensures that _natural speech characteristics_ like speaker idiosyncrasies, accents, and emotions are preserved in the output ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=2,specific%20identity%20and%20timbre)). Because the acoustic tokens explicitly encode things like voice timbre, the decoder can reproduce the unique sound of the speaker’s voice (or even mimic a style) as long as it has seen tokens from that voice in context. This plays into CSM’s ability to do _voice switching_ in multi-speaker conversations. It’s worth noting that training a multi-codebook decoder can be challenging – models must learn the right dependency order and not confuse the codebooks. Sesame’s approach of splitting at code0 is a clever simplification: it removes the need for the backbone to explicitly output prosody into a separate “spectrogram” representation, and instead uses the same Transformer to handle text and prosody. The decoder’s existence then doesn’t bottleneck the expressivity; it simply ensures the technical reconstruction of audio is accurate. In summary, the audio decoder component can be seen as **CSM’s neural vocoder**, one that is tightly integrated into the language modeling process (unlike traditional vocoders that operate independently after text processing).

**Expressive Speech Modeling and Emotion Classification:** One of Sesame’s goals was to imbue the model with **emotional intelligence** and expressive range, so that it can react to the subtleties of human speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Key%20components)). To this end, CSM incorporates an explicit emotion recognition component and training signals to handle expressive speech. According to Sesame’s technical disclosures, CSM features a **6-layer emotion classifier** network as part of its pipeline ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). This is likely a small Transformer or feed-forward model that takes as input either the audio tokens or the hidden state of the backbone corresponding to an utterance, and predicts an emotion category (e.g., neutral, happy, sad, angry, etc.). By training this classifier on the conversation data (possibly with labels inferred from an external model or certain dataset), the system gains the ability to **classify the emotion** in a speaker’s voice. This information can then be used to influence generation. One way Sesame could integrate this is by feeding the detected emotion of the user’s last utterance into the backbone (for example, as an additional token or embedding that the backbone attends to when producing its next outputs). That would allow the model to adjust its speaking style in response – for instance, speaking more softly or soothingly if the user’s emotion was detected as upset. Another possibility is multi-task training: the backbone might be trained not only to predict the next token, but also to predict the emotion of the current speaker from its hidden state, effectively learning representations that are sensitive to emotional tone. The presence of _emotion tokens_ in the Mimi codec’s semantic representation could also help – if the first codebook token encodes some emotional nuance (since Mimi compresses prosody), the backbone is inherently getting a signal of emotion from the audio context. The 6-layer classifier would refine that. In generating expressive speech, CSM benefits from its large training corpus which likely contained a wide range of natural emotions and speaking styles (e.g., excited storytelling, angry debates, empathetic responses). The model, seeing these in context, learned to mimic them. For example, in the demo, the voice can laugh, sigh, or change pacing – these are emergent capabilities from training on real conversational data and optimizing the right objectives. The **prosody** of CSM’s outputs is notably human-like (as external testers have pointed out, sometimes to an eerie degree). It injects pauses, emphases, and intonation patterns that fit the context. If a question is asked, CSM’s voice will likely end on a rising pitch. If it’s conveying sad news, it may speak in a lower, slower tone. All this is achieved without explicit rule-based controls, but through the model’s internalization of patterns. In summary, the technical components dealing with expressivity include (a) the model architecture’s ability to ingest and produce prosodic signals (through audio tokens), and (b) auxiliary components like the emotion classifier that guide the model to _understand_ and appropriately _produce_ emotional cues. This combination allows a machine learning engineer or developer using CSM to get a voice that isn’t just intelligible, but **expressively appropriate** to the conversation at hand. (In practical use, one might even select a desired emotion for the AI’s response by tweaking input prompts or using a fine-tuned version of CSM, although the base model doesn’t have a user-facing emotion control beyond what context implies.)



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Model Architecture](architecture.md)
* Next: [Training Pipeline](training.md)

