# Final Considerations for Recreating CSM

Recreating CSM from scratch is a complex but achievable project, given the information and resources now available. Here we outline a step-by-step roadmap and important considerations for an engineer attempting this, as well as the compute requirements and potential challenges.

**Step-by-Step Development Plan:**

1. **Obtain/Prepare a Large Conversational Speech Dataset:** As a foundation, gather a substantial amount of audio data with transcriptions. Aim for diverse, multi-speaker conversation data. Public datasets can be combined: for example, LibriSpeech/LibriLight for sheer hours of speech, Switchboard and Fisher for telephone conversations, Common Voice or VoxPopuli for crowd-sourced speech, podcast datasets for natural dialogue, etc. You’ll need transcripts and speaker labels. If not readily available, use an ASR like Whisper to transcribe audio and a diarization tool (like Pyannote or SpeakerNet) to label speaker turns. The goal is to produce a training corpus of token sequences where text and audio tokens alternate by speaker. If replicating at smaller scale, you might start with say 1–10k hours instead of 1M, but more data will noticeably improve naturalness and reduce overfitting. Ensure audio is standardized (e.g., 16 or 24 kHz mono) and segmented into chunks (e.g., 20–120 seconds segments) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)).

2. **Train or Use an Audio Tokenizer (RVQ-VAE):** CSM relies on the Mimi tokenizer, which you can either reuse or reimplement. The easiest path is to use the **pre-trained Mimi codec** from Hugging Face ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)). It provides an encoder that converts wav audio into a series of discrete codes and a decoder that does the reverse. If you want to train your own tokenizer, you’d train an autoencoder on audio: encoder produces a sequence of latent vectors (one per 80 ms frame), then have multiple quantization layers (codebooks) that progressively quantize the latent with residuals, and a decoder reconstructing the waveform. You’d use an adversarial + reconstruction loss as in SoundStream ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). This is a significant endeavor itself. Given Mimi exists, you can save time by adopting it – treat the Mimi encoder as a black box that gives you code0..N-1 for each frame. You might still want to verify its performance on your data (Mimi was trained on speech, so it should generalize to most voices). Using Mimi, encode all your training audio into token sequences and store them (preferably in a compressed form or on-the-fly compute). These tokens will form the “audio portion” of your training samples ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)).

3. **Define the Model Architecture:** Implement the dual-transformer model. You can use an existing Transformer framework (like HuggingFace Transformers) to create a GPT-like model, but you will need to extend it to handle two types of inputs and two transformers. Specifically:

   - Create the **Backbone** model as a decoder-only Transformer with a causal self-attention mask. The backbone’s vocabulary is the union of text tokens and audio tokens (code0 specifically). One approach: assign a range of token IDs for text (e.g., using a standard tokenizer like SentencePiece for text) and another range for audio code0 tokens. You might treat code0 tokens as additional vocabulary symbols. Alternatively, you can treat them as a separate embedding matrix and combine via a modality embedding. But simpler is one hot vocabulary with size = text vocab size + size_of_code0_vocab. The code0 vocab size is the number of entries in the semantic codebook (Mimi’s first codebook, which might be 1024 or similar). The backbone will be trained to predict the next token which could be either a text subword or a code0.
   - The backbone needs to be able to attend across a long history (so implement positional embeddings for up to 2048 or more positions). Follow LLaMA’s hyperparameters for layer count, hidden size, heads, etc., if aiming to replicate Sesame’s quality. For example, an 8B LLaMA-like model might have 32 layers, 4096-dim feedforward layers, and 32 attention heads. The open 1B model might have around 16 layers, 2048-dim FFN, etc.
   - Create the **Audio Decoder** model. This can be a smaller Transformer. One design: treat each codebook prediction as a “time step” in this decoder. For instance, if there are 8 codebooks (0-7), the decoder will produce codebooks 1 through 7. Code0 is given by backbone. So you can feed backbone’s output embedding as a start token to the decoder, then have the decoder run for 7 steps (each step predicts one deeper code). Use a causal mask so each step can only attend to previous ones (ensuring code2 sees code1, etc.). Or, to use the distinct head approach, you could simplify this: have the decoder attend to backbone output and itself (maybe 1 self-attn layer is enough since there’s a fixed small sequence) and then have multiple output heads that directly predict each codebook level in parallel ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)). During training, you’d provide the ground-truth code1..7 to compute loss on each head. During inference, you input the backbone output, and have the decoder output all required codes. Implement whichever approach is simpler for you – sequential decoding of 7 tokens is fine (7 isn’t much). The decoder should be significantly smaller: e.g., Sesame’s 300M decoder might be 6 layers, 1024 hidden size, 16 heads, something in that ballpark. It also needs an embedding for each codebook level it’s predicting (to know which level it’s doing if sequential).
   - Integrate the two: The backbone will output something (could be a probability distribution for code0). In training, you have the true code0 as next token, so the backbone loss is straightforward cross-entropy on code0 prediction. Then, conditioned on the _actual_ code0 (teacher forcing), you run the decoder to predict code1..N-1 for that frame, and compute decoder loss. In inference, you would sample backbone’s code0 (or take argmax) then feed it to decoder to sample the rest. Managing these pieces requires custom code beyond a standard language model training loop, but it’s manageable with PyTorch: you’ll write a forward function that does backbone(x) and decoder(backbone_output).
   - Don’t forget to prepend special tokens or embeddings for speaker IDs in the input. For example, you can reserve an ID in the text vocabulary for <speaker0>, <speaker1>, etc., and insert them accordingly in the text sequence. CSM used this to condition voices ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). You may decide how many distinct speaker embeddings you allow (Sesame might have used a generic scheme or a fixed small set and reused them, since they had arbitrary many speakers in data). You could also encode the speaker info in the audio tokens themselves (less direct). The simplest: add a token like “[S0]” before each speaker’s utterance text in the training data. The model will learn association of [S0] with certain audio that follows.

4. **Training Setup:** Set up a training loop that can feed the data to the model. Each training example is a sequence of tokens representing an exchange. Build batches of these (they should all be padded to the same length in the batch). Use a masking to ensure loss is only computed on real tokens, not padding. Use an optimizer like AdamW with a learning rate schedule. You will likely need distributed training (DDP) if using GPUs – at least if training the full 8B. Start with a smaller model first (for example, a 100M backbone + 20M decoder on maybe 1000 hours of data) to verify the pipeline works and the model can overfit a small sample, etc. Monitor training losses for both backbone and decoder. The backbone should learn to predict text and code0 tokens; the decoder learns code1+ tokens. Implement the **compute amortization**: when computing decoder loss, randomly choose a subset of frames in each sequence to apply decoder loss. For example, out of all frame positions in the sequence, sample ~6% and only compute loss for decoder’s outputs at those frames ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). For other frames, you can ignore decoder loss (and optionally even skip running the decoder to save compute, or run it but detach it to not backprop – whichever is easier). Ensure code0 loss is still computed at every frame to train backbone fully. This will require careful indexing, but it’s doable: you know which positions correspond to code0 tokens (every time a new frame starts). You can tag those, and then randomly mask out most of them for decoder training each step.  
   As for hyperparameters: likely a few hundred thousand steps with a batch size such that each step covers perhaps a couple of minutes of audio per GPU. Sesame did 5 epochs on 1M hours ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)), which is effectively 5e6 hours processed. If you have less data, you might do more epochs. Use gradient accumulation or micro-batching if needed to simulate a large batch size (to stabilize training for these large models, a total batch on the order of 0.5 to 2 hours of audio worth of tokens might be good). Keep an eye on validation – you can set aside some hours of dialogue as a dev set to see if the model isn’t overfitting and if the losses for text vs audio tokens behave well.

5. **Scaling Up:** Once the training pipeline works on smaller scale, scale the model to larger sizes gradually and the dataset accordingly. This will require multiple GPUs – consider using model parallelism or sequence parallelism if memory is an issue. The compute amortization will help memory, but the 8B backbone still will be heavy. You might utilize mixed precision and gradient checkpointing on the Transformer layers to manage memory. Training could be done on cloud TPUs as well (since the model is similar to a large LM, JAX/Flax or DeepSpeed frameworks might help). This stage is the most resource-intensive: an 8B model on say 100k hours might take several weeks on 16 GPUs. To reach 1M hours you likely need tens of GPUs for multiple weeks (Sesame likely used a specialized hardware setup or a lot of compute). Ensure to shuffle data well (so each epoch is random order) and periodically evaluate the model’s outputs for intelligibility and quality.

6. **Fine-Tuning and Specialty Training:** After base training, you may fine-tune the model on specific sub-tasks:

   - Fine-tune on **emotion-rich data**: If you have an emotion-labeled dataset (like CREMA-D, RAVDESS, or some internal set), you can fine-tune (or continue training) the model with an auxiliary loss to classify emotion from the context or from its own generated tokens. For example, add a small classifier head on the backbone’s final layer to predict an emotion label for the current utterance, and train with a multitask loss (LM loss + emotion classification loss). This mimics what Sesame’s 6-layer emotion classifier might do ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). This will encourage the backbone to encode emotional state in its hidden vectors. You can then condition generation subtly by emotion (or just let it be implicit).
   - Fine-tune for **voice consistency**: If your base model’s voices are too variable, you might fine-tune on a subset where one speaker (the assistant) uses a consistent voice (this could be done by narrowing down speaker embedding or injecting a target voice sample always for speaker 0). This will reduce variation and give you a stable persona voice for the AI.
   - Fine-tune to reduce **verbal filler**: Depending on data, model might generate some “um” or stutters if it heard them. If unwanted, filter those from training data or fine-tune on cleaner speech.
   - If you integrate with an LLM, you might fine-tune the _text style_ the model expects. For instance, if your LLM outputs very formal text but you want a conversational tone, you could have the LLM produce a hidden tag like “[casual]” and include that in text for CSM, and fine-tune CSM to alter its tone accordingly.

7. **Inference Pipeline Development:** With the model trained (or while training smaller versions), develop the inference code. This includes implementing the generation loop: feeding context tokens, using cached attention states for efficiency, and generating until an end-of-utterance token is produced. You should test that given some prompt text and some context, the model produces reasonable speech tokens. Then incorporate the Mimi decoder to turn those tokens into audio. Verify the audio quality. Often there might be a need for some **post-processing**: for example, inserting a short pause between sentences (the model might output tokens that correspond to silence if needed, or you can manually splice small silence if desired). Also ensure that the sampling strategy is right: CSM likely does _greedy or deterministic decoding_ for most cases because it’s aiming to render the given text accurately (not much stochastic freedom except maybe in how to express something). They might sample code0 from a small distribution to add variation in tone, but likely they pick the most probable tokens to match the transcript exactly. Experiment with this – if you sample too much, you might get prosody or even pronunciation drifting. Greedy might be safest to replicate their results of intelligibility. The decoder can be run in streaming mode – generate a few tokens, decode to audio frames, output – to reduce user perceived latency. Ensure the system can handle multiple speakers: test that if you provide a user utterance with one speaker ID and prompt a response with another, it indeed speaks differently. The GitHub example with `Segment` objects can guide how to do this in code ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=speakers%20%3D%20,%5D%20audio_paths%20%3D)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)).

8. **Hardware and Optimization:** When deploying the model, consider using quantization (8-bit or 4-bit) to shrink model size with minimal quality drop. Projects like GPTQ or Nvidia’s quantization can be used on the backbone. The decoder is small anyway, but quantizing it too won’t hurt. For 8B model, 8-bit quantization can cut memory roughly in half, allowing possibly a single GPU deployment. Use high-performance inference libraries if available; e.g., ONNX Runtime or FasterTransformer for the backbone (these can handle GPT models efficiently). Since CSM must often run alongside an ASR and an NLP model, optimizing each is important. A 4090 or A6000 GPU can run CSM 8B easily under 500 ms per sentence as shown ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)); for scaling to many users, you’d use multiple GPUs or a server cluster, or drop to the 1B model for faster, albeit slightly lower-quality, results.

9. **Testing and Evaluation:** After you have a working system, evaluate it thoroughly. Use objective metrics: WER (of re-transcribed output) to see if it pronounces words correctly, MOS (mean opinion score) or CMOS via listening tests to gauge naturalness, and the contextual tests like those Sesame did (homograph disambiguation ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20better%20assess%20pronunciation%20and,based%20benchmarks)), pronunciation consistency ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=model%20correctly%20pronounced%20different%20words,of%20the%20speaker%20and%20context))). Also test **dialogue coherence**: have multi-turn exchanges and see if the voice remains consistent and appropriate. Since you likely won’t reach the scale of Sesame’s training immediately, expect some gaps: perhaps the model might still have minor audio glitches or less stability in very long contexts. Iteratively refine by adjusting training (more data, or fine-tuning on problematic cases).

10. **Iterative Improvement:** Identify any shortcomings. For example, maybe your model struggles with a certain accent or with very fast speech in context. You could then incorporate more data of that type or adjust the model (maybe the acoustic model needs more capacity for those cases). Another area: if the model sometimes produces odd intonation for questions, you might fine-tune with more clear question examples. This process might be needed to approach the polish of CSM. Additionally, consider implementing the **evaluation suite** Sesame mentioned to quantify improvements. Over time, with enough data and tuning, your replica should approach human-level naturalness and strong contextual adaptivity, just like CSM.

**Compute and Hardware Requirements:** Building CSM from scratch is computationally intensive. To train an 8B parameter transformer on 1M hours (which is ~\*30 billion\* tokens if we roughly estimate tokens at 0.5 sec each) is a project that likely requires a supercomputer-grade setup. In practical terms, one would use parallel GPUs. For example, if using NVIDIA A100 40GB GPUs, one might use 32 of them and train for several weeks, using gradient accumulation to handle large sequences. If reducing scope, a 1B model on perhaps 100k hours could be trained on 8 GPUs in a few weeks. It’s advisable to start with something manageable like that to prove it out. A realistic hardware minimum for a serious attempt might be: 8 GPUs (each with 24+ GB VRAM), 100 terabytes of disk for storing data and intermediate tokens (if needed), and a lot of patience. Using cloud TPU v4 pods or multi-node GPU clusters with good interconnect can significantly speed things up. Inference, on the other hand, is much lighter: the final 8B model can run on a single high-end GPU in real-time, or on two for added ease. If you plan to serve many simultaneous users, you’d deploy multiple instances or use a model with fewer parameters or quantized weights.

**Potential Challenges:**

- _Data Noise and Quality:_ Training on a million hours of found data will include errors (ASR transcript errors, speaker label errors, etc.). The model might learn some unwanted things like disfluencies or mimic ASR mistakes. Sesame mitigated this with filtering ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)) and presumably the sheer scale washes out individual errors. Still, careful data preprocessing is crucial. An overly noisy dataset can lead to muffled or slurred speech generation or incorrect pronunciations.

- _Long-range dependencies:_ Training with 2048-token context is heavy. Some tricks like FlashAttention and efficient positional embeddings (rotary) help, but the model might still struggle to perfectly utilize very long contexts (some attention heads might effectively ignore far context if not trained well). Ensuring the model really uses that capability (for example, by including full conversations, not just disjoint utterances, in training) is important. Also, the model might get confused if context is too long and contains many different speakers or topics – managing that (maybe truncating irrelevant old context) could be needed in deployment.

- _Balancing text vs. audio learning:_ The model has to learn both to be a decent language model (to predict text tokens correctly) and an acoustic model. If one dominates (e.g., too high weight on audio token loss), it might sacrifice language understanding. Conversely, if language part is overweighted, it might produce correct transcripts but with dull prosody. In training, keep an eye on this balance. You might empirically find you need to scale losses from text vs. audio differently. Sesame’s results suggest they got a good balance (since word error rate is low and prosody is good) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Objective%20metrics)).

- _Reproducing emotion and style:_ It’s challenging to make a model truly emotionally nuanced. Sesame had top talent and massive data; a smaller replication might end up more monotone. If that happens, you might need to incorporate explicit emotion modeling (as discussed) or add more expressive data (like acted emotional speech). Also, some subtle prosody elements (like laughter, gasps) are hard to get unless those appear in training data sufficiently. Sesame showcased laughter in samples, meaning their model somehow captured it (maybe from talk show data or such). That’s a high bar to reach.

- _Deployment issues:_ If you aim to deploy widely, consider runtime optimizations: distillation (compress the model by training a smaller model to mimic the big one’s outputs), or using knowledge transfer to separate models. For example, one could train a smaller 2B model that achieves nearly the same quality and runs on mobile devices. These are separate research efforts though. Also, be mindful of the **ethical and legal aspects**: generating voices that sound real can cross into deepfake territory. It’s good to implement safeguards (maybe a classifier to detect if the voice being generated is the same as a known public figure and block it, etc.) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=This%20project%20provides%20a%20high,we%20explicitly%20prohibit%20the%20following)). Sesame explicitly prohibits misuse like impersonation ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=explicitly%20prohibit%20the%20following%3A)). As a replicator, you should do the same for responsible AI practice.

- _Multilinguality:_ If you care about languages beyond English, note that CSM was primarily English-trained and didn’t do well in other languages ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)). To support other languages, you’d need to incorporate multi-lingual data. That might mean more code switching in text and maybe a larger text vocabulary (or byte-level encoding). This complicates training but is doable. Alternatively, train separate models per language or fine-tune an English model on other languages (the acoustic part of Mimi likely can handle other languages’ sounds given enough data, but semantic code may need retraining if language differs a lot).

In summary, replicating CSM entails combining expertise in **NLP (language modeling)**, **speech processing (audio coding and TTS)**, and **large-scale distributed training**. It’s a prime example of modern AI systems that straddle multiple domains. The publicly released code and model provide a blueprint, and by following the outlined steps – from data prep to model design to training tricks – a determined ML engineer or research group can build a system with similar capabilities. It’s important to set realistic milestones, leverage existing open-source components like Mimi, and iterate. The final outcome, if successful, is a powerful conversational speech model that can bring human-like voice interactions to life, marking a significant achievement in the intersection of speech and language AI.

**Sources:** ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Achieving%20voice%20presence)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=A%20common%20strategy%20first%20models,this%20during%20training%20is%20challenging)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20a%20multimodal%2C%20text,end)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=interleaved%20text%20and%20audio%20to,end)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20stream%20for%20Moshi%20is,dependencies%20for%20a%20given%20step)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)) ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)) ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=More%20fundamentally%2C%20Moshi%20is%20an,integrated%20multimodal%20modeling%20of%20Moshi)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=This%20project%20provides%20a%20high,we%20explicitly%20prohibit%20the%20following)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)) ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=%E2%9A%A0%EF%B8%8F%20Limitations%3A))


---

**Navigation**

* [Back to Index](index.md)
* Previous: [Comparison with Moshi](comparison.md)

