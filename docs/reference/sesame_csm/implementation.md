# Code Implementations and Publicly Available Resources

Sesame AI Labs has made significant portions of CSM available to the public, which is a boon for anyone looking to understand or replicate the model. The core resource is the **CSM GitHub repository** (`SesameAILabs/csm`) which contains code and documentation for the model ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM)). As of March 13, 2025, they have released the **1B-parameter variant** of CSM (1B backbone + 100M decoder) along with a pre-trained checkpoint for that model on Hugging Face Hub ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=2025%2F03%2F13%20,checkpoint%20is%20hosted%20on%20HuggingFace)). The larger models (3B, 8B) were not immediately open-sourced, presumably due to their size and the sensitivity of extremely high-quality voices, but the 1B model still demonstrates the system’s capabilities and provides a reference for the architecture.

**GitHub Repository (SesameAILabs/csm):** The repository includes the model definition, training and inference code, and usage examples. The model architecture is implemented in Python (likely using PyTorch given the context) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=%2A%20%20Python%20100.0)). The code is organized to load the backbone and decoder models and run generation. For instance, there’s a `generator.py` that provides a high-level `generate` function to produce audio from text ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=from%20huggingface_hub%20import%20hf_hub_download%20from,generator%20import%20load_csm_1b%20import%20torchaudio)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=audio_paths%20%3D%20%5B%20,)). Using the repo is straightforward: after installation, you can do something like:

```python
from generator import load_csm_1b
generator = load_csm_1b("path/to/ckpt.pt", device="cuda")
audio = generator.generate(text="Hello from Sesame.", speaker=0, context=[])
```

This would output a PyTorch tensor with the waveform for the spoken sentence ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=from%20huggingface_hub%20import%20hf_hub_download%20from,generator%20import%20load_csm_1b%20import%20torchaudio)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=generator%20%3D%20load_csm_1b%28model_path%2C%20,speaker%3D0%2C%20context%3D%5B%5D%2C%20max_audio_length_ms%3D10_000%2C)). The `context` parameter can be used to pass in a list of prior segments (each segment containing text, speaker ID, and possibly raw audio or audio tokens) to ground the generation in conversation ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20sounds%20best%20when%20provided,for%20each%20speaker%20utterance)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)). The repository likely defines a `Segment` class or similar to structure that data. The example in the README shows how to load audio files for previous utterances and prepare segments with `Segment(text=..., speaker=..., audio=...)` ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=audio_paths%20%3D%20%5B%20,)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)). By studying this code, one can learn how the model expects inputs and how it formats outputs.

The GitHub also contains an **Apache 2.0 license**, meaning you can use and modify the code freely in your own projects ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=License)). The README provides important information such as the fact that the model is _not_ a general-purpose language model and cannot generate arbitrary text (so users shouldn’t try to have CSM invent new dialogue – it’s purely for speech generation from given text) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=match%20at%20L319%20CSM%20is,separate%20LLM%20for%20text%20generation)). It also addresses language support (mostly English) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=Does%20it%20support%20other%20languages%3F)) and some ethical guidelines for using the model responsibly ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=This%20project%20provides%20a%20high,we%20explicitly%20prohibit%20the%20following)).

For replication, the GitHub repository includes useful components beyond just the model code. It has (as per the ComfyUI Wiki summary) a **“complete architecture whitepaper”** and possibly design docs ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)) – this might be the blog post or an extended PDF describing CSM’s internals. It also references **REST API examples** ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)), which could help integrate CSM into applications (like a server that takes text and returns audio). An **audio preprocessing toolkit** is mentioned ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)), which likely contains scripts to tokenize audio with Mimi (so if you have your own audio, you can produce the RVQ codes) and perhaps to decode tokens back to audio. This is extremely useful if one wants to train or fine-tune the model on new data – you’d need to encode that data into tokens first using the same tokenizer. The **model quantization guide** implies they provide instructions or tools to convert the model to lower precision for faster inference or smaller memory footprint ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=GitHub%20Repository%20includes%3A)). All these resources lower the barrier to entry for experimentation.

**Hugging Face Models and Demos:** Sesame has uploaded the 1B model checkpoint to Hugging Face (`sesame/csm_1b`) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=2025%2F03%2F13%20,checkpoint%20is%20hosted%20on%20HuggingFace)). While the weights are gated (requiring acceptance of terms) ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm_1b#:~:text=You%20need%20to%20agree%20to,information%20to%20access%20this%20model)), it makes it convenient to download and use via the `huggingface_hub` library as shown in their example code ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=from%20huggingface_hub%20import%20hf_hub_download%20from,generator%20import%20load_csm_1b%20import%20torchaudio)). The model card on Hugging Face reiterates the architecture: a LLaMA-based backbone with a smaller decoder producing Mimi audio codes ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). It also links back to Sesame’s site for the blog post and to the GitHub repo for code ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=smaller%20audio%20decoder%20that%20produces,Mimi%20audio%20codes)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)). Hugging Face also hosts a **CSM demo Space** (likely a Gradio or Streamlit app) where users can input text and get audio out, or even simulate a conversation by inputting multi-turn dialogues ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=smaller%20audio%20decoder%20that%20produces,Mimi%20audio%20codes)). This is a quick way to test the model’s outputs without setting up code locally ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)).

Additionally, the **Mimi codec** itself is available as an open model: Kyutai Labs has a repository on Hugging Face for Mimi (`kyutai/mimi`) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=kyutai)) with a model card describing its function and even instructions to use it with Transformers or with Moshi ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Mimi%20codec%20is%20a%20state,1kbps)) ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Usage%20with%20)). The Mimi model card provides technical details like the frame rate (12Hz) and bitrate (1.1 kbps) and notes that Mimi’s first codebook is aligned with WavLM features ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=to%20get%20closer%20to%20the,improvements%20in%20terms%20of%20subjective)). It also links to a **GitHub repo for Mimi** (which might be part of the Moshi repository) and a paper reference ([kyutai/mimi · Hugging Face](https://huggingface.co/kyutai/mimi#:~:text=Model%20Sources)). If one wanted to replicate CSM’s tokenizer, they could use the Mimi code and pre-trained model directly instead of training a new codec from scratch – this saves a lot of effort and ensures compatibility (Sesame chose Mimi presumably because it was excellent and open). The Mimi HF model can encode audio to tokens and decode tokens to audio. This means if you generate tokens with CSM, you can synthesize them to sound using Mimi’s decoder (the CSM code likely wraps this process so you might not need to call Mimi directly).

**Other Resources and Discussions:** The research community has been actively discussing CSM and similar models. There are references to CSM in articles and forums. For example, Ars Technica wrote about the demo’s realism and the concept of “voice presence,” quoting Sesame’s goals ([Eerily realistic AI voice demo sparks amazement and discomfort online](https://arstechnica.com/ai/2025/03/users-report-emotional-bonds-with-startlingly-realistic-ai-voice-demo/#:~:text=Eerily%20realistic%20AI%20voice%20demo,)). This and other media (e.g., YouTube demos, the Product Hunt listing ([ Sesame - Conversational speech model that achieves voice presence | Product Hunt](https://www.producthunt.com/posts/sesame-4#:~:text=Upvote%20181))) provide qualitative insight and third-party impressions which can be valuable in understanding how humans perceive the model. Kyutai Labs’ Moshi project provides a point of comparison; their GitHub (`kyutai-labs/moshi`) is open-source and includes not only code but also a preprint on arXiv detailing Moshi’s architecture ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=,Face)) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=audio%20streams%2C%20Moshi%20predicts%20text,200ms%20on%20an%20L4%20GPU)). By reading Moshi’s code (which is in the same domain of speech-text modeling), an engineer can glean ideas about implementing streaming ASR, integrating a text model, etc., which could be complementary to what CSM does. Moshi’s code might also contain a PyTorch implementation of Mimi (since it uses Mimi as well) which could double-check how to do tokenization properly.

In terms of replication, these resources mean one does not have to start from zero. You can:

- Use the open-source **CSM 1B model** to experiment and familiarize yourself with the model’s behavior. This gives a baseline for voice quality and helps validate your environment (tokenization, model forward pass, etc.).
- Leverage the **code** to see how the model class is built. The architecture details like number of layers, hidden size, attention heads, etc., might be explicitly defined in the code or config. (If not in the readme, one might have to dive into the Python code in the repo to find, for example, a module that creates a `LlamaModel` or similar for the backbone, and a `DecoderModel` for the audio part.)
- Use the **preprocessing scripts** to prepare any custom data. For instance, if you want to feed a new audio file as context, you can convert it to tokens with Mimi so that it’s in the exact form the model was trained on.
- Follow the **evaluation scripts or metrics** if provided, to test output quality against references if you do any fine-tuning. The mention of homograph tests and CMOS in the blog indicates they have evaluation code internally – it might or might not be in the repo, but at least they described the procedures.

One thing to note: at the time of writing, the **full training code** for CSM had not been released, though the wiki suggests it may come later in 2025 ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=%E2%9A%A0%EF%B8%8F%20Limitations%3A)). This means that while we have the model architecture and an inference code path, setting up the exact same training (with distributed data loading, etc.) might require some work and guesswork. The open code likely allows fine-tuning on smaller data out of the box (since they expect researchers might try adapting the model). If one is replicating from scratch, they could reimplement training in PyTorch fairly readily using the model definitions as a guide. The biggest missing piece from code might be the _training schedule/hyperparams_, but those can be tuned or inferred from similar projects (e.g., LLaMA training recipes).

In addition to Sesame’s and Kyutai’s resources, it’s wise to look at related research like **AudioLM, VALL-E, and SpeechLM**. Some of those projects have released code or models that, while not identical in purpose, share components (e.g., AudioLM’s use of SoundStream tokens, VALL-E’s approach to zero-shot voice cloning with codebooks). OpenAI’s **Whisper** ASR and Microsoft’s **VALL-E X** (if code available) could provide further context for voice cloning and multilingual aspects. However, given that Sesame’s CSM is open and quite bleeding-edge, it likely represents the current state-of-the-art one can directly study.

To summarize, the publicly available resources for CSM include: the open-source **code repository**, a ready-to-use **1B model checkpoint**, the underlying **Mimi codec code/model**, a detailed **technical blog post**, and various **evaluation results and discussions** shared by the team. Together these give a comprehensive picture that a machine learning engineer can use to fully understand CSM’s design. By leveraging the code and models, one can experiment hands-on, which is invaluable for learning or attempting a reimplementation of the model from scratch.



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Inference and Processing](inference.md)
* Next: [Comparison with Moshi](comparison.md)

