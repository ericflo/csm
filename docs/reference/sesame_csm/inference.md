# Inference and Real-Time Processing

**Low-Latency Inference Strategies:** Despite its size, CSM is optimized to deliver quick responses suitable for real-time interactions. There are several strategies and design choices that enable this. First, as discussed, the decision to use the Mimi codec at 12.5 Hz greatly reduces the number of generation steps required per second of speech. If a response is, say, 5 seconds long, the backbone will produce on the order of 5 sec \* 12.5 tokens/sec ≈ 63 semantic tokens, and the decoder will produce the accompanying acoustic tokens. 63 steps of the large model is very manageable, even if the model is large, especially since self-attention at each step is only over at most 2048 context tokens (many of which can be cached from the past) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Image%3A%20CSM%20model%20inference%20process)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Both%20transformers%20are%20variants%20of,directly%20in%20the%20text%20representation)). Indeed, the Transformer’s autoregressive nature means that at inference, one does not reprocess the entire 2048 tokens at every step – one can cache the key/value vectors from the self-attention for the past context and only compute new ones for the new token. CSM takes full advantage of this standard Transformer caching to avoid recomputation. This means the cost per step is roughly constant (and much lower than the cost of processing the whole history in one go).

Another factor in latency is the model’s **two-stage generation**. By splitting tasks, the heavy backbone runs less frequently. The backbone essentially runs one forward pass per audio frame. The decoder runs N−1 forward passes for that frame’s acoustic tokens, but it’s small. If N is, say, 8, the decoder does 7 quick passes. This is still likely faster than if the backbone had to generate all 8 tokens sequentially by itself. Moreover, the decoder’s operations could be parallelized or fused – since it’s predicting a fixed small sequence of tokens, one could configure it to output all acoustic codes in one go (for example, by unrolling those 7 steps within the decoder with a single multi-head attention that outputs a sequence of length 7). It’s not explicitly stated if Sesame did this, but the phrase “distinct linear head for each codebook” ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=transformers,end)) hints that they might predict all acoustic tokens in parallel (each head produces its codebook’s token) once the decoder attends to the backbone output. If that’s the case, decoder latency is nearly constant (one pass for all levels). Even if it’s sequential, it’s a small constant factor per frame. So _time-to-first-frame_ of audio is just one backbone forward + decoder forwards. This contrasts with older approaches where an autoregressive model might have to generate, say, 50 coarse tokens before any audio could be produced (the “delay pattern” problem) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=RVQ,time%20scenario)). CSM can start outputting audio after generating the very first frame’s tokens.

Additionally, Sesame likely employed standard **efficient inference practices**: using half-precision (FP16) or even 8-bit quantization for weights to speed up matrix multiplies, using optimized kernels (FlashAttention for long context, etc.), and possibly doing small batch inference to amortize overheads. They achieved an average end-to-end latency of ~380 ms for a response, which likely includes the text input already in hand ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=1.%20Context%20Awareness%3A%202,speaker%20Support%3A%20Simultaneous%20voice%20processing)). (It’s not entirely clear if that 380 ms is just for TTS or includes some pipeline around it. But given <500 ms is reported, we assume it’s TTS on GPU.) Running the 8B backbone on a GPU like an RTX 4090 is feasible in real-time because 8B isn’t huge by modern standards – it fits in 24 GB VRAM with room to spare for cache, especially if weights are 8-bit. The 300M decoder is trivial in comparison. The self-attention mechanism might be the bottleneck due to 2048 context length, but efficient implementations handle that with tiling.

Another trick for low latency is **incremental audio playback**. CSM can generate audio frame by frame. In a live demo, the system doesn’t necessarily wait to finish generating the entire sentence before starting to play audio. Instead, as soon as a few frames are ready (e.g., 0.5 seconds of speech), it can be sent to the audio output. Because Mimi’s codec and decoder are streaming, the user can start hearing the speech while the tail end is still being generated. This pipelining hides some of the inference time. For example, the model might take 0.3 seconds to generate 2 seconds of speech, but the user hears those 2 seconds in real-time; by the time those 2 seconds have played, the model would have generated perhaps the next chunk if needed. This way the perceived latency is just the initial wait before speech starts (maybe a few hundred milliseconds). Full-duplex systems like Moshi go even further, but even in CSM’s turn-based scenario, chunked streaming output is beneficial.

**Sub-500ms Response Time Optimizations:** Achieving under half a second latency required optimizing every part of the pipeline. On the software side, they likely used PyTorch’s GPU acceleration and possibly TensorRT or ONNX Runtime for deployment to squeeze out extra speed. On the hardware side, an NVIDIA A100 or RTX 4090 can easily handle 8B param models with FP16 at many tokens per second. The reported average ~380 ms latency suggests that in many cases the model is faster (perhaps around 200–300 ms) for shorter utterances, but they account for some longer ones too. The <500 ms figure matches the typical expectation for an interactive voice assistant (it feels instantaneous). There is mention that Moshi’s design yields ~200 ms on an NVIDIA L4 GPU (which is a datacenter GPU) ([GitHub - kyutai-labs/moshi: Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi#:~:text=match%20at%20L303%20dependencies,200ms%20on%20an%20L4%20GPU)). CSM on a 4090 at 380 ms is in a similar ballpark; the difference might be due to Moshi not needing an external LLM step and some parallelism.

One specific challenge is that CSM must handle **multi-speaker context** without exploding latency or memory. The context could include another speaker’s audio. But note: during the assistant’s response generation, the user’s last utterance (audio) is already converted to tokens and is part of the context. There’s no _active_ processing of user audio at that moment; it’s just tokens to attend to. So multi-speaker doesn’t inherently slow down generation, it just adds to the context length which is fixed at max 2048. The heavy lifting of ASR (to get user’s text) is presumed external. So CSM basically always deals with one active speaker at generation time (itself), which simplifies things – it’s not generating two voices simultaneously, except in concatenated form in context.

Memory-wise, to deploy CSM one needs enough GPU memory for the model weights and the KV cache. The backbone self-attention cache for 2048 tokens _ 32 layers (if LLaMA-32) _ 8 heads (just guessing) could be a few hundred MB. It fits in 24GB along with weights. The decoder’s cache is negligible. If memory were tighter, one could reduce context length or use a lower-precision cache (FP16 to FP8 perhaps). But the recommended **hardware is an RTX 4090 or better** for running CSM ([Sesame Unveils CSM Voice Model for Natural Conversations | ComfyUI Wiki](https://comfyui-wiki.com/en/news/2025-03-03-sesame-csm#:~:text=Parameter%20Details%20Training%20Data%201M,Support%20RTX%204090%20or%20higher)), indicating ~24GB VRAM is needed for comfortable real-time use. This aligns with the 8B model size and large context.

**Handling Multi-Speaker Scenarios:** CSM was built to naturally handle dialogues with multiple speakers. At inference, the user can specify a sequence of `Segment` objects as context, where each segment contains a piece of text, an optional audio sample, and a speaker ID ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=speakers%20%3D%20,%5D%20audio_paths%20%3D)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)). The model will process these in order. Speaker IDs allow the model to maintain distinct voice characteristics for each participant in the conversation. If we want the assistant to have a consistent voice across turns, we use the same speaker ID (say, 0) for all its turns. The user could be speaker ID 1. During training, the diarization provided these IDs, so the model learned that tokens from speaker 0 should sound like one voice and speaker 1 like another within a single conversation. The open model wasn’t explicitly fine-tuned to any specific voice, but it can **mimic a voice given a sample**. For example, if you have a waveform of speaker 1 talking, you feed those audio tokens in the context with speaker=1. Then when the model generates a reply as speaker 1 in the future, it will tend to use the acoustic codes that match that voice timbre (because it “remembers” the recent audio tokens from that same ID). This way, CSM can carry multiple voices in one dialogue – essentially doing voice cloning on the fly for any speaker ID that it has audio for. In practice, the demo may have a preset voice for the AI (without needing external audio every time), and for the user it might treat the input audio as speaker 1’s exemplar. The generation call in the code example shows providing separate audio files for each utterance in context ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=audio_paths%20%3D%20%5B%20,)) ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=segments%20%3D%20,speaker%3D1)), which suggests the model was indeed given the actual audio of prior turns, not just the transcribed text. This is how it captures things like the user’s speaking style or the background tone, enabling it to respond appropriately (or even speak _as_ that user if needed).

When it comes to **real-time audio synthesis**, once the model has generated the audio code tokens for its response, those tokens must be converted to a waveform. The Mimi codec includes a decoder (which likely is a separate model or algorithm, possibly a small neural net) that takes the sequence of tokens and produces the audio samples. This decoding is very fast – typically neural codecs can decode faster than real time by a factor of 20-50x even on CPU. So the bottleneck is the Transformer generation, not the final waveform reconstruction. In a deployed system, one would run the CSM model on GPU to get the RVQ codes, and then run the Mimi decoder either on GPU or CPU to get actual sound. Since the model outputs 80 ms of audio at a time, it could potentially stream those 80 ms chunks through the decoder and start playing them out through speakers immediately. This streaming decoder approach ensures that the user hears the voice with minimal delay.

**Memory Efficiency and Deployment Optimizations:** For deployment, in addition to quantizing weights, one can also **optimize the context management**. For example, if the conversation exceeds 2048 tokens, one might start dropping the oldest context (in a chatbot scenario after a while). Or compress older audio context by keeping only semantic tokens from far-back turns instead of all acoustic tokens (to save space). The open-source project might not have implemented context compression, but a replicator could consider it. Another optimization is using a **smaller model for inference** if some quality can be traded for speed – the 1B CSM runs faster and could possibly achieve <100 ms latency, which might be valuable for certain applications, albeit with reduced voice naturalness.

In the interactive demo setting, the system likely works as follows: (1) User speaks, (2) ASR transcribes to text and also provides audio tokens, (3) CSM takes ASR text and prior context (including user audio tokens) as input and generates response audio tokens, (4) Mimi decoder produces speech waveform, (5) play audio. Steps 2-4 each add a bit of latency, but through concurrency (ASR on CPU, TTS on GPU, streaming, etc.) they manage a smooth experience. The target of sub-500 ms means the user perceives almost no lag.

To conclude, CSM’s inference pipeline is designed for **responsive, real-time performance**. It leverages the efficiency of discrete audio tokens, caching in Transformers, a small second-stage model, and possibly parallelization to ensure that even though it’s doing a lot under the hood (considering context and generating rich speech), it operates within the tight timing constraints of natural conversation. For a machine learning engineer, reproducing these latency numbers will involve carefully profiling the model, using GPU acceleration to the fullest, and possibly trimming model size or context where necessary. The good news is that Sesame’s results show it’s very achievable to have an 8B parameter model driving a realistic voice that responds in under half a second – a milestone that opens the door to truly interactive voice AI.



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Training Pipeline](training.md)
* Next: [Code Implementation](implementation.md)

