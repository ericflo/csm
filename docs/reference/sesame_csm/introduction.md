# Introduction to Sesame AI Lab and CSM

Sesame AI Lab is a research team focused on advancing AI-driven voice technology to achieve what they call “voice presence” – the feeling that a machine’s voice is as genuine and engaging as a human’s ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Achieving%20voice%20presence)). Traditional voice assistants and text-to-speech systems often sound neutral and lack emotional depth, which makes interactions feel flat over time ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Today%E2%80%99s%20digital%20voice%20assistants%20lack,the%20initial%20novelty%20wears%20off)). To address this, Sesame AI Lab set out to create AI companions that can carry _interactive, context-aware conversations_ with natural prosody and emotion. Their Conversational Speech Model (CSM) is the centerpiece of this effort, designed to generate speech that not only has high audio quality but also responds appropriately to the conversational context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)).

CSM’s development was driven by key innovations in conversational speech generation. First, it frames speech synthesis as an **end-to-end multimodal problem**: unlike conventional two-stage pipelines (where a model predicts a transcript or prosody tokens and a separate vocoder produces audio), CSM directly generates audio tokens from text and audio context in a single integrated process ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)). This _single-stage_ approach improves efficiency and expressivity by allowing the model to jointly consider linguistic content and acoustic context when producing speech ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20this%2C%20we%20introduce,common%20public%20evaluations%20are%20saturated)). Second, CSM is explicitly designed to leverage **conversation history** – it looks at previous dialogue (both what was said and how it was said) to inform the speech it generates ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)). This helps solve the “one-to-many” mapping problem in speech generation: a given sentence can be spoken in many ways, and by providing context (tone, tempo, recent dialogue), CSM can choose a rendition that fits naturally ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20create%20AI%20companions%20that,Capturing%20these%20nuances%20requires)). In practice, CSM can adapt its speaking style to the situation – for example, sounding sympathetic if the prior user turn was sad, or more energetic if the conversation is upbeat. Another innovation is Sesame’s focus on new evaluation metrics and realistic benchmarks for conversational speech. They found that common metrics like word error rate or speaker similarity are saturated (modern TTS models already score near-human) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,human%20performance%20on%20these%20metrics)), so they introduced context-specific tests (like disambiguating homographs through context, and maintaining pronunciation consistency in dialogue) to measure CSM’s understanding of context ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=faithfulness%20to%20text%2C%20context%20utilization%2C,study%20using%20the%20Expresso%20dataset)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20better%20assess%20pronunciation%20and,based%20benchmarks)). These efforts underscore that CSM isn’t just about generating **audio** – it’s about generating the **right** audio for a given conversational moment.



---

**Navigation**

* [Back to Index](index.md)
* Next: [Model Architecture](architecture.md)

