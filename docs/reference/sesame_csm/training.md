# Training Pipeline and Data Processing

**Data Collection and Preprocessing:** Training CSM required an enormous amount of conversational speech data. Sesame assembled a custom dataset of approximately **1 million hours of audio** (predominantly English) drawn from public sources ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)). They have not named specific datasets, but likely candidates include audiobook collections, podcast archives, YouTube videos with dialogues, call center recordings made public for research, etc. The key was to get diverse, **multi-speaker conversational** audio. Each audio source was processed with automated speech recognition (to get transcripts) and **diarization** (to identify speaker turns) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)). Diarization is important because the model needs to know _who_ is speaking when, so that it can learn to assign consistent voices to each speaker ID and follow turn-taking structure. The data was also segmented into manageable chunks – rather than feeding the model unbroken hours, they cut it into dialogue segments roughly a couple of minutes long or shorter, to fit the 2048-token window. Filtering was applied to ensure quality: for instance, segments with bad ASR transcripts or extremely noisy audio might be removed ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Dataset%3A%20We%20use%20a%20large,hours%20of%20predominantly%20English%20audio)). The resulting training examples each contain a sequence of text and audio tokens. A single example might look like: _[Speaker0: “Hello, how are you?” + audio of that]_, _[Speaker1: “I’m doing well, thanks.” + audio]_, _[Speaker0: “Great to hear.” + audio]_, etc., all concatenated. By structuring the data this way, the model learns to predict each speaker’s next audio given the ongoing conversation. Moreover, using a million hours ensures a vast coverage of speaking styles (different ages, accents, emotions) and linguistic content (from casual chats to narrative storytelling). This massive scale – orders of magnitude larger than typical TTS datasets – is what empowers the large backbone model to generalize and produce human-like dialogue. We can infer some sources: public audiobooks (like LibriVox) could contribute thousands of hours of read speech; switching between narrator and character dialogues might simulate multi-speaker. Podcasts or talk shows provide more natural conversational cadence. The dataset likely also included **Expressive TTS datasets** (like Blizzard, VCTK, etc.) and **Emotion datasets** merged in, to boost the expressive range. All audio was presumably normalized to 24 kHz and processed through the Mimi tokenizer to convert into discrete codes for training. The text was probably normalized (lowercased, no punctuation maybe) to match ASR style transcripts. In sum, Sesame’s data pipeline created a training corpus that pairs _text+audio at a very large scale_, well-aligned for the model to learn speech generation in context.

**Training Framework and Hyperparameters:** Training a model as complex as CSM demanded careful design of the training loop and substantial compute resources. The team trained CSM end-to-end using the paired data, treating it as a **language modeling task** over an extended token vocabulary (which includes both text subword tokens and audio code tokens). Concretely, at training time the model is fed a long token sequence (2048 tokens) and it tries to predict the next token at each position. These tokens could be text or audio, so the model is learning to handle both. Whenever a text token is next in sequence (e.g., the next words of a transcript), the model’s job is effectively like that of a language model predicting text. Whenever an audio token is next (e.g., the next code0 or code1 of a response), the model’s job is to output the correct code index. The loss function is the sum of the cross-entropy losses for predicting all these next tokens. Because audio tokens far outnumber text tokens in the sequence (continuous speech has many frames, whereas text might be shorter), a large portion of the training optimizes predicting the audio tokens correctly. Training was done for **5 epochs** over the 1M-hour dataset ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=,decoder)). One epoch here means the model saw each hour of audio once (though likely shuffled in short segments). Five epochs means effectively 5 million hours worth of audio-equivalent training. Even at batch sizes of e.g. one hour per GPU, this is an immense number of iterations. To handle this, the training was almost certainly distributed across many GPUs (or TPUs). The backbone being 8B parameters and the context 2048 tokens means memory per example is huge – distributed data parallelism with gradient checkpointing and possibly model parallelism (sharding layers across devices) would be needed. The training utilized mixed precision (FP16 or bfloat16) to speed up computation and fit in memory. While exact hyperparameters aren’t public, we can surmise they used an AdamW optimizer (common for transformers) with a learning rate schedule (probably warming up then decaying). The batch size (in terms of tokens) might have been chosen to maximize throughput without overflow. One aspect mentioned in the GitHub is that **the open-source release is the 1B variant** ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=CSM)). That suggests the full 8B model might have been more of an internal or demo model, whereas they made a smaller one easier to retrain or run by the community. The 1B model training would be easier (still hundreds of thousands of hours, but can be done on fewer GPUs). For replicating the model from scratch, one should plan for _industrial-scale_ training if targeting the full size and data – i.e. multiple weeks on a large GPU cluster. It’s worth noting that unlike some speech models that first pre-train on audio alone or text alone, CSM was trained _from scratch on the joint task_. They did not first train the backbone as a text LM (and indeed they mention it does not use pre-trained LLM weights) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=match%20at%20L371%20CSM%20is,trained%20language%20models)), nor did they pre-train an audio-only model except for the tokenizer. This joint training is enabled by the huge data: the model effectively learns ASR (speech recognition) internally to map audio context to an internal text-like understanding, and TTS internally to map text to audio tokens – all as part of one sequence modeling objective. It’s an ambitious training setup, but the end result is a single model that “knows” speech and text modalities together.

**Compute Amortization for Efficiency:** One of the most novel training techniques used in CSM is what Sesame calls a _compute amortization scheme_, which addresses the heavy memory and compute load introduced by the multi-codebook audio decoder ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=This%20design%20introduces%20significant%20infrastructure,which%20are%20crucial%20for%20performance)) ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). The problem is that if the model had to calculate loss for every single audio token (including all N codebooks for every frame in a 2-minute sequence), the amount of computation would be enormous – the decoder would backpropagate through maybe tens of thousands of tokens per sample. To make this feasible, Sesame **sparsified the training of the audio decoder**. In each training batch, for each training example, they randomly select only a small fraction of audio frames for the decoder to learn on ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). Specifically, they used _1/16 of the frames_ (about 6.25%) and computed the decoder’s loss on those frames, ignoring the others ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). The backbone, however, still learns on every frame (predicting code0 for each frame in the sequence) – so the semantic modeling is always fully trained ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). This approach dramatically reduces gradient computations for the decoder. Essentially, they “amortize” the decoder training over multiple batches: any given batch updates the decoder on a subset of frames, but over many batches (on expectation after 16 batches) the decoder sees all frames. Importantly, they reported that this did **not hurt the decoder’s performance** – there was no noticeable difference in decoder loss or output quality when using this 1/16th frame sampling ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=To%20address%20these%20challenges%2C%20we,training%20when%20using%20this%20approach)). Likely the redundancy in speech (lots of frames are similar) and the strong conditioning from the backbone makes it possible for the decoder to learn effectively even with sparse feedback. This trick is a big reason they could scale to an 8B backbone with an RVQ decoder. It mitigates the memory bottleneck: since the decoder is smaller, and now it’s also only active for a few frames per sequence on the backward pass, the peak GPU memory usage and computation time per iteration drop significantly. From an engineering perspective, implementing this means when preparing each training sample, you mark most audio frame positions as “don’t compute decoder loss”. You still run the decoder during the forward pass for continuity (or you might even skip forward passes on some frames), but you only compute gradients for the chosen subset. Over the course of training, this random sampling ensures all frames and positions influence the decoder weights eventually. Another benefit is faster iteration – more batches per second – allowing more epochs or larger batches, which can improve convergence. This technique could be seen as a form of _stochastic layer skipping_ during training, and it might inspire similar approaches in other multi-stage models. For someone replicating CSM, adopting this amortized training method would be crucial to reach the same scale without a prohibitive compute budget.

**Fine-Tuning for Contextual Memory and Quality:** After the main training, the base CSM model can generate speech given text and context. Sesame then applied fine-tuning for specific improvements and uses. They mention that a _fine-tuned variant_ of CSM was used to power their interactive voice demo ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=A%20fine,shown%20in%20our%20blog%20post)). This suggests that after the general training on 1M hours, they performed additional training passes on targeted data. Possible fine-tuning steps include:

- **Conversation fine-tuning:** They might fine-tune the model on actual _conversations_ (as opposed to random segments) to reinforce dialogue coherence. If the initial training data included a lot of monologue or audiobook style speech, an extra fine-tune on dialogue-heavy data (like customer service conversations or multi-turn chat transcripts) could help the model handle back-and-forth interactions even better, especially to maintain _2048-token conversation retention_ in a stable way.
- **Style/Persona fine-tuning:** The demo voices were noted to be “optimized for friendliness and expressivity” ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=Building%20a%20digital%20companion%20with,the%20potential%20of%20our%20approach)). To achieve this, Sesame could fine-tune CSM on a subset of data where the assistant speaks in a friendly tone. They might have picked training examples with a certain emotional tone or even recorded custom dataset for the assistant’s persona. This would adjust model weights to prefer a particular style when appropriate.
- **Safety and appropriateness:** In an interactive setting, they’d want the model not to use offensive tones or mimic sensitive voices. Fine-tuning (or instruction via the prompt) could be used to imbue the model with a “polite filter” on how it speaks.
- **Multi-speaker consistency:** The base model can speak in many voices (since it wasn’t tuned to one). They might fine-tune separate _voice profiles_ by conditioning on a single speaker’s data to get a more consistent voice if needed for certain deployments. However, the open-sourced model **does not** come with specific voices baked in – it’s a _base model_ capable of many voices ([GitHub - SesameAILabs/csm: A Conversational Speech Generation Model](https://github.com/SesameAILabs/csm#:~:text=Does%20this%20model%20come%20with,any%20voices)). That implies no single-voice fine-tune was done for the release (which is good for generality).
- **Emotion recognition integration:** If the emotion classifier was trained after the base model, they could fine-tune the backbone to better utilize the classifier’s output. Alternatively, train the classifier alongside the model in a multi-task fine-tune phase. This would sharpen the model’s sensitivity to emotional context.

In comparing to **Moshi’s training approach**, Moshi likely went through an analogous two-step process: first training on raw audio/text, then **multimodal instruction tuning** to make it behave as a conversational agent ([Meet Moshi, the first real-time voice AI](https://kyutai.org/2024/07/03/meet-moshi.html#:~:text=Developing%20Moshi%20required%20significant%20contributions,a%20live%20demonstration%20on%20stage)). The big difference is Moshi used a pre-trained language model (the 7B “Helium”) as a starting point for text understanding ([Moshi: a speech-text foundation model for real-time dialogue](https://www.researchgate.net/publication/384563750_Moshi_a_speech-text_foundation_model_for_real-time_dialogue#:~:text=In%20this%20work%2C%20we%20introduce,is%20Helium%2C%20a%207B)), whereas CSM did not leverage a pre-trained text model ([Crossing the uncanny valley of conversational voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#:~:text=CSM%20is%20currently%20trained%20on,trained%20language%20models)). Moshi’s team reported going from scratch to a working model in 6 months with 8 researchers, suggesting they used transfer learning and heavy existing components, whereas Sesame spent more time curating data and training from ground up. Additionally, Moshi had to coordinate ASR and TTS together (since it’s full-duplex), possibly training in stages (first ASR, first TTS, then combined). CSM’s training pipeline is simpler in that it is one unified sequence task (albeit huge). In terms of scale, both likely used several hundred GPUs – the details aren’t public, but training multi-billion parameter models on ~million-hour data is at the cutting edge of what’s being done in industry research.

To summarize, training CSM from scratch involved: building a massive dual-modal dataset, designing a special architecture and tokenizer, using a clever training trick to handle the load, and possibly fine-tuning for performance and style. Each of these steps required careful engineering but they are documented at a high level in Sesame’s blog and code release, providing a roadmap for replication.



---

**Navigation**

* [Back to Index](index.md)
* Previous: [Technical Components](components.md)
* Next: [Inference and Processing](inference.md)

