"""
Implementation of LoRA (Low-Rank Adaptation) for MLX models.

This module provides LoRA implementation for fine-tuning MLX models
with parameter-efficient training methods.
"""

from typing import Dict, List, Optional, Tuple, Union, Set
import math
import numpy as np
import mlx.core as mx
import mlx.nn as nn

class LoRALinear:
    """
    LoRA implementation for linear layers in MLX.
    
    This class implements the LoRA method from the paper:
    "LoRA: Low-Rank Adaptation of Large Language Models"
    https://arxiv.org/abs/2106.09685
    
    It adds low-rank adapters to a linear layer for efficient fine-tuning.
    """
    
    def __init__(
        self,
        base_weight: mx.array,
        base_bias: Optional[mx.array] = None,
        r: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
        use_bias: bool = False,
        name: str = ""
    ):
        """
        Initialize a LoRA adapter for a linear layer.
        
        Args:
            base_weight: The original weight matrix [out_features, in_features]
            base_bias: Optional bias vector [out_features]
            r: LoRA rank
            alpha: LoRA scaling factor
            dropout: Dropout probability for LoRA layers
            use_bias: Whether to use bias in LoRA layers
            name: Name for the layer (for parameter naming)
        """
        self.name = name
        self.in_features = base_weight.shape[1]
        self.out_features = base_weight.shape[0]
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r
        self.dropout = dropout
        
        # Store the original weights (frozen)
        self.base_weight = base_weight
        self.base_bias = base_bias
        
        # Initialize LoRA matrices with small values
        # Default initialization follows original LoRA paper
        # A is initialized with random normal, B is initialized with zeros
        self.lora_A = mx.random.normal(
            (r, self.in_features),
            scale=1.0 / math.sqrt(self.in_features)
        )
        self.lora_B = mx.zeros((self.out_features, r))
        
        # Optional LoRA bias
        self.lora_bias = mx.zeros((self.out_features,)) if use_bias else None
    
    def __call__(self, x: mx.array) -> mx.array:
        """
        Forward pass through the LoRA-adapted linear layer.
        
        Args:
            x: Input tensor [batch_size, seq_len, in_features]
            
        Returns:
            Output tensor [batch_size, seq_len, out_features]
        """
        # Original path with frozen weights
        base_output = mx.matmul(x, self.base_weight.T)
        if self.base_bias is not None:
            base_output = base_output + self.base_bias.reshape(1, 1, -1)
        
        # LoRA path
        # Apply dropout if needed
        x_for_lora = x
        if self.dropout > 0:
            x_for_lora = nn.dropout(x_for_lora, self.dropout, deterministic=False)
        
        # Apply low-rank adaptation
        # (x @ A.T) @ B.T = x @ (B @ A).T
        lora_output = mx.matmul(x_for_lora, self.lora_A.T)
        lora_output = mx.matmul(lora_output, self.lora_B.T)
        
        # Scale output based on alpha/r
        lora_output = lora_output * self.scaling
        
        # Add LoRA bias if used
        if self.lora_bias is not None:
            lora_output = lora_output + self.lora_bias.reshape(1, 1, -1)
        
        # Combine outputs
        return base_output + lora_output
    
    def parameters(self) -> Dict[str, mx.array]:
        """
        Get trainable parameters (only LoRA matrices).
        
        Returns:
            Dictionary of parameter name to parameter value
        """
        params = {}
        params[f"{self.name}.lora_A"] = self.lora_A
        params[f"{self.name}.lora_B"] = self.lora_B
        
        if self.lora_bias is not None:
            params[f"{self.name}.lora_bias"] = self.lora_bias
            
        return params
    
    def update(self, params_dict: Dict[str, mx.array]) -> None:
        """
        Update LoRA parameters from dictionary.
        
        Args:
            params_dict: Dictionary of parameter name to parameter value
        """
        # Update LoRA matrices if present
        if f"{self.name}.lora_A" in params_dict:
            self.lora_A = params_dict[f"{self.name}.lora_A"]
        
        if f"{self.name}.lora_B" in params_dict:
            self.lora_B = params_dict[f"{self.name}.lora_B"]
        
        if self.lora_bias is not None and f"{self.name}.lora_bias" in params_dict:
            self.lora_bias = params_dict[f"{self.name}.lora_bias"]
    
    def merge_with_base(self) -> mx.array:
        """
        Merge LoRA weights with base weights.
        
        Returns:
            Combined weight matrix
        """
        # Calculate LoRA contribution
        lora_weight = mx.matmul(self.lora_B, self.lora_A) * self.scaling
        
        # Add to base weights
        merged_weight = self.base_weight + lora_weight
        
        return merged_weight
    
    def get_base_weight(self) -> mx.array:
        """Get the base weight matrix."""
        return self.base_weight
    
    def get_base_bias(self) -> Optional[mx.array]:
        """Get the base bias vector."""
        return self.base_bias


class LoRATransformerLayer:
    """LoRA adapter for a transformer layer."""
    
    def __init__(
        self,
        transformer_layer,
        r: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
        target_modules: Optional[List[str]] = None,
        use_bias: bool = False,
        layer_idx: int = 0
    ):
        """
        Initialize LoRA adapters for a transformer layer.
        
        Args:
            transformer_layer: MLXTransformerLayer instance
            r: LoRA rank
            alpha: LoRA scaling factor
            dropout: Dropout probability for LoRA layers
            target_modules: List of module types to apply LoRA to
                            Default is ["q_proj", "k_proj", "v_proj", "o_proj"]
            use_bias: Whether to use bias in LoRA layers
            layer_idx: Index of this layer in the transformer model
        """
        self.base_layer = transformer_layer
        self.layer_idx = layer_idx
        self.hidden_size = transformer_layer.hidden_size
        self.head_dim = transformer_layer.head_dim
        
        # Default target modules for attention
        if target_modules is None:
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
        
        self.target_modules = target_modules
        self.lora_adapters = {}
        
        # Set up LoRA adapters for target modules
        for module_name in target_modules:
            # Get base weights
            if module_name == "q_proj" and transformer_layer.q_proj_weight is not None:
                base_weight = transformer_layer.q_proj_weight
                base_bias = transformer_layer.q_proj_bias
                name = f"layers.{layer_idx}.attn.{module_name}"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
            elif module_name == "k_proj" and transformer_layer.k_proj_weight is not None:
                base_weight = transformer_layer.k_proj_weight
                base_bias = transformer_layer.k_proj_bias
                name = f"layers.{layer_idx}.attn.{module_name}"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
            elif module_name == "v_proj" and transformer_layer.v_proj_weight is not None:
                base_weight = transformer_layer.v_proj_weight
                base_bias = transformer_layer.v_proj_bias
                name = f"layers.{layer_idx}.attn.{module_name}"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
            elif module_name == "o_proj" and transformer_layer.o_proj_weight is not None:
                base_weight = transformer_layer.o_proj_weight
                base_bias = transformer_layer.o_proj_bias
                name = f"layers.{layer_idx}.attn.output_proj"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
            elif module_name == "gate_proj" and transformer_layer.gate_proj_weight is not None:
                base_weight = transformer_layer.gate_proj_weight
                base_bias = transformer_layer.gate_proj_bias
                name = f"layers.{layer_idx}.mlp.w1"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
            elif module_name == "up_proj" and transformer_layer.up_proj_weight is not None:
                base_weight = transformer_layer.up_proj_weight
                base_bias = transformer_layer.up_proj_bias
                name = f"layers.{layer_idx}.mlp.w3"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
            elif module_name == "down_proj" and transformer_layer.down_proj_weight is not None:
                base_weight = transformer_layer.down_proj_weight
                base_bias = transformer_layer.down_proj_bias
                name = f"layers.{layer_idx}.mlp.w2"
                self.lora_adapters[module_name] = LoRALinear(
                    base_weight=base_weight,
                    base_bias=base_bias,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    use_bias=use_bias,
                    name=name
                )
    
    def __call__(self, hidden_states, attention_mask=None, position_ids=None, past_key_value=None, output_attentions=False):
        """
        Forward pass through the LoRA-adapted transformer layer.
        
        Args:
            hidden_states: Input tensor
            attention_mask: Attention mask
            position_ids: Position IDs
            past_key_value: Past key-value pairs for caching
            output_attentions: Whether to output attention weights
            
        Returns:
            Output tensor
        """
        residual = hidden_states
        
        # Apply layer norm
        layernorm_output = self.base_layer._layernorm(
            hidden_states,
            self.base_layer.input_layernorm_weight,
            self.base_layer.input_layernorm_bias
        )
        
        # Self-attention with LoRA adapters for attention projections
        attention_output = self._lora_attention(
            layernorm_output,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value
        )
        
        # First residual connection
        hidden_states = residual + attention_output
        
        # Second residual connection with feedforward network
        residual = hidden_states
        
        # Apply second layer norm
        layernorm_output = self.base_layer._layernorm(
            hidden_states,
            self.base_layer.post_attention_layernorm_weight,
            self.base_layer.post_attention_layernorm_bias
        )
        
        # Apply feedforward network with LoRA adapters for MLP if specified
        feedforward_output = self._lora_feedforward(layernorm_output)
        
        # Second residual connection
        hidden_states = residual + feedforward_output
        
        return hidden_states
        
    # Keep forward method for backward compatibility
    forward = __call__
    
    def _lora_attention(self, hidden_states, attention_mask=None, position_ids=None, past_key_value=None):
        """Apply multi-head attention with LoRA adapters."""
        batch_size, seq_length, hidden_size = hidden_states.shape
        
        # Project query, key, value with LoRA adapters if available
        if "q_proj" in self.lora_adapters:
            query_states = self.lora_adapters["q_proj"](hidden_states)
        else:
            # Fallback to base implementation
            if self.base_layer.q_proj_weight is not None:
                query_states = mx.matmul(hidden_states, self.base_layer.q_proj_weight.T)
                if self.base_layer.q_proj_bias is not None:
                    query_states = query_states + self.base_layer.q_proj_bias
            else:
                raise ValueError("Query projection weight is not loaded")
            
        if "k_proj" in self.lora_adapters:
            key_states = self.lora_adapters["k_proj"](hidden_states)
        else:
            # Fallback to base implementation
            if self.base_layer.k_proj_weight is not None:
                key_states = mx.matmul(hidden_states, self.base_layer.k_proj_weight.T)
                if self.base_layer.k_proj_bias is not None:
                    key_states = key_states + self.base_layer.k_proj_bias
            else:
                raise ValueError("Key projection weight is not loaded")
            
        if "v_proj" in self.lora_adapters:
            value_states = self.lora_adapters["v_proj"](hidden_states)
        else:
            # Fallback to base implementation
            if self.base_layer.v_proj_weight is not None:
                value_states = mx.matmul(hidden_states, self.base_layer.v_proj_weight.T)
                if self.base_layer.v_proj_bias is not None:
                    value_states = value_states + self.base_layer.v_proj_bias
            else:
                raise ValueError("Value projection weight is not loaded")
        
        # Step 2: Reshape for multi-head attention
        head_dim = hidden_size // self.base_layer.num_heads
        
        query_states = query_states.reshape(batch_size, seq_length, self.base_layer.num_heads, head_dim)
        key_states = key_states.reshape(batch_size, seq_length, self.base_layer.num_kv_heads, head_dim)
        value_states = value_states.reshape(batch_size, seq_length, self.base_layer.num_kv_heads, head_dim)
        
        # Apply rotary embeddings if position_ids are provided
        if position_ids is not None and self.base_layer.cos_cached is not None and self.base_layer.sin_cached is not None:
            # Apply rotary embeddings to query and key states
            cos = mx.take(self.base_layer.cos_cached, position_ids, axis=0)
            sin = mx.take(self.base_layer.sin_cached, position_ids, axis=0)
            
            # Reshape for multiplication
            cos = cos.reshape(batch_size, seq_length, 1, head_dim)
            sin = sin.reshape(batch_size, seq_length, 1, head_dim)
            
            # Apply rotary embedding operation
            query_states = self.base_layer._apply_rotary_pos_emb(query_states, cos, sin)
            key_states = self.base_layer._apply_rotary_pos_emb(key_states, cos, sin)
        
        # Transpose for matrix multiplication
        query_states = mx.transpose(query_states, (0, 2, 1, 3))
        key_states = mx.transpose(key_states, (0, 2, 1, 3))
        value_states = mx.transpose(value_states, (0, 2, 1, 3))
        
        # Calculate attention scores
        attention_scores = mx.matmul(query_states, mx.transpose(key_states, (0, 1, 3, 2)))
        
        # Scale attention scores
        attention_scores = attention_scores / math.sqrt(head_dim)
        
        # Apply attention mask if provided
        if attention_mask is not None:
            # Ensure mask has right shape for broadcasting
            if len(attention_mask.shape) == 3:
                attention_mask = mx.expand_dims(attention_mask, axis=1)
                
            # Apply mask by adding a large negative value to masked positions
            attention_scores = mx.where(attention_mask, attention_scores, mx.full_like(attention_scores, -1e9))
        
        # Apply softmax to get attention probabilities
        attention_probs = mx.softmax(attention_scores, axis=-1)
        
        # Compute context as weighted sum of values
        context = mx.matmul(attention_probs, value_states)
        
        # Transpose and reshape back
        context = mx.transpose(context, (0, 2, 1, 3))
        context = context.reshape(batch_size, seq_length, hidden_size)
        
        # Apply output projection with LoRA if available
        if "o_proj" in self.lora_adapters:
            context = self.lora_adapters["o_proj"](context)
        else:
            # Fallback to base implementation
            if self.base_layer.o_proj_weight is not None:
                context = mx.matmul(context, self.base_layer.o_proj_weight.T)
                if self.base_layer.o_proj_bias is not None:
                    context = context + self.base_layer.o_proj_bias
            else:
                raise ValueError("Output projection weight is not loaded")
            
        return context
    
    def _lora_feedforward(self, hidden_states):
        """Apply feedforward network with LoRA adapters."""
        # Step 1: Calculate gating and up-projection with LoRA if available
        if "gate_proj" in self.lora_adapters:
            gate_proj = self.lora_adapters["gate_proj"](hidden_states)
        else:
            # Fallback to base implementation
            if self.base_layer.gate_proj_weight is not None:
                gate_proj = mx.matmul(hidden_states, self.base_layer.gate_proj_weight.T)
                if self.base_layer.gate_proj_bias is not None:
                    gate_proj = gate_proj + self.base_layer.gate_proj_bias
            else:
                raise ValueError("Gate projection weight is not loaded")
            
        if "up_proj" in self.lora_adapters:
            up_proj = self.lora_adapters["up_proj"](hidden_states)
        else:
            # Fallback to base implementation
            if self.base_layer.up_proj_weight is not None:
                up_proj = mx.matmul(hidden_states, self.base_layer.up_proj_weight.T)
                if self.base_layer.up_proj_bias is not None:
                    up_proj = up_proj + self.base_layer.up_proj_bias
            else:
                raise ValueError("Up projection weight is not loaded")
            
        # Step 2: Apply SwiGLU activation
        # swish(x) = x * sigmoid(x)
        swish = up_proj * mx.sigmoid(up_proj)
        intermediate = gate_proj * swish
        
        # Step 3: Apply down projection with LoRA if available
        if "down_proj" in self.lora_adapters:
            output = self.lora_adapters["down_proj"](intermediate)
        else:
            # Fallback to base implementation
            if self.base_layer.down_proj_weight is not None:
                output = mx.matmul(intermediate, self.base_layer.down_proj_weight.T)
                if self.base_layer.down_proj_bias is not None:
                    output = output + self.base_layer.down_proj_bias
            else:
                raise ValueError("Down projection weight is not loaded")
            
        return output
    
    def parameters(self):
        """
        Get trainable LoRA parameters.
        
        Returns:
            Dictionary of parameter name to parameter value
        """
        params = {}
        for module_name, lora_adapter in self.lora_adapters.items():
            params.update(lora_adapter.parameters())
        return params
    
    def update(self, params_dict):
        """
        Update LoRA parameters from dictionary.
        
        Args:
            params_dict: Dictionary of parameter name to parameter value
        """
        for module_name, lora_adapter in self.lora_adapters.items():
            lora_adapter.update(params_dict)


class LoRATransformer:
    """LoRA adapter for a transformer model."""
    
    def __init__(
        self,
        transformer_model,
        r: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
        target_modules: Optional[List[str]] = None,
        target_layers: Optional[List[int]] = None,
        use_bias: bool = False
    ):
        """
        Initialize LoRA adapters for a transformer model.
        
        Args:
            transformer_model: MLXTransformer instance
            r: LoRA rank
            alpha: LoRA scaling factor
            dropout: Dropout probability for LoRA layers
            target_modules: List of module types to apply LoRA to
                            Default is ["q_proj", "k_proj", "v_proj", "o_proj"]
            target_layers: List of layer indices to apply LoRA to
                          Default is all layers
            use_bias: Whether to use bias in LoRA layers
        """
        self.base_model = transformer_model
        self.hidden_size = transformer_model.hidden_size
        self.num_layers = transformer_model.num_layers
        
        # Default all layers if not specified
        if target_layers is None:
            target_layers = list(range(self.num_layers))
        
        self.target_layers = target_layers
        self.lora_layers = []
        
        # Create LoRA adapters for target layers
        for layer_idx, base_layer in enumerate(transformer_model.layers):
            if layer_idx in target_layers:
                lora_layer = LoRATransformerLayer(
                    transformer_layer=base_layer,
                    r=r,
                    alpha=alpha,
                    dropout=dropout,
                    target_modules=target_modules,
                    use_bias=use_bias,
                    layer_idx=layer_idx
                )
                self.lora_layers.append((layer_idx, lora_layer))
            else:
                self.lora_layers.append((layer_idx, base_layer))
    
    def __call__(self, hidden_states, attention_mask=None, position_ids=None, input_pos=None, mask=None):
        """
        Forward pass through the LoRA-adapted transformer model.
        
        Args:
            hidden_states: Input tensor [batch_size, seq_length, hidden_size]
            attention_mask: Optional attention mask
            position_ids: Optional position indices
            input_pos: Alias for position_ids for compatibility
            mask: Alias for attention_mask for compatibility
            
        Returns:
            Output tensor [batch_size, seq_length, hidden_size]
        """
        # For compatibility with different calling conventions
        if attention_mask is None and mask is not None:
            attention_mask = mask
        
        if position_ids is None and input_pos is not None:
            position_ids = input_pos
            
        # Process through each transformer layer
        for layer_idx, layer in self.lora_layers:
            if hasattr(layer, '__call__'):
                hidden_states = layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids
                )
            elif hasattr(layer, 'forward'):
                hidden_states = layer.forward(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids
                )
            else:
                raise ValueError(f"Layer {layer_idx} has neither __call__ nor forward method")
            
        # Apply final layer norm if available in base model
        if self.base_model.final_layernorm_weight is not None:
            # Get mean and variance for layer normalization
            mean = mx.mean(hidden_states, axis=-1, keepdims=True)
            variance = mx.mean((hidden_states - mean) ** 2, axis=-1, keepdims=True)
            
            # Normalize using weight and bias
            hidden_states = (hidden_states - mean) / mx.sqrt(variance + 1e-5)
            
            # Apply weight and bias if present
            hidden_states = hidden_states * self.base_model.final_layernorm_weight.reshape(1, 1, -1)
            
            if self.base_model.final_layernorm_bias is not None:
                hidden_states = hidden_states + self.base_model.final_layernorm_bias.reshape(1, 1, -1)
                
        return hidden_states
    
    def parameters(self):
        """
        Get trainable LoRA parameters.
        
        Returns:
            Dictionary of parameter name to parameter value
        """
        params = {}
        for layer_idx, layer in self.lora_layers:
            if isinstance(layer, LoRATransformerLayer):
                params.update(layer.parameters())
        return params
    
    def update(self, params_dict):
        """
        Update LoRA parameters from dictionary.
        
        Args:
            params_dict: Dictionary of parameter name to parameter value
        """
        for layer_idx, layer in self.lora_layers:
            if isinstance(layer, LoRATransformerLayer):
                layer_params = {}
                for name, param in params_dict.items():
                    if name.startswith(f"layers.{layer_idx}."):
                        layer_params[name] = param
                layer.update(layer_params)
    
    def merge_with_base(self):
        """
        Merge LoRA weights with base weights.
        
        Returns:
            New transformer model with merged weights
        """
        # Create a new transformer model with the same configuration
        from copy import deepcopy
        merged_model = deepcopy(self.base_model)
        
        # Merge weights for all LoRA layers
        for layer_idx, layer in self.lora_layers:
            if isinstance(layer, LoRATransformerLayer):
                for module_name, lora_adapter in layer.lora_adapters.items():
                    merged_weight = lora_adapter.merge_with_base()
                    
                    # Update the appropriate weight in the merged model
                    if module_name == "q_proj":
                        merged_model.layers[layer_idx].q_proj_weight = merged_weight
                    elif module_name == "k_proj":
                        merged_model.layers[layer_idx].k_proj_weight = merged_weight
                    elif module_name == "v_proj":
                        merged_model.layers[layer_idx].v_proj_weight = merged_weight
                    elif module_name == "o_proj":
                        merged_model.layers[layer_idx].o_proj_weight = merged_weight
                    elif module_name == "gate_proj":
                        merged_model.layers[layer_idx].gate_proj_weight = merged_weight
                    elif module_name == "up_proj":
                        merged_model.layers[layer_idx].up_proj_weight = merged_weight
                    elif module_name == "down_proj":
                        merged_model.layers[layer_idx].down_proj_weight = merged_weight
        
        return merged_model


def apply_lora_to_model(
    model,
    r: int = 8,
    alpha: float = 16.0,
    dropout: float = 0.0,
    target_modules: Optional[List[str]] = None,
    target_layers: Optional[List[int]] = None,
    use_bias: bool = False
):
    """
    Apply LoRA to a CSM model.
    
    Args:
        model: CSM model with backbone and decoder
        r: LoRA rank
        alpha: LoRA scaling factor
        dropout: Dropout probability for LoRA layers
        target_modules: List of module types to apply LoRA to
        target_layers: List of layer indices to apply LoRA to
        use_bias: Whether to use bias in LoRA layers
        
    Returns:
        A model with LoRA adapters applied to specified modules
    """
    # First check if the model has backbone and decoder attributes
    if not hasattr(model, 'backbone') or not hasattr(model, 'decoder'):
        raise ValueError("Model must have backbone and decoder attributes")
    
    # LoRA defaults for CSM
    if target_modules is None:
        # Default to attention modules for best efficiency/performance trade-off
        target_modules = ["q_proj", "v_proj"]
    
    # Apply LoRA to backbone
    lora_backbone = LoRATransformer(
        transformer_model=model.backbone,
        r=r,
        alpha=alpha,
        dropout=dropout,
        target_modules=target_modules,
        target_layers=target_layers,
        use_bias=use_bias
    )
    model.backbone = lora_backbone
    
    # Apply LoRA to decoder
    lora_decoder = LoRATransformer(
        transformer_model=model.decoder,
        r=r,
        alpha=alpha,
        dropout=dropout,
        target_modules=target_modules,
        target_layers=target_layers,
        use_bias=use_bias
    )
    model.decoder = lora_decoder
    
    # Add LoRA-specific methods to model
    def get_lora_params(self):
        """Get only the trainable LoRA parameters."""
        params = {}
        
        # Get backbone LoRA parameters
        if hasattr(self.backbone, 'parameters'):
            backbone_params = self.backbone.parameters()
            params.update(backbone_params)
        
        # Get decoder LoRA parameters
        if hasattr(self.decoder, 'parameters'):
            decoder_params = self.decoder.parameters()
            params.update(decoder_params)
            
        return params
    
    model.get_lora_params = get_lora_params.__get__(model)
    
    # Add method to merge weights
    def merge_lora_weights(self):
        """Merge LoRA weights with base weights."""
        if hasattr(self.backbone, 'merge_with_base'):
            self.backbone = self.backbone.merge_with_base()
        
        if hasattr(self.decoder, 'merge_with_base'):
            self.decoder = self.decoder.merge_with_base()
        
        return self
    
    model.merge_lora_weights = merge_lora_weights.__get__(model)
    
    return model